{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dkm4/Fully-connected-neural-network-project/blob/main/Fully_connected_Neural_Network_ipynb%EC%9D%98_%EC%82%AC%EB%B3%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Change the runtime to use GPU"
      ],
      "metadata": {
        "id": "40dw6T8YqiNx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# imports"
      ],
      "metadata": {
        "id": "oFSLySalqrbw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "print(torch.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_z3XfIFlqqZf",
        "outputId": "5c3da30c-5fe0-47a3-ac8a-960cb24cd9c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.1.0+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Find the device"
      ],
      "metadata": {
        "id": "wxb4vpJ9q3K3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxvQAeM8q4au",
        "outputId": "cb5b2881-ebff-4300-9a15-69653a2e278c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build my X values"
      ],
      "metadata": {
        "id": "JWD5lPJJqxon"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.arange(0, 1, 0.02).to(device)\n",
        "print(X.shape)\n",
        "X = X.unsqueeze(dim=1)\n",
        "print(X.shape)\n",
        "print(X.device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wU9tuHf9q0Bv",
        "outputId": "ab8537ca-ab6c-458a-ff9a-059d4d018de7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([50])\n",
            "torch.Size([50, 1])\n",
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build my Y values"
      ],
      "metadata": {
        "id": "1hEKXOmVrK1H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "slope = -9.0\n",
        "intercept = -5.0\n",
        "y = slope*X+intercept\n",
        "print(y.shape)\n",
        "print(y.device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtEHasZBrMgf",
        "outputId": "e54691a4-0099-453d-ffb3-701b2c6cfac8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([50, 1])\n",
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Input Shape {X.shape} and output shape {y.shape}\")\n",
        "print(f\"type of X is {type(X)} and type of y is {type(y)}\")\n",
        "print(len(X))\n",
        "print(len(y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRtOxSdQrSF4",
        "outputId": "06232312-8f94-428a-e85a-fbf028c361d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Shape torch.Size([50, 1]) and output shape torch.Size([50, 1])\n",
            "type of X is <class 'torch.Tensor'> and type of y is <class 'torch.Tensor'>\n",
            "50\n",
            "50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print(len(X_train), len(X_test), len(y_train), len(y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFWpJgOTrUZ_",
        "outputId": "5e601b4b-6735-48d7-a97b-8b549112ebc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "40 10 40 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(X_train.cpu(), y_train.cpu(), c=\"b\", s=7, label=\"Training Data\")\n",
        "plt.scatter(X_test.cpu(), y_test.cpu(), c=\"r\", s=7, label=\"Test Data\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "HbJToeAPrXHI",
        "outputId": "39bf742b-4728-4032-8a49-17770b6c486e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x795558d0a500>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_dataset(train_data, test_data, train_labels, test_labels, predictions=None):\n",
        "  plt.scatter(train_data.cpu(), train_labels.cpu(), c=\"b\", s=7, label=\"Training Data\")\n",
        "  plt.scatter(test_data.cpu(), test_labels.cpu(), c=\"g\", s=7, label=\"Test Data\")\n",
        "  if predictions is not None:\n",
        "    plt.scatter(test_data.cpu(), predictions.cpu(), c=\"r\", s=7, label=\"Predictions\")\n",
        "  plt.legend(prop={\"size\":10})"
      ],
      "metadata": {
        "id": "SNEyJV93ru4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_dataset(X_train, X_test, y_train, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "f33Nj6gXr2_o",
        "outputId": "097ae3c9-e8a0-4d45-d9e9-1df04c312289"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_dataset(X_train, X_test, y_train, y_test, y_test+0.3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "58x_kqxMr5iA",
        "outputId": "abbacdc2-4dff-4176-d010-ae4fe23d3705"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetworkFC(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.linear_layer = nn.Linear(in_features=1, out_features=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.linear_layer(x)"
      ],
      "metadata": {
        "id": "neEuYpAUr9W4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_model = NeuralNetworkFC().to(device)\n",
        "print(f\"Model parameters {my_model.state_dict()}\")\n",
        "print(f\"Model parameters {list(my_model.parameters())}\")\n",
        "print(f\"Device of my parameters: {next(my_model.parameters()).device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXQpmWnBr-9q",
        "outputId": "7f227502-9c26-4403-ff9c-5f66b8291783"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model parameters OrderedDict([('linear_layer.weight', tensor([[0.6283]], device='cuda:0')), ('linear_layer.bias', tensor([0.9044], device='cuda:0'))])\n",
            "Model parameters [Parameter containing:\n",
            "tensor([[0.6283]], device='cuda:0', requires_grad=True), Parameter containing:\n",
            "tensor([0.9044], device='cuda:0', requires_grad=True)]\n",
            "Device of my parameters: cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_model.eval()\n",
        "with torch.no_grad():\n",
        "  y_preds = my_model(X_test)\n",
        "print(X_test)\n",
        "print(f\"X test shape is {X_test.shape}\")\n",
        "print(y_preds)\n",
        "print(f\"y preds shape is {y_preds.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lfi2whb8siS3",
        "outputId": "2de02cc7-f599-40d3-8bcf-8d7806e063dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2600],\n",
            "        [0.7800],\n",
            "        [0.6000],\n",
            "        [0.9000],\n",
            "        [0.3400],\n",
            "        [0.9600],\n",
            "        [0.5200],\n",
            "        [0.5000],\n",
            "        [0.6400],\n",
            "        [0.3800]], device='cuda:0')\n",
            "X test shape is torch.Size([10, 1])\n",
            "tensor([[1.0678],\n",
            "        [1.3945],\n",
            "        [1.2814],\n",
            "        [1.4699],\n",
            "        [1.1180],\n",
            "        [1.5075],\n",
            "        [1.2311],\n",
            "        [1.2185],\n",
            "        [1.3065],\n",
            "        [1.1431]], device='cuda:0')\n",
            "y preds shape is torch.Size([10, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_dataset(X_train, X_test, y_train, y_test, y_preds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "9p-xNWIHskwQ",
        "outputId": "0de2faf6-99ae-43d9-9ae8-cf97f0135a1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.L1Loss()\n",
        "optimizer = torch.optim.SGD(params = my_model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "gZ8e2xymsniZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 50000\n",
        "for epoch in range(epochs):\n",
        "  my_model.train()\n",
        "\n",
        "  y_preds = my_model(X_train)\n",
        "  training_loss = loss_fn(y_preds, y_train)\n",
        "\n",
        "  #make all the previous gradients to zero\n",
        "  optimizer.zero_grad()\n",
        "  #compute the gradients\n",
        "  training_loss.backward()\n",
        "  #take an optimization step\n",
        "  optimizer.step()\n",
        "\n",
        "  my_model.eval()\n",
        "  with torch.no_grad():\n",
        "    test_predictions = my_model(X_test)\n",
        "    test_loss = loss_fn(test_predictions, y_test)\n",
        "\n",
        "  if epoch%10==0:\n",
        "    print(f\"Epoch: {epoch} | Training Loss: {training_loss} | Testing Loss: {test_loss}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V516anB1sraQ",
        "outputId": "341f1cf4-7b97-44f0-ac37-98741f6d5b3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 | Training Loss: 10.386363983154297 | Testing Loss: 11.564555168151855\n",
            "Epoch: 10 | Training Loss: 10.374197006225586 | Testing Loss: 11.551816940307617\n",
            "Epoch: 20 | Training Loss: 10.362030029296875 | Testing Loss: 11.539079666137695\n",
            "Epoch: 30 | Training Loss: 10.349863052368164 | Testing Loss: 11.52634334564209\n",
            "Epoch: 40 | Training Loss: 10.33769702911377 | Testing Loss: 11.513606071472168\n",
            "Epoch: 50 | Training Loss: 10.325530052185059 | Testing Loss: 11.500868797302246\n",
            "Epoch: 60 | Training Loss: 10.313362121582031 | Testing Loss: 11.488131523132324\n",
            "Epoch: 70 | Training Loss: 10.301196098327637 | Testing Loss: 11.475394248962402\n",
            "Epoch: 80 | Training Loss: 10.289029121398926 | Testing Loss: 11.46265697479248\n",
            "Epoch: 90 | Training Loss: 10.276862144470215 | Testing Loss: 11.449920654296875\n",
            "Epoch: 100 | Training Loss: 10.26469612121582 | Testing Loss: 11.437183380126953\n",
            "Epoch: 110 | Training Loss: 10.252528190612793 | Testing Loss: 11.424446105957031\n",
            "Epoch: 120 | Training Loss: 10.240362167358398 | Testing Loss: 11.41170883178711\n",
            "Epoch: 130 | Training Loss: 10.228195190429688 | Testing Loss: 11.398971557617188\n",
            "Epoch: 140 | Training Loss: 10.216028213500977 | Testing Loss: 11.386235237121582\n",
            "Epoch: 150 | Training Loss: 10.203862190246582 | Testing Loss: 11.373497009277344\n",
            "Epoch: 160 | Training Loss: 10.191695213317871 | Testing Loss: 11.360760688781738\n",
            "Epoch: 170 | Training Loss: 10.179527282714844 | Testing Loss: 11.348024368286133\n",
            "Epoch: 180 | Training Loss: 10.16736125946045 | Testing Loss: 11.335286140441895\n",
            "Epoch: 190 | Training Loss: 10.155194282531738 | Testing Loss: 11.322549819946289\n",
            "Epoch: 200 | Training Loss: 10.143027305603027 | Testing Loss: 11.309812545776367\n",
            "Epoch: 210 | Training Loss: 10.130861282348633 | Testing Loss: 11.297076225280762\n",
            "Epoch: 220 | Training Loss: 10.118693351745605 | Testing Loss: 11.28433895111084\n",
            "Epoch: 230 | Training Loss: 10.106526374816895 | Testing Loss: 11.271600723266602\n",
            "Epoch: 240 | Training Loss: 10.0943603515625 | Testing Loss: 11.258864402770996\n",
            "Epoch: 250 | Training Loss: 10.082193374633789 | Testing Loss: 11.246127128601074\n",
            "Epoch: 260 | Training Loss: 10.070025444030762 | Testing Loss: 11.233389854431152\n",
            "Epoch: 270 | Training Loss: 10.057859420776367 | Testing Loss: 11.220653533935547\n",
            "Epoch: 280 | Training Loss: 10.045692443847656 | Testing Loss: 11.207915306091309\n",
            "Epoch: 290 | Training Loss: 10.033525466918945 | Testing Loss: 11.195178031921387\n",
            "Epoch: 300 | Training Loss: 10.021358489990234 | Testing Loss: 11.182441711425781\n",
            "Epoch: 310 | Training Loss: 10.00919246673584 | Testing Loss: 11.16970443725586\n",
            "Epoch: 320 | Training Loss: 9.997025489807129 | Testing Loss: 11.156968116760254\n",
            "Epoch: 330 | Training Loss: 9.984858512878418 | Testing Loss: 11.144229888916016\n",
            "Epoch: 340 | Training Loss: 9.972691535949707 | Testing Loss: 11.131492614746094\n",
            "Epoch: 350 | Training Loss: 9.960524559020996 | Testing Loss: 11.118756294250488\n",
            "Epoch: 360 | Training Loss: 9.948358535766602 | Testing Loss: 11.106019973754883\n",
            "Epoch: 370 | Training Loss: 9.936190605163574 | Testing Loss: 11.093281745910645\n",
            "Epoch: 380 | Training Loss: 9.924025535583496 | Testing Loss: 11.080545425415039\n",
            "Epoch: 390 | Training Loss: 9.911857604980469 | Testing Loss: 11.067808151245117\n",
            "Epoch: 400 | Training Loss: 9.899690628051758 | Testing Loss: 11.055070877075195\n",
            "Epoch: 410 | Training Loss: 9.887523651123047 | Testing Loss: 11.04233455657959\n",
            "Epoch: 420 | Training Loss: 9.875357627868652 | Testing Loss: 11.029597282409668\n",
            "Epoch: 430 | Training Loss: 9.863189697265625 | Testing Loss: 11.016860008239746\n",
            "Epoch: 440 | Training Loss: 9.85102367401123 | Testing Loss: 11.004122734069824\n",
            "Epoch: 450 | Training Loss: 9.83885669708252 | Testing Loss: 10.991385459899902\n",
            "Epoch: 460 | Training Loss: 9.826689720153809 | Testing Loss: 10.97864818572998\n",
            "Epoch: 470 | Training Loss: 9.814523696899414 | Testing Loss: 10.965911865234375\n",
            "Epoch: 480 | Training Loss: 9.802355766296387 | Testing Loss: 10.953173637390137\n",
            "Epoch: 490 | Training Loss: 9.790188789367676 | Testing Loss: 10.940437316894531\n",
            "Epoch: 500 | Training Loss: 9.778021812438965 | Testing Loss: 10.927699089050293\n",
            "Epoch: 510 | Training Loss: 9.76585578918457 | Testing Loss: 10.914962768554688\n",
            "Epoch: 520 | Training Loss: 9.753687858581543 | Testing Loss: 10.902225494384766\n",
            "Epoch: 530 | Training Loss: 9.741522789001465 | Testing Loss: 10.88948917388916\n",
            "Epoch: 540 | Training Loss: 9.729354858398438 | Testing Loss: 10.876751899719238\n",
            "Epoch: 550 | Training Loss: 9.717187881469727 | Testing Loss: 10.864013671875\n",
            "Epoch: 560 | Training Loss: 9.705021858215332 | Testing Loss: 10.851277351379395\n",
            "Epoch: 570 | Training Loss: 9.692854881286621 | Testing Loss: 10.838540077209473\n",
            "Epoch: 580 | Training Loss: 9.680686950683594 | Testing Loss: 10.825803756713867\n",
            "Epoch: 590 | Training Loss: 9.6685209274292 | Testing Loss: 10.813066482543945\n",
            "Epoch: 600 | Training Loss: 9.656353950500488 | Testing Loss: 10.800329208374023\n",
            "Epoch: 610 | Training Loss: 9.644186973571777 | Testing Loss: 10.787591934204102\n",
            "Epoch: 620 | Training Loss: 9.632020950317383 | Testing Loss: 10.774855613708496\n",
            "Epoch: 630 | Training Loss: 9.619853019714355 | Testing Loss: 10.762118339538574\n",
            "Epoch: 640 | Training Loss: 9.607686042785645 | Testing Loss: 10.749381065368652\n",
            "Epoch: 650 | Training Loss: 9.59552001953125 | Testing Loss: 10.73664379119873\n",
            "Epoch: 660 | Training Loss: 9.583353042602539 | Testing Loss: 10.723906517028809\n",
            "Epoch: 670 | Training Loss: 9.571185111999512 | Testing Loss: 10.711169242858887\n",
            "Epoch: 680 | Training Loss: 9.559019088745117 | Testing Loss: 10.698431968688965\n",
            "Epoch: 690 | Training Loss: 9.546852111816406 | Testing Loss: 10.685694694519043\n",
            "Epoch: 700 | Training Loss: 9.534686088562012 | Testing Loss: 10.672957420349121\n",
            "Epoch: 710 | Training Loss: 9.522517204284668 | Testing Loss: 10.6602201461792\n",
            "Epoch: 720 | Training Loss: 9.510351181030273 | Testing Loss: 10.647482872009277\n",
            "Epoch: 730 | Training Loss: 9.498184204101562 | Testing Loss: 10.634745597839355\n",
            "Epoch: 740 | Training Loss: 9.486017227172852 | Testing Loss: 10.622008323669434\n",
            "Epoch: 750 | Training Loss: 9.473849296569824 | Testing Loss: 10.609271049499512\n",
            "Epoch: 760 | Training Loss: 9.461684226989746 | Testing Loss: 10.596534729003906\n",
            "Epoch: 770 | Training Loss: 9.449516296386719 | Testing Loss: 10.583796501159668\n",
            "Epoch: 780 | Training Loss: 9.437349319458008 | Testing Loss: 10.571060180664062\n",
            "Epoch: 790 | Training Loss: 9.425182342529297 | Testing Loss: 10.558321952819824\n",
            "Epoch: 800 | Training Loss: 9.413016319274902 | Testing Loss: 10.545584678649902\n",
            "Epoch: 810 | Training Loss: 9.400848388671875 | Testing Loss: 10.53284740447998\n",
            "Epoch: 820 | Training Loss: 9.388681411743164 | Testing Loss: 10.520111083984375\n",
            "Epoch: 830 | Training Loss: 9.376514434814453 | Testing Loss: 10.507372856140137\n",
            "Epoch: 840 | Training Loss: 9.364348411560059 | Testing Loss: 10.494636535644531\n",
            "Epoch: 850 | Training Loss: 9.352180480957031 | Testing Loss: 10.481900215148926\n",
            "Epoch: 860 | Training Loss: 9.340014457702637 | Testing Loss: 10.469161987304688\n",
            "Epoch: 870 | Training Loss: 9.327847480773926 | Testing Loss: 10.456425666809082\n",
            "Epoch: 880 | Training Loss: 9.315680503845215 | Testing Loss: 10.44368839263916\n",
            "Epoch: 890 | Training Loss: 9.303513526916504 | Testing Loss: 10.430951118469238\n",
            "Epoch: 900 | Training Loss: 9.291346549987793 | Testing Loss: 10.418214797973633\n",
            "Epoch: 910 | Training Loss: 9.279179573059082 | Testing Loss: 10.405476570129395\n",
            "Epoch: 920 | Training Loss: 9.267012596130371 | Testing Loss: 10.392740249633789\n",
            "Epoch: 930 | Training Loss: 9.254846572875977 | Testing Loss: 10.380002975463867\n",
            "Epoch: 940 | Training Loss: 9.24267864227295 | Testing Loss: 10.367265701293945\n",
            "Epoch: 950 | Training Loss: 9.230511665344238 | Testing Loss: 10.35452938079834\n",
            "Epoch: 960 | Training Loss: 9.218344688415527 | Testing Loss: 10.341791152954102\n",
            "Epoch: 970 | Training Loss: 9.206178665161133 | Testing Loss: 10.329054832458496\n",
            "Epoch: 980 | Training Loss: 9.194010734558105 | Testing Loss: 10.316317558288574\n",
            "Epoch: 990 | Training Loss: 9.181843757629395 | Testing Loss: 10.303580284118652\n",
            "Epoch: 1000 | Training Loss: 9.169676780700684 | Testing Loss: 10.29084300994873\n",
            "Epoch: 1010 | Training Loss: 9.157510757446289 | Testing Loss: 10.278105735778809\n",
            "Epoch: 1020 | Training Loss: 9.145342826843262 | Testing Loss: 10.265368461608887\n",
            "Epoch: 1030 | Training Loss: 9.13317584991455 | Testing Loss: 10.252631187438965\n",
            "Epoch: 1040 | Training Loss: 9.121009826660156 | Testing Loss: 10.239893913269043\n",
            "Epoch: 1050 | Training Loss: 9.108842849731445 | Testing Loss: 10.227156639099121\n",
            "Epoch: 1060 | Training Loss: 9.096674919128418 | Testing Loss: 10.2144193649292\n",
            "Epoch: 1070 | Training Loss: 9.084508895874023 | Testing Loss: 10.201683044433594\n",
            "Epoch: 1080 | Training Loss: 9.072341918945312 | Testing Loss: 10.188944816589355\n",
            "Epoch: 1090 | Training Loss: 9.060174942016602 | Testing Loss: 10.176207542419434\n",
            "Epoch: 1100 | Training Loss: 9.048007011413574 | Testing Loss: 10.163471221923828\n",
            "Epoch: 1110 | Training Loss: 9.035841941833496 | Testing Loss: 10.150733947753906\n",
            "Epoch: 1120 | Training Loss: 9.023674011230469 | Testing Loss: 10.137996673583984\n",
            "Epoch: 1130 | Training Loss: 9.011507034301758 | Testing Loss: 10.125259399414062\n",
            "Epoch: 1140 | Training Loss: 8.999340057373047 | Testing Loss: 10.112523078918457\n",
            "Epoch: 1150 | Training Loss: 8.987174034118652 | Testing Loss: 10.099785804748535\n",
            "Epoch: 1160 | Training Loss: 8.975007057189941 | Testing Loss: 10.087048530578613\n",
            "Epoch: 1170 | Training Loss: 8.96284008026123 | Testing Loss: 10.074312210083008\n",
            "Epoch: 1180 | Training Loss: 8.95067310333252 | Testing Loss: 10.06157398223877\n",
            "Epoch: 1190 | Training Loss: 8.938506126403809 | Testing Loss: 10.048837661743164\n",
            "Epoch: 1200 | Training Loss: 8.926340103149414 | Testing Loss: 10.036100387573242\n",
            "Epoch: 1210 | Training Loss: 8.914173126220703 | Testing Loss: 10.02336311340332\n",
            "Epoch: 1220 | Training Loss: 8.902007102966309 | Testing Loss: 10.010626792907715\n",
            "Epoch: 1230 | Training Loss: 8.889839172363281 | Testing Loss: 9.997889518737793\n",
            "Epoch: 1240 | Training Loss: 8.87767219543457 | Testing Loss: 9.985152244567871\n",
            "Epoch: 1250 | Training Loss: 8.865506172180176 | Testing Loss: 9.97241497039795\n",
            "Epoch: 1260 | Training Loss: 8.853339195251465 | Testing Loss: 9.959678649902344\n",
            "Epoch: 1270 | Training Loss: 8.84117317199707 | Testing Loss: 9.946940422058105\n",
            "Epoch: 1280 | Training Loss: 8.829005241394043 | Testing Loss: 9.9342041015625\n",
            "Epoch: 1290 | Training Loss: 8.816839218139648 | Testing Loss: 9.921467781066895\n",
            "Epoch: 1300 | Training Loss: 8.804671287536621 | Testing Loss: 9.908729553222656\n",
            "Epoch: 1310 | Training Loss: 8.792505264282227 | Testing Loss: 9.89599323272705\n",
            "Epoch: 1320 | Training Loss: 8.780338287353516 | Testing Loss: 9.883256912231445\n",
            "Epoch: 1330 | Training Loss: 8.768172264099121 | Testing Loss: 9.870518684387207\n",
            "Epoch: 1340 | Training Loss: 8.75600528717041 | Testing Loss: 9.857782363891602\n",
            "Epoch: 1350 | Training Loss: 8.7438383102417 | Testing Loss: 9.84504508972168\n",
            "Epoch: 1360 | Training Loss: 8.731671333312988 | Testing Loss: 9.832308769226074\n",
            "Epoch: 1370 | Training Loss: 8.719504356384277 | Testing Loss: 9.819571495056152\n",
            "Epoch: 1380 | Training Loss: 8.707338333129883 | Testing Loss: 9.80683422088623\n",
            "Epoch: 1390 | Training Loss: 8.695171356201172 | Testing Loss: 9.794096946716309\n",
            "Epoch: 1400 | Training Loss: 8.683005332946777 | Testing Loss: 9.781359672546387\n",
            "Epoch: 1410 | Training Loss: 8.67083740234375 | Testing Loss: 9.768623352050781\n",
            "Epoch: 1420 | Training Loss: 8.658670425415039 | Testing Loss: 9.755887031555176\n",
            "Epoch: 1430 | Training Loss: 8.646504402160645 | Testing Loss: 9.743148803710938\n",
            "Epoch: 1440 | Training Loss: 8.634337425231934 | Testing Loss: 9.730412483215332\n",
            "Epoch: 1450 | Training Loss: 8.622170448303223 | Testing Loss: 9.717674255371094\n",
            "Epoch: 1460 | Training Loss: 8.610003471374512 | Testing Loss: 9.704937934875488\n",
            "Epoch: 1470 | Training Loss: 8.597837448120117 | Testing Loss: 9.692200660705566\n",
            "Epoch: 1480 | Training Loss: 8.585670471191406 | Testing Loss: 9.679465293884277\n",
            "Epoch: 1490 | Training Loss: 8.573503494262695 | Testing Loss: 9.666727066040039\n",
            "Epoch: 1500 | Training Loss: 8.561336517333984 | Testing Loss: 9.653990745544434\n",
            "Epoch: 1510 | Training Loss: 8.54917049407959 | Testing Loss: 9.641252517700195\n",
            "Epoch: 1520 | Training Loss: 8.537002563476562 | Testing Loss: 9.62851619720459\n",
            "Epoch: 1530 | Training Loss: 8.524836540222168 | Testing Loss: 9.615778923034668\n",
            "Epoch: 1540 | Training Loss: 8.512669563293457 | Testing Loss: 9.603041648864746\n",
            "Epoch: 1550 | Training Loss: 8.500502586364746 | Testing Loss: 9.590304374694824\n",
            "Epoch: 1560 | Training Loss: 8.488336563110352 | Testing Loss: 9.577568054199219\n",
            "Epoch: 1570 | Training Loss: 8.476168632507324 | Testing Loss: 9.564830780029297\n",
            "Epoch: 1580 | Training Loss: 8.464003562927246 | Testing Loss: 9.552093505859375\n",
            "Epoch: 1590 | Training Loss: 8.451835632324219 | Testing Loss: 9.53935718536377\n",
            "Epoch: 1600 | Training Loss: 8.439668655395508 | Testing Loss: 9.526619911193848\n",
            "Epoch: 1610 | Training Loss: 8.427502632141113 | Testing Loss: 9.513882637023926\n",
            "Epoch: 1620 | Training Loss: 8.415335655212402 | Testing Loss: 9.50114631652832\n",
            "Epoch: 1630 | Training Loss: 8.403167724609375 | Testing Loss: 9.488409042358398\n",
            "Epoch: 1640 | Training Loss: 8.39100170135498 | Testing Loss: 9.475671768188477\n",
            "Epoch: 1650 | Training Loss: 8.378835678100586 | Testing Loss: 9.462935447692871\n",
            "Epoch: 1660 | Training Loss: 8.366668701171875 | Testing Loss: 9.45019817352295\n",
            "Epoch: 1670 | Training Loss: 8.354501724243164 | Testing Loss: 9.437460899353027\n",
            "Epoch: 1680 | Training Loss: 8.34233570098877 | Testing Loss: 9.424724578857422\n",
            "Epoch: 1690 | Training Loss: 8.330168724060059 | Testing Loss: 9.411986351013184\n",
            "Epoch: 1700 | Training Loss: 8.318000793457031 | Testing Loss: 9.399249076843262\n",
            "Epoch: 1710 | Training Loss: 8.305834770202637 | Testing Loss: 9.386513710021973\n",
            "Epoch: 1720 | Training Loss: 8.293667793273926 | Testing Loss: 9.37377643585205\n",
            "Epoch: 1730 | Training Loss: 8.281500816345215 | Testing Loss: 9.361038208007812\n",
            "Epoch: 1740 | Training Loss: 8.26933479309082 | Testing Loss: 9.348301887512207\n",
            "Epoch: 1750 | Training Loss: 8.257166862487793 | Testing Loss: 9.335565567016602\n",
            "Epoch: 1760 | Training Loss: 8.245000839233398 | Testing Loss: 9.32282829284668\n",
            "Epoch: 1770 | Training Loss: 8.232833862304688 | Testing Loss: 9.310091972351074\n",
            "Epoch: 1780 | Training Loss: 8.220666885375977 | Testing Loss: 9.297354698181152\n",
            "Epoch: 1790 | Training Loss: 8.208500862121582 | Testing Loss: 9.28461742401123\n",
            "Epoch: 1800 | Training Loss: 8.196333885192871 | Testing Loss: 9.271880149841309\n",
            "Epoch: 1810 | Training Loss: 8.184165954589844 | Testing Loss: 9.259143829345703\n",
            "Epoch: 1820 | Training Loss: 8.17199993133545 | Testing Loss: 9.246405601501465\n",
            "Epoch: 1830 | Training Loss: 8.159833908081055 | Testing Loss: 9.233668327331543\n",
            "Epoch: 1840 | Training Loss: 8.147666931152344 | Testing Loss: 9.220932006835938\n",
            "Epoch: 1850 | Training Loss: 8.135499954223633 | Testing Loss: 9.208195686340332\n",
            "Epoch: 1860 | Training Loss: 8.123332977294922 | Testing Loss: 9.19545841217041\n",
            "Epoch: 1870 | Training Loss: 8.111166954040527 | Testing Loss: 9.182721138000488\n",
            "Epoch: 1880 | Training Loss: 8.098999977111816 | Testing Loss: 9.169984817504883\n",
            "Epoch: 1890 | Training Loss: 8.086833000183105 | Testing Loss: 9.157247543334961\n",
            "Epoch: 1900 | Training Loss: 8.074666023254395 | Testing Loss: 9.144510269165039\n",
            "Epoch: 1910 | Training Loss: 8.062499046325684 | Testing Loss: 9.131772994995117\n",
            "Epoch: 1920 | Training Loss: 8.050331115722656 | Testing Loss: 9.119034767150879\n",
            "Epoch: 1930 | Training Loss: 8.038164138793945 | Testing Loss: 9.106297492980957\n",
            "Epoch: 1940 | Training Loss: 8.025997161865234 | Testing Loss: 9.093559265136719\n",
            "Epoch: 1950 | Training Loss: 8.01382827758789 | Testing Loss: 9.08082103729248\n",
            "Epoch: 1960 | Training Loss: 8.001662254333496 | Testing Loss: 9.068083763122559\n",
            "Epoch: 1970 | Training Loss: 7.989494323730469 | Testing Loss: 9.055346488952637\n",
            "Epoch: 1980 | Training Loss: 7.9773268699646 | Testing Loss: 9.042609214782715\n",
            "Epoch: 1990 | Training Loss: 7.965159893035889 | Testing Loss: 9.029870986938477\n",
            "Epoch: 2000 | Training Loss: 7.9529924392700195 | Testing Loss: 9.017133712768555\n",
            "Epoch: 2010 | Training Loss: 7.94082498550415 | Testing Loss: 9.004396438598633\n",
            "Epoch: 2020 | Training Loss: 7.928657531738281 | Testing Loss: 8.991658210754395\n",
            "Epoch: 2030 | Training Loss: 7.916489601135254 | Testing Loss: 8.978919982910156\n",
            "Epoch: 2040 | Training Loss: 7.904322147369385 | Testing Loss: 8.966182708740234\n",
            "Epoch: 2050 | Training Loss: 7.892154693603516 | Testing Loss: 8.953445434570312\n",
            "Epoch: 2060 | Training Loss: 7.879986763000488 | Testing Loss: 8.940707206726074\n",
            "Epoch: 2070 | Training Loss: 7.8678202629089355 | Testing Loss: 8.927969932556152\n",
            "Epoch: 2080 | Training Loss: 7.85565185546875 | Testing Loss: 8.915231704711914\n",
            "Epoch: 2090 | Training Loss: 7.843485355377197 | Testing Loss: 8.902494430541992\n",
            "Epoch: 2100 | Training Loss: 7.83131742477417 | Testing Loss: 8.88975715637207\n",
            "Epoch: 2110 | Training Loss: 7.819150447845459 | Testing Loss: 8.877018928527832\n",
            "Epoch: 2120 | Training Loss: 7.806982517242432 | Testing Loss: 8.864280700683594\n",
            "Epoch: 2130 | Training Loss: 7.794816017150879 | Testing Loss: 8.851544380187988\n",
            "Epoch: 2140 | Training Loss: 7.782647609710693 | Testing Loss: 8.838805198669434\n",
            "Epoch: 2150 | Training Loss: 7.770480632781982 | Testing Loss: 8.826067924499512\n",
            "Epoch: 2160 | Training Loss: 7.758313179016113 | Testing Loss: 8.81333065032959\n",
            "Epoch: 2170 | Training Loss: 7.746145725250244 | Testing Loss: 8.800593376159668\n",
            "Epoch: 2180 | Training Loss: 7.733978271484375 | Testing Loss: 8.787856101989746\n",
            "Epoch: 2190 | Training Loss: 7.721810817718506 | Testing Loss: 8.775117874145508\n",
            "Epoch: 2200 | Training Loss: 7.709643840789795 | Testing Loss: 8.76237964630127\n",
            "Epoch: 2210 | Training Loss: 7.697476387023926 | Testing Loss: 8.749641418457031\n",
            "Epoch: 2220 | Training Loss: 7.68530797958374 | Testing Loss: 8.736905097961426\n",
            "Epoch: 2230 | Training Loss: 7.6731414794921875 | Testing Loss: 8.724166870117188\n",
            "Epoch: 2240 | Training Loss: 7.66097354888916 | Testing Loss: 8.71142864227295\n",
            "Epoch: 2250 | Training Loss: 7.648807048797607 | Testing Loss: 8.698692321777344\n",
            "Epoch: 2260 | Training Loss: 7.636638641357422 | Testing Loss: 8.685954093933105\n",
            "Epoch: 2270 | Training Loss: 7.624471187591553 | Testing Loss: 8.673215866088867\n",
            "Epoch: 2280 | Training Loss: 7.612304210662842 | Testing Loss: 8.660478591918945\n",
            "Epoch: 2290 | Training Loss: 7.600135803222656 | Testing Loss: 8.647741317749023\n",
            "Epoch: 2300 | Training Loss: 7.5879693031311035 | Testing Loss: 8.635003089904785\n",
            "Epoch: 2310 | Training Loss: 7.575801372528076 | Testing Loss: 8.622265815734863\n",
            "Epoch: 2320 | Training Loss: 7.563634395599365 | Testing Loss: 8.609527587890625\n",
            "Epoch: 2330 | Training Loss: 7.551466464996338 | Testing Loss: 8.596789360046387\n",
            "Epoch: 2340 | Training Loss: 7.539299964904785 | Testing Loss: 8.584052085876465\n",
            "Epoch: 2350 | Training Loss: 7.5271315574646 | Testing Loss: 8.571314811706543\n",
            "Epoch: 2360 | Training Loss: 7.514965057373047 | Testing Loss: 8.558577537536621\n",
            "Epoch: 2370 | Training Loss: 7.5027971267700195 | Testing Loss: 8.545839309692383\n",
            "Epoch: 2380 | Training Loss: 7.49062967300415 | Testing Loss: 8.533101081848145\n",
            "Epoch: 2390 | Training Loss: 7.478462219238281 | Testing Loss: 8.520364761352539\n",
            "Epoch: 2400 | Training Loss: 7.466294765472412 | Testing Loss: 8.5076265335083\n",
            "Epoch: 2410 | Training Loss: 7.454127788543701 | Testing Loss: 8.494888305664062\n",
            "Epoch: 2420 | Training Loss: 7.441960334777832 | Testing Loss: 8.48215103149414\n",
            "Epoch: 2430 | Training Loss: 7.429792881011963 | Testing Loss: 8.469412803649902\n",
            "Epoch: 2440 | Training Loss: 7.417625427246094 | Testing Loss: 8.45667552947998\n",
            "Epoch: 2450 | Training Loss: 7.405457496643066 | Testing Loss: 8.443938255310059\n",
            "Epoch: 2460 | Training Loss: 7.393290996551514 | Testing Loss: 8.43120002746582\n",
            "Epoch: 2470 | Training Loss: 7.381122589111328 | Testing Loss: 8.418463706970215\n",
            "Epoch: 2480 | Training Loss: 7.368955135345459 | Testing Loss: 8.405725479125977\n",
            "Epoch: 2490 | Training Loss: 7.356788158416748 | Testing Loss: 8.392987251281738\n",
            "Epoch: 2500 | Training Loss: 7.344621181488037 | Testing Loss: 8.3802490234375\n",
            "Epoch: 2510 | Training Loss: 7.33245325088501 | Testing Loss: 8.367511749267578\n",
            "Epoch: 2520 | Training Loss: 7.320285320281982 | Testing Loss: 8.354774475097656\n",
            "Epoch: 2530 | Training Loss: 7.3081183433532715 | Testing Loss: 8.342036247253418\n",
            "Epoch: 2540 | Training Loss: 7.295950412750244 | Testing Loss: 8.329298973083496\n",
            "Epoch: 2550 | Training Loss: 7.283783912658691 | Testing Loss: 8.316561698913574\n",
            "Epoch: 2560 | Training Loss: 7.271615505218506 | Testing Loss: 8.303823471069336\n",
            "Epoch: 2570 | Training Loss: 7.259449005126953 | Testing Loss: 8.291086196899414\n",
            "Epoch: 2580 | Training Loss: 7.247281074523926 | Testing Loss: 8.278347969055176\n",
            "Epoch: 2590 | Training Loss: 7.235113620758057 | Testing Loss: 8.265609741210938\n",
            "Epoch: 2600 | Training Loss: 7.2229461669921875 | Testing Loss: 8.252872467041016\n",
            "Epoch: 2610 | Training Loss: 7.210778713226318 | Testing Loss: 8.240135192871094\n",
            "Epoch: 2620 | Training Loss: 7.198611736297607 | Testing Loss: 8.227396965026855\n",
            "Epoch: 2630 | Training Loss: 7.186444282531738 | Testing Loss: 8.214659690856934\n",
            "Epoch: 2640 | Training Loss: 7.174276828765869 | Testing Loss: 8.201922416687012\n",
            "Epoch: 2650 | Training Loss: 7.162109375 | Testing Loss: 8.18918514251709\n",
            "Epoch: 2660 | Training Loss: 7.149941921234131 | Testing Loss: 8.176446914672852\n",
            "Epoch: 2670 | Training Loss: 7.13777494430542 | Testing Loss: 8.16370964050293\n",
            "Epoch: 2680 | Training Loss: 7.125606536865234 | Testing Loss: 8.150971412658691\n",
            "Epoch: 2690 | Training Loss: 7.113440036773682 | Testing Loss: 8.13823413848877\n",
            "Epoch: 2700 | Training Loss: 7.101272106170654 | Testing Loss: 8.125495910644531\n",
            "Epoch: 2710 | Training Loss: 7.089105129241943 | Testing Loss: 8.11275863647461\n",
            "Epoch: 2720 | Training Loss: 7.076937198638916 | Testing Loss: 8.100020408630371\n",
            "Epoch: 2730 | Training Loss: 7.064769744873047 | Testing Loss: 8.08728313446045\n",
            "Epoch: 2740 | Training Loss: 7.052602291107178 | Testing Loss: 8.074545860290527\n",
            "Epoch: 2750 | Training Loss: 7.04043436050415 | Testing Loss: 8.061807632446289\n",
            "Epoch: 2760 | Training Loss: 7.028267860412598 | Testing Loss: 8.04906940460205\n",
            "Epoch: 2770 | Training Loss: 7.0161004066467285 | Testing Loss: 8.036333084106445\n",
            "Epoch: 2780 | Training Loss: 7.003932952880859 | Testing Loss: 8.02359390258789\n",
            "Epoch: 2790 | Training Loss: 6.991765022277832 | Testing Loss: 8.010856628417969\n",
            "Epoch: 2800 | Training Loss: 6.979598522186279 | Testing Loss: 7.998118877410889\n",
            "Epoch: 2810 | Training Loss: 6.967430114746094 | Testing Loss: 7.985381603240967\n",
            "Epoch: 2820 | Training Loss: 6.955262660980225 | Testing Loss: 7.9726433753967285\n",
            "Epoch: 2830 | Training Loss: 6.943095684051514 | Testing Loss: 7.959906101226807\n",
            "Epoch: 2840 | Training Loss: 6.9309282302856445 | Testing Loss: 7.947167873382568\n",
            "Epoch: 2850 | Training Loss: 6.918760776519775 | Testing Loss: 7.9344305992126465\n",
            "Epoch: 2860 | Training Loss: 6.906593322753906 | Testing Loss: 7.921693325042725\n",
            "Epoch: 2870 | Training Loss: 6.894425868988037 | Testing Loss: 7.9089555740356445\n",
            "Epoch: 2880 | Training Loss: 6.882258892059326 | Testing Loss: 7.896217346191406\n",
            "Epoch: 2890 | Training Loss: 6.870091438293457 | Testing Loss: 7.883479595184326\n",
            "Epoch: 2900 | Training Loss: 6.857923984527588 | Testing Loss: 7.870742321014404\n",
            "Epoch: 2910 | Training Loss: 6.845756530761719 | Testing Loss: 7.858005046844482\n",
            "Epoch: 2920 | Training Loss: 6.833590030670166 | Testing Loss: 7.845268249511719\n",
            "Epoch: 2930 | Training Loss: 6.8214240074157715 | Testing Loss: 7.832531929016113\n",
            "Epoch: 2940 | Training Loss: 6.809257507324219 | Testing Loss: 7.81979513168335\n",
            "Epoch: 2950 | Training Loss: 6.797091960906982 | Testing Loss: 7.8070597648620605\n",
            "Epoch: 2960 | Training Loss: 6.784925937652588 | Testing Loss: 7.794322967529297\n",
            "Epoch: 2970 | Training Loss: 6.772759437561035 | Testing Loss: 7.781585693359375\n",
            "Epoch: 2980 | Training Loss: 6.760592937469482 | Testing Loss: 7.7688493728637695\n",
            "Epoch: 2990 | Training Loss: 6.748426914215088 | Testing Loss: 7.756112575531006\n",
            "Epoch: 3000 | Training Loss: 6.736260414123535 | Testing Loss: 7.743377208709717\n",
            "Epoch: 3010 | Training Loss: 6.724094390869141 | Testing Loss: 7.730640411376953\n",
            "Epoch: 3020 | Training Loss: 6.711927890777588 | Testing Loss: 7.717904090881348\n",
            "Epoch: 3030 | Training Loss: 6.699761867523193 | Testing Loss: 7.705167293548584\n",
            "Epoch: 3040 | Training Loss: 6.687595367431641 | Testing Loss: 7.6924309730529785\n",
            "Epoch: 3050 | Training Loss: 6.675429821014404 | Testing Loss: 7.679694652557373\n",
            "Epoch: 3060 | Training Loss: 6.663262844085693 | Testing Loss: 7.666957378387451\n",
            "Epoch: 3070 | Training Loss: 6.651097297668457 | Testing Loss: 7.654221534729004\n",
            "Epoch: 3080 | Training Loss: 6.638930797576904 | Testing Loss: 7.641485691070557\n",
            "Epoch: 3090 | Training Loss: 6.626763820648193 | Testing Loss: 7.628748416900635\n",
            "Epoch: 3100 | Training Loss: 6.614598274230957 | Testing Loss: 7.6160125732421875\n",
            "Epoch: 3110 | Training Loss: 6.6024322509765625 | Testing Loss: 7.603276252746582\n",
            "Epoch: 3120 | Training Loss: 6.590266704559326 | Testing Loss: 7.59053897857666\n",
            "Epoch: 3130 | Training Loss: 6.578099250793457 | Testing Loss: 7.5778021812438965\n",
            "Epoch: 3140 | Training Loss: 6.5659332275390625 | Testing Loss: 7.565065860748291\n",
            "Epoch: 3150 | Training Loss: 6.553767681121826 | Testing Loss: 7.552330017089844\n",
            "Epoch: 3160 | Training Loss: 6.541600704193115 | Testing Loss: 7.539593696594238\n",
            "Epoch: 3170 | Training Loss: 6.5294342041015625 | Testing Loss: 7.526856899261475\n",
            "Epoch: 3180 | Training Loss: 6.517268657684326 | Testing Loss: 7.514120578765869\n",
            "Epoch: 3190 | Training Loss: 6.505102634429932 | Testing Loss: 7.501383304595947\n",
            "Epoch: 3200 | Training Loss: 6.492936134338379 | Testing Loss: 7.4886474609375\n",
            "Epoch: 3210 | Training Loss: 6.480769634246826 | Testing Loss: 7.4759111404418945\n",
            "Epoch: 3220 | Training Loss: 6.468603610992432 | Testing Loss: 7.463174343109131\n",
            "Epoch: 3230 | Training Loss: 6.456437110900879 | Testing Loss: 7.450438022613525\n",
            "Epoch: 3240 | Training Loss: 6.444270610809326 | Testing Loss: 7.43770170211792\n",
            "Epoch: 3250 | Training Loss: 6.432104587554932 | Testing Loss: 7.424964904785156\n",
            "Epoch: 3260 | Training Loss: 6.419938564300537 | Testing Loss: 7.412228584289551\n",
            "Epoch: 3270 | Training Loss: 6.407773017883301 | Testing Loss: 7.399491786956787\n",
            "Epoch: 3280 | Training Loss: 6.39560604095459 | Testing Loss: 7.386755466461182\n",
            "Epoch: 3290 | Training Loss: 6.383439540863037 | Testing Loss: 7.374019145965576\n",
            "Epoch: 3300 | Training Loss: 6.371273517608643 | Testing Loss: 7.3612823486328125\n",
            "Epoch: 3310 | Training Loss: 6.359107494354248 | Testing Loss: 7.348546028137207\n",
            "Epoch: 3320 | Training Loss: 6.346940994262695 | Testing Loss: 7.335809230804443\n",
            "Epoch: 3330 | Training Loss: 6.334774971008301 | Testing Loss: 7.323072910308838\n",
            "Epoch: 3340 | Training Loss: 6.322608947753906 | Testing Loss: 7.310336589813232\n",
            "Epoch: 3350 | Training Loss: 6.3104424476623535 | Testing Loss: 7.297599792480469\n",
            "Epoch: 3360 | Training Loss: 6.298276424407959 | Testing Loss: 7.284863471984863\n",
            "Epoch: 3370 | Training Loss: 6.286109924316406 | Testing Loss: 7.2721266746521\n",
            "Epoch: 3380 | Training Loss: 6.2739434242248535 | Testing Loss: 7.259390354156494\n",
            "Epoch: 3390 | Training Loss: 6.261777400970459 | Testing Loss: 7.246654033660889\n",
            "Epoch: 3400 | Training Loss: 6.249610900878906 | Testing Loss: 7.233917236328125\n",
            "Epoch: 3410 | Training Loss: 6.23744535446167 | Testing Loss: 7.2211809158325195\n",
            "Epoch: 3420 | Training Loss: 6.225279331207275 | Testing Loss: 7.208444118499756\n",
            "Epoch: 3430 | Training Loss: 6.2131123542785645 | Testing Loss: 7.19570779800415\n",
            "Epoch: 3440 | Training Loss: 6.20094633102417 | Testing Loss: 7.182971477508545\n",
            "Epoch: 3450 | Training Loss: 6.188780307769775 | Testing Loss: 7.170234680175781\n",
            "Epoch: 3460 | Training Loss: 6.176613807678223 | Testing Loss: 7.157498359680176\n",
            "Epoch: 3470 | Training Loss: 6.16444730758667 | Testing Loss: 7.144761562347412\n",
            "Epoch: 3480 | Training Loss: 6.152281284332275 | Testing Loss: 7.132025241851807\n",
            "Epoch: 3490 | Training Loss: 6.140115261077881 | Testing Loss: 7.119288921356201\n",
            "Epoch: 3500 | Training Loss: 6.127948760986328 | Testing Loss: 7.106553077697754\n",
            "Epoch: 3510 | Training Loss: 6.115782260894775 | Testing Loss: 7.09381628036499\n",
            "Epoch: 3520 | Training Loss: 6.103616237640381 | Testing Loss: 7.081079006195068\n",
            "Epoch: 3530 | Training Loss: 6.0914506912231445 | Testing Loss: 7.068343639373779\n",
            "Epoch: 3540 | Training Loss: 6.079284191131592 | Testing Loss: 7.055606365203857\n",
            "Epoch: 3550 | Training Loss: 6.067117214202881 | Testing Loss: 7.042869567871094\n",
            "Epoch: 3560 | Training Loss: 6.0549516677856445 | Testing Loss: 7.030133247375488\n",
            "Epoch: 3570 | Training Loss: 6.042785167694092 | Testing Loss: 7.017396450042725\n",
            "Epoch: 3580 | Training Loss: 6.030619144439697 | Testing Loss: 7.0046610832214355\n",
            "Epoch: 3590 | Training Loss: 6.0184526443481445 | Testing Loss: 6.991924285888672\n",
            "Epoch: 3600 | Training Loss: 6.00628662109375 | Testing Loss: 6.979187965393066\n",
            "Epoch: 3610 | Training Loss: 5.9941205978393555 | Testing Loss: 6.9664506912231445\n",
            "Epoch: 3620 | Training Loss: 5.981954097747803 | Testing Loss: 6.953713893890381\n",
            "Epoch: 3630 | Training Loss: 5.96978759765625 | Testing Loss: 6.940978527069092\n",
            "Epoch: 3640 | Training Loss: 5.957622051239014 | Testing Loss: 6.928241729736328\n",
            "Epoch: 3650 | Training Loss: 5.945455074310303 | Testing Loss: 6.915505409240723\n",
            "Epoch: 3660 | Training Loss: 5.933289527893066 | Testing Loss: 6.902768611907959\n",
            "Epoch: 3670 | Training Loss: 5.921123027801514 | Testing Loss: 6.8900322914123535\n",
            "Epoch: 3680 | Training Loss: 5.908956527709961 | Testing Loss: 6.877295017242432\n",
            "Epoch: 3690 | Training Loss: 5.896790504455566 | Testing Loss: 6.864559173583984\n",
            "Epoch: 3700 | Training Loss: 5.884624004364014 | Testing Loss: 6.851822853088379\n",
            "Epoch: 3710 | Training Loss: 5.872457981109619 | Testing Loss: 6.839086055755615\n",
            "Epoch: 3720 | Training Loss: 5.860291957855225 | Testing Loss: 6.826350688934326\n",
            "Epoch: 3730 | Training Loss: 5.848124980926514 | Testing Loss: 6.8136138916015625\n",
            "Epoch: 3740 | Training Loss: 5.835959434509277 | Testing Loss: 6.800877571105957\n",
            "Epoch: 3750 | Training Loss: 5.823792934417725 | Testing Loss: 6.788140773773193\n",
            "Epoch: 3760 | Training Loss: 5.81162691116333 | Testing Loss: 6.7754034996032715\n",
            "Epoch: 3770 | Training Loss: 5.7994608879089355 | Testing Loss: 6.762667179107666\n",
            "Epoch: 3780 | Training Loss: 5.787293910980225 | Testing Loss: 6.7499308586120605\n",
            "Epoch: 3790 | Training Loss: 5.775128364562988 | Testing Loss: 6.737195014953613\n",
            "Epoch: 3800 | Training Loss: 5.7629618644714355 | Testing Loss: 6.72445821762085\n",
            "Epoch: 3810 | Training Loss: 5.750795364379883 | Testing Loss: 6.711721897125244\n",
            "Epoch: 3820 | Training Loss: 5.73862886428833 | Testing Loss: 6.698985576629639\n",
            "Epoch: 3830 | Training Loss: 5.7264628410339355 | Testing Loss: 6.686248779296875\n",
            "Epoch: 3840 | Training Loss: 5.714296817779541 | Testing Loss: 6.6735124588012695\n",
            "Epoch: 3850 | Training Loss: 5.7021307945251465 | Testing Loss: 6.660775661468506\n",
            "Epoch: 3860 | Training Loss: 5.689964294433594 | Testing Loss: 6.6480393409729\n",
            "Epoch: 3870 | Training Loss: 5.677798271179199 | Testing Loss: 6.635303020477295\n",
            "Epoch: 3880 | Training Loss: 5.6656317710876465 | Testing Loss: 6.622566223144531\n",
            "Epoch: 3890 | Training Loss: 5.653465270996094 | Testing Loss: 6.609829902648926\n",
            "Epoch: 3900 | Training Loss: 5.641299724578857 | Testing Loss: 6.597093105316162\n",
            "Epoch: 3910 | Training Loss: 5.629133224487305 | Testing Loss: 6.584356784820557\n",
            "Epoch: 3920 | Training Loss: 5.616966724395752 | Testing Loss: 6.571620464324951\n",
            "Epoch: 3930 | Training Loss: 5.604800701141357 | Testing Loss: 6.5588836669921875\n",
            "Epoch: 3940 | Training Loss: 5.592634677886963 | Testing Loss: 6.546147346496582\n",
            "Epoch: 3950 | Training Loss: 5.580468654632568 | Testing Loss: 6.533410549163818\n",
            "Epoch: 3960 | Training Loss: 5.568302154541016 | Testing Loss: 6.520674228668213\n",
            "Epoch: 3970 | Training Loss: 5.556135654449463 | Testing Loss: 6.507937908172607\n",
            "Epoch: 3980 | Training Loss: 5.54396915435791 | Testing Loss: 6.495201110839844\n",
            "Epoch: 3990 | Training Loss: 5.531803607940674 | Testing Loss: 6.482464790344238\n",
            "Epoch: 4000 | Training Loss: 5.519637107849121 | Testing Loss: 6.469727993011475\n",
            "Epoch: 4010 | Training Loss: 5.507471084594727 | Testing Loss: 6.456991672515869\n",
            "Epoch: 4020 | Training Loss: 5.495304584503174 | Testing Loss: 6.444255352020264\n",
            "Epoch: 4030 | Training Loss: 5.483138561248779 | Testing Loss: 6.4315185546875\n",
            "Epoch: 4040 | Training Loss: 5.470972537994385 | Testing Loss: 6.4187822341918945\n",
            "Epoch: 4050 | Training Loss: 5.458806037902832 | Testing Loss: 6.406045436859131\n",
            "Epoch: 4060 | Training Loss: 5.446639537811279 | Testing Loss: 6.393309593200684\n",
            "Epoch: 4070 | Training Loss: 5.434473514556885 | Testing Loss: 6.38057279586792\n",
            "Epoch: 4080 | Training Loss: 5.422307014465332 | Testing Loss: 6.367835998535156\n",
            "Epoch: 4090 | Training Loss: 5.4101409912109375 | Testing Loss: 6.355099678039551\n",
            "Epoch: 4100 | Training Loss: 5.397974491119385 | Testing Loss: 6.342363357543945\n",
            "Epoch: 4110 | Training Loss: 5.38580846786499 | Testing Loss: 6.32962703704834\n",
            "Epoch: 4120 | Training Loss: 5.373642444610596 | Testing Loss: 6.316890239715576\n",
            "Epoch: 4130 | Training Loss: 5.361476421356201 | Testing Loss: 6.304153919219971\n",
            "Epoch: 4140 | Training Loss: 5.34930944442749 | Testing Loss: 6.291417121887207\n",
            "Epoch: 4150 | Training Loss: 5.337143898010254 | Testing Loss: 6.278680801391602\n",
            "Epoch: 4160 | Training Loss: 5.324977397918701 | Testing Loss: 6.265944480895996\n",
            "Epoch: 4170 | Training Loss: 5.312810897827148 | Testing Loss: 6.253208160400391\n",
            "Epoch: 4180 | Training Loss: 5.300644874572754 | Testing Loss: 6.240471363067627\n",
            "Epoch: 4190 | Training Loss: 5.288478851318359 | Testing Loss: 6.2277350425720215\n",
            "Epoch: 4200 | Training Loss: 5.276312351226807 | Testing Loss: 6.214998245239258\n",
            "Epoch: 4210 | Training Loss: 5.264146327972412 | Testing Loss: 6.202261924743652\n",
            "Epoch: 4220 | Training Loss: 5.251979827880859 | Testing Loss: 6.189525604248047\n",
            "Epoch: 4230 | Training Loss: 5.239814281463623 | Testing Loss: 6.176789283752441\n",
            "Epoch: 4240 | Training Loss: 5.227647304534912 | Testing Loss: 6.164052486419678\n",
            "Epoch: 4250 | Training Loss: 5.215481281280518 | Testing Loss: 6.151316165924072\n",
            "Epoch: 4260 | Training Loss: 5.203315258026123 | Testing Loss: 6.138579368591309\n",
            "Epoch: 4270 | Training Loss: 5.191148281097412 | Testing Loss: 6.125843048095703\n",
            "Epoch: 4280 | Training Loss: 5.178982734680176 | Testing Loss: 6.113106727600098\n",
            "Epoch: 4290 | Training Loss: 5.166816234588623 | Testing Loss: 6.100369930267334\n",
            "Epoch: 4300 | Training Loss: 5.1546502113342285 | Testing Loss: 6.0876336097717285\n",
            "Epoch: 4310 | Training Loss: 5.142483711242676 | Testing Loss: 6.074897289276123\n",
            "Epoch: 4320 | Training Loss: 5.130317687988281 | Testing Loss: 6.062160015106201\n",
            "Epoch: 4330 | Training Loss: 5.1181511878967285 | Testing Loss: 6.049424171447754\n",
            "Epoch: 4340 | Training Loss: 5.105985164642334 | Testing Loss: 6.03668737411499\n",
            "Epoch: 4350 | Training Loss: 5.093818664550781 | Testing Loss: 6.023951053619385\n",
            "Epoch: 4360 | Training Loss: 5.081653118133545 | Testing Loss: 6.011214733123779\n",
            "Epoch: 4370 | Training Loss: 5.069486141204834 | Testing Loss: 5.998478412628174\n",
            "Epoch: 4380 | Training Loss: 5.0573201179504395 | Testing Loss: 5.985742092132568\n",
            "Epoch: 4390 | Training Loss: 5.045154094696045 | Testing Loss: 5.9730048179626465\n",
            "Epoch: 4400 | Training Loss: 5.03298807144165 | Testing Loss: 5.960268497467041\n",
            "Epoch: 4410 | Training Loss: 5.020821571350098 | Testing Loss: 5.947532653808594\n",
            "Epoch: 4420 | Training Loss: 5.008655071258545 | Testing Loss: 5.93479585647583\n",
            "Epoch: 4430 | Training Loss: 4.99648904800415 | Testing Loss: 5.922059535980225\n",
            "Epoch: 4440 | Training Loss: 4.984323024749756 | Testing Loss: 5.909322738647461\n",
            "Epoch: 4450 | Training Loss: 4.972156524658203 | Testing Loss: 5.8965864181518555\n",
            "Epoch: 4460 | Training Loss: 4.95999002456665 | Testing Loss: 5.883849620819092\n",
            "Epoch: 4470 | Training Loss: 4.947824001312256 | Testing Loss: 5.871113300323486\n",
            "Epoch: 4480 | Training Loss: 4.935657501220703 | Testing Loss: 5.858376979827881\n",
            "Epoch: 4490 | Training Loss: 4.923491954803467 | Testing Loss: 5.845640659332275\n",
            "Epoch: 4500 | Training Loss: 4.911324977874756 | Testing Loss: 5.832903861999512\n",
            "Epoch: 4510 | Training Loss: 4.8991594314575195 | Testing Loss: 5.820167064666748\n",
            "Epoch: 4520 | Training Loss: 4.886992931365967 | Testing Loss: 5.807430744171143\n",
            "Epoch: 4530 | Training Loss: 4.874826908111572 | Testing Loss: 5.794694423675537\n",
            "Epoch: 4540 | Training Loss: 4.862660884857178 | Testing Loss: 5.781958103179932\n",
            "Epoch: 4550 | Training Loss: 4.850494384765625 | Testing Loss: 5.769221782684326\n",
            "Epoch: 4560 | Training Loss: 4.838327884674072 | Testing Loss: 5.7564849853515625\n",
            "Epoch: 4570 | Training Loss: 4.826161861419678 | Testing Loss: 5.743748664855957\n",
            "Epoch: 4580 | Training Loss: 4.813995361328125 | Testing Loss: 5.731011867523193\n",
            "Epoch: 4590 | Training Loss: 4.8018293380737305 | Testing Loss: 5.718275547027588\n",
            "Epoch: 4600 | Training Loss: 4.789662837982178 | Testing Loss: 5.705539226531982\n",
            "Epoch: 4610 | Training Loss: 4.777496814727783 | Testing Loss: 5.692802429199219\n",
            "Epoch: 4620 | Training Loss: 4.765330791473389 | Testing Loss: 5.680066108703613\n",
            "Epoch: 4630 | Training Loss: 4.753164291381836 | Testing Loss: 5.66732931137085\n",
            "Epoch: 4640 | Training Loss: 4.740998268127441 | Testing Loss: 5.654592990875244\n",
            "Epoch: 4650 | Training Loss: 4.728831768035889 | Testing Loss: 5.641856670379639\n",
            "Epoch: 4660 | Training Loss: 4.716665744781494 | Testing Loss: 5.629119873046875\n",
            "Epoch: 4670 | Training Loss: 4.7044997215271 | Testing Loss: 5.6163835525512695\n",
            "Epoch: 4680 | Training Loss: 4.692333221435547 | Testing Loss: 5.603647232055664\n",
            "Epoch: 4690 | Training Loss: 4.680166721343994 | Testing Loss: 5.590910911560059\n",
            "Epoch: 4700 | Training Loss: 4.6680006980896 | Testing Loss: 5.578174114227295\n",
            "Epoch: 4710 | Training Loss: 4.655834197998047 | Testing Loss: 5.565437316894531\n",
            "Epoch: 4720 | Training Loss: 4.6436686515808105 | Testing Loss: 5.552700996398926\n",
            "Epoch: 4730 | Training Loss: 4.6315016746521 | Testing Loss: 5.53996467590332\n",
            "Epoch: 4740 | Training Loss: 4.619335651397705 | Testing Loss: 5.527227878570557\n",
            "Epoch: 4750 | Training Loss: 4.6071696281433105 | Testing Loss: 5.514491558074951\n",
            "Epoch: 4760 | Training Loss: 4.595003128051758 | Testing Loss: 5.501755237579346\n",
            "Epoch: 4770 | Training Loss: 4.582837104797363 | Testing Loss: 5.48901891708374\n",
            "Epoch: 4780 | Training Loss: 4.5706706047058105 | Testing Loss: 5.476282596588135\n",
            "Epoch: 4790 | Training Loss: 4.558504581451416 | Testing Loss: 5.463546276092529\n",
            "Epoch: 4800 | Training Loss: 4.5463385581970215 | Testing Loss: 5.450809478759766\n",
            "Epoch: 4810 | Training Loss: 4.534172058105469 | Testing Loss: 5.43807315826416\n",
            "Epoch: 4820 | Training Loss: 4.522005558013916 | Testing Loss: 5.4253363609313965\n",
            "Epoch: 4830 | Training Loss: 4.5098395347595215 | Testing Loss: 5.412599563598633\n",
            "Epoch: 4840 | Training Loss: 4.497673511505127 | Testing Loss: 5.399863243103027\n",
            "Epoch: 4850 | Training Loss: 4.485507488250732 | Testing Loss: 5.387126445770264\n",
            "Epoch: 4860 | Training Loss: 4.47334098815918 | Testing Loss: 5.374390602111816\n",
            "Epoch: 4870 | Training Loss: 4.461174964904785 | Testing Loss: 5.361653804779053\n",
            "Epoch: 4880 | Training Loss: 4.449008464813232 | Testing Loss: 5.348917484283447\n",
            "Epoch: 4890 | Training Loss: 4.436842441558838 | Testing Loss: 5.336180686950684\n",
            "Epoch: 4900 | Training Loss: 4.424675941467285 | Testing Loss: 5.32344388961792\n",
            "Epoch: 4910 | Training Loss: 4.412509441375732 | Testing Loss: 5.310707092285156\n",
            "Epoch: 4920 | Training Loss: 4.40034294128418 | Testing Loss: 5.297971248626709\n",
            "Epoch: 4930 | Training Loss: 4.388176918029785 | Testing Loss: 5.285233974456787\n",
            "Epoch: 4940 | Training Loss: 4.376010417938232 | Testing Loss: 5.272498607635498\n",
            "Epoch: 4950 | Training Loss: 4.363844394683838 | Testing Loss: 5.259761810302734\n",
            "Epoch: 4960 | Training Loss: 4.351678371429443 | Testing Loss: 5.247025012969971\n",
            "Epoch: 4970 | Training Loss: 4.339511871337891 | Testing Loss: 5.234288692474365\n",
            "Epoch: 4980 | Training Loss: 4.327346324920654 | Testing Loss: 5.22155237197876\n",
            "Epoch: 4990 | Training Loss: 4.315179347991943 | Testing Loss: 5.208816051483154\n",
            "Epoch: 5000 | Training Loss: 4.303013324737549 | Testing Loss: 5.196079254150391\n",
            "Epoch: 5010 | Training Loss: 4.290847301483154 | Testing Loss: 5.183342933654785\n",
            "Epoch: 5020 | Training Loss: 4.278680324554443 | Testing Loss: 5.1706061363220215\n",
            "Epoch: 5030 | Training Loss: 4.266514778137207 | Testing Loss: 5.157869815826416\n",
            "Epoch: 5040 | Training Loss: 4.2543487548828125 | Testing Loss: 5.1451334953308105\n",
            "Epoch: 5050 | Training Loss: 4.24218225479126 | Testing Loss: 5.132396697998047\n",
            "Epoch: 5060 | Training Loss: 4.230016231536865 | Testing Loss: 5.119660377502441\n",
            "Epoch: 5070 | Training Loss: 4.2178497314453125 | Testing Loss: 5.106923580169678\n",
            "Epoch: 5080 | Training Loss: 4.20568323135376 | Testing Loss: 5.094187259674072\n",
            "Epoch: 5090 | Training Loss: 4.193517208099365 | Testing Loss: 5.081450939178467\n",
            "Epoch: 5100 | Training Loss: 4.181351184844971 | Testing Loss: 5.068714141845703\n",
            "Epoch: 5110 | Training Loss: 4.169185161590576 | Testing Loss: 5.055977821350098\n",
            "Epoch: 5120 | Training Loss: 4.157018184661865 | Testing Loss: 5.043241024017334\n",
            "Epoch: 5130 | Training Loss: 4.144852638244629 | Testing Loss: 5.0305047035217285\n",
            "Epoch: 5140 | Training Loss: 4.132686138153076 | Testing Loss: 5.017768383026123\n",
            "Epoch: 5150 | Training Loss: 4.120520114898682 | Testing Loss: 5.005032062530518\n",
            "Epoch: 5160 | Training Loss: 4.108353614807129 | Testing Loss: 4.992295742034912\n",
            "Epoch: 5170 | Training Loss: 4.096187114715576 | Testing Loss: 4.97955846786499\n",
            "Epoch: 5180 | Training Loss: 4.084021091461182 | Testing Loss: 4.966822147369385\n",
            "Epoch: 5190 | Training Loss: 4.071855068206787 | Testing Loss: 4.954085826873779\n",
            "Epoch: 5200 | Training Loss: 4.059688568115234 | Testing Loss: 4.941349506378174\n",
            "Epoch: 5210 | Training Loss: 4.047523021697998 | Testing Loss: 4.928613185882568\n",
            "Epoch: 5220 | Training Loss: 4.035356044769287 | Testing Loss: 4.915876388549805\n",
            "Epoch: 5230 | Training Loss: 4.023190021514893 | Testing Loss: 4.903139591217041\n",
            "Epoch: 5240 | Training Loss: 4.011023998260498 | Testing Loss: 4.890403747558594\n",
            "Epoch: 5250 | Training Loss: 3.9988574981689453 | Testing Loss: 4.877667427062988\n",
            "Epoch: 5260 | Training Loss: 3.9866912364959717 | Testing Loss: 4.864930629730225\n",
            "Epoch: 5270 | Training Loss: 3.974525213241577 | Testing Loss: 4.852194309234619\n",
            "Epoch: 5280 | Training Loss: 3.9623589515686035 | Testing Loss: 4.8394575119018555\n",
            "Epoch: 5290 | Training Loss: 3.95019268989563 | Testing Loss: 4.82672119140625\n",
            "Epoch: 5300 | Training Loss: 3.9380264282226562 | Testing Loss: 4.8139848709106445\n",
            "Epoch: 5310 | Training Loss: 3.9258599281311035 | Testing Loss: 4.801248073577881\n",
            "Epoch: 5320 | Training Loss: 3.913694143295288 | Testing Loss: 4.788511753082275\n",
            "Epoch: 5330 | Training Loss: 3.9015274047851562 | Testing Loss: 4.77577543258667\n",
            "Epoch: 5340 | Training Loss: 3.889361619949341 | Testing Loss: 4.763038635253906\n",
            "Epoch: 5350 | Training Loss: 3.877195119857788 | Testing Loss: 4.750302314758301\n",
            "Epoch: 5360 | Training Loss: 3.8650290966033936 | Testing Loss: 4.737565517425537\n",
            "Epoch: 5370 | Training Loss: 3.852862596511841 | Testing Loss: 4.724829196929932\n",
            "Epoch: 5380 | Training Loss: 3.840696334838867 | Testing Loss: 4.712092876434326\n",
            "Epoch: 5390 | Training Loss: 3.8285300731658936 | Testing Loss: 4.6993560791015625\n",
            "Epoch: 5400 | Training Loss: 3.816364049911499 | Testing Loss: 4.686619758605957\n",
            "Epoch: 5410 | Training Loss: 3.8041977882385254 | Testing Loss: 4.673882961273193\n",
            "Epoch: 5420 | Training Loss: 3.79203200340271 | Testing Loss: 4.661146640777588\n",
            "Epoch: 5430 | Training Loss: 3.779865264892578 | Testing Loss: 4.648410320281982\n",
            "Epoch: 5440 | Training Loss: 3.7676990032196045 | Testing Loss: 4.635673522949219\n",
            "Epoch: 5450 | Training Loss: 3.75553297996521 | Testing Loss: 4.622937202453613\n",
            "Epoch: 5460 | Training Loss: 3.7433667182922363 | Testing Loss: 4.610200881958008\n",
            "Epoch: 5470 | Training Loss: 3.7312004566192627 | Testing Loss: 4.597464561462402\n",
            "Epoch: 5480 | Training Loss: 3.71903395652771 | Testing Loss: 4.584727764129639\n",
            "Epoch: 5490 | Training Loss: 3.7068676948547363 | Testing Loss: 4.571990966796875\n",
            "Epoch: 5500 | Training Loss: 3.6947014331817627 | Testing Loss: 4.5592546463012695\n",
            "Epoch: 5510 | Training Loss: 3.682535171508789 | Testing Loss: 4.546517848968506\n",
            "Epoch: 5520 | Training Loss: 3.6703689098358154 | Testing Loss: 4.533782005310059\n",
            "Epoch: 5530 | Training Loss: 3.658203125 | Testing Loss: 4.521045207977295\n",
            "Epoch: 5540 | Training Loss: 3.6460366249084473 | Testing Loss: 4.5083088874816895\n",
            "Epoch: 5550 | Training Loss: 3.6338703632354736 | Testing Loss: 4.495572566986084\n",
            "Epoch: 5560 | Training Loss: 3.6217041015625 | Testing Loss: 4.4828362464904785\n",
            "Epoch: 5570 | Training Loss: 3.6095383167266846 | Testing Loss: 4.470099925994873\n",
            "Epoch: 5580 | Training Loss: 3.597371816635132 | Testing Loss: 4.457363128662109\n",
            "Epoch: 5590 | Training Loss: 3.585205554962158 | Testing Loss: 4.444626808166504\n",
            "Epoch: 5600 | Training Loss: 3.5730392932891846 | Testing Loss: 4.43189001083374\n",
            "Epoch: 5610 | Training Loss: 3.560873031616211 | Testing Loss: 4.419153690338135\n",
            "Epoch: 5620 | Training Loss: 3.5487067699432373 | Testing Loss: 4.406417369842529\n",
            "Epoch: 5630 | Training Loss: 3.5365407466888428 | Testing Loss: 4.393680572509766\n",
            "Epoch: 5640 | Training Loss: 3.524374485015869 | Testing Loss: 4.38094425201416\n",
            "Epoch: 5650 | Training Loss: 3.5122087001800537 | Testing Loss: 4.368207931518555\n",
            "Epoch: 5660 | Training Loss: 3.500042676925659 | Testing Loss: 4.355472087860107\n",
            "Epoch: 5670 | Training Loss: 3.4878768920898438 | Testing Loss: 4.34273624420166\n",
            "Epoch: 5680 | Training Loss: 3.4757115840911865 | Testing Loss: 4.330000877380371\n",
            "Epoch: 5690 | Training Loss: 3.463545560836792 | Testing Loss: 4.317264556884766\n",
            "Epoch: 5700 | Training Loss: 3.4513802528381348 | Testing Loss: 4.304528713226318\n",
            "Epoch: 5710 | Training Loss: 3.4392144680023193 | Testing Loss: 4.291793346405029\n",
            "Epoch: 5720 | Training Loss: 3.427048444747925 | Testing Loss: 4.279057502746582\n",
            "Epoch: 5730 | Training Loss: 3.4148831367492676 | Testing Loss: 4.266321659088135\n",
            "Epoch: 5740 | Training Loss: 3.402717351913452 | Testing Loss: 4.2535858154296875\n",
            "Epoch: 5750 | Training Loss: 3.390551805496216 | Testing Loss: 4.24084997177124\n",
            "Epoch: 5760 | Training Loss: 3.3783860206604004 | Testing Loss: 4.228114604949951\n",
            "Epoch: 5770 | Training Loss: 3.366220235824585 | Testing Loss: 4.215378284454346\n",
            "Epoch: 5780 | Training Loss: 3.3540546894073486 | Testing Loss: 4.202642917633057\n",
            "Epoch: 5790 | Training Loss: 3.341888904571533 | Testing Loss: 4.189907073974609\n",
            "Epoch: 5800 | Training Loss: 3.329723358154297 | Testing Loss: 4.177171230316162\n",
            "Epoch: 5810 | Training Loss: 3.3175575733184814 | Testing Loss: 4.164435863494873\n",
            "Epoch: 5820 | Training Loss: 3.305391788482666 | Testing Loss: 4.151699542999268\n",
            "Epoch: 5830 | Training Loss: 3.2932262420654297 | Testing Loss: 4.13896369934082\n",
            "Epoch: 5840 | Training Loss: 3.2810604572296143 | Testing Loss: 4.126228332519531\n",
            "Epoch: 5850 | Training Loss: 3.268894910812378 | Testing Loss: 4.113492488861084\n",
            "Epoch: 5860 | Training Loss: 3.2567291259765625 | Testing Loss: 4.100756645202637\n",
            "Epoch: 5870 | Training Loss: 3.244563341140747 | Testing Loss: 4.088020324707031\n",
            "Epoch: 5880 | Training Loss: 3.2323977947235107 | Testing Loss: 4.075284957885742\n",
            "Epoch: 5890 | Training Loss: 3.2202320098876953 | Testing Loss: 4.062549114227295\n",
            "Epoch: 5900 | Training Loss: 3.208066701889038 | Testing Loss: 4.049813270568848\n",
            "Epoch: 5910 | Training Loss: 3.196408987045288 | Testing Loss: 4.037377834320068\n",
            "Epoch: 5920 | Training Loss: 3.1852188110351562 | Testing Loss: 4.025142669677734\n",
            "Epoch: 5930 | Training Loss: 3.1740288734436035 | Testing Loss: 4.012907981872559\n",
            "Epoch: 5940 | Training Loss: 3.1628386974334717 | Testing Loss: 4.000672817230225\n",
            "Epoch: 5950 | Training Loss: 3.151648759841919 | Testing Loss: 3.9884374141693115\n",
            "Epoch: 5960 | Training Loss: 3.140458583831787 | Testing Loss: 3.9762024879455566\n",
            "Epoch: 5970 | Training Loss: 3.1292686462402344 | Testing Loss: 3.9639670848846436\n",
            "Epoch: 5980 | Training Loss: 3.1180782318115234 | Testing Loss: 3.9517323970794678\n",
            "Epoch: 5990 | Training Loss: 3.10688853263855 | Testing Loss: 3.939497470855713\n",
            "Epoch: 6000 | Training Loss: 3.095698595046997 | Testing Loss: 3.9272618293762207\n",
            "Epoch: 6010 | Training Loss: 3.0845084190368652 | Testing Loss: 3.915026903152466\n",
            "Epoch: 6020 | Training Loss: 3.0733182430267334 | Testing Loss: 3.902791738510132\n",
            "Epoch: 6030 | Training Loss: 3.0621280670166016 | Testing Loss: 3.890556812286377\n",
            "Epoch: 6040 | Training Loss: 3.050938129425049 | Testing Loss: 3.878321886062622\n",
            "Epoch: 6050 | Training Loss: 3.039888620376587 | Testing Loss: 3.866187334060669\n",
            "Epoch: 6060 | Training Loss: 3.0296332836151123 | Testing Loss: 3.8544585704803467\n",
            "Epoch: 6070 | Training Loss: 3.0193774700164795 | Testing Loss: 3.8427300453186035\n",
            "Epoch: 6080 | Training Loss: 3.009122133255005 | Testing Loss: 3.831001043319702\n",
            "Epoch: 6090 | Training Loss: 2.9988667964935303 | Testing Loss: 3.81927227973938\n",
            "Epoch: 6100 | Training Loss: 2.9886112213134766 | Testing Loss: 3.8075432777404785\n",
            "Epoch: 6110 | Training Loss: 2.978355884552002 | Testing Loss: 3.7958145141601562\n",
            "Epoch: 6120 | Training Loss: 2.9681005477905273 | Testing Loss: 3.784085988998413\n",
            "Epoch: 6130 | Training Loss: 2.9578452110290527 | Testing Loss: 3.7723567485809326\n",
            "Epoch: 6140 | Training Loss: 2.94758939743042 | Testing Loss: 3.7606282234191895\n",
            "Epoch: 6150 | Training Loss: 2.9373340606689453 | Testing Loss: 3.748899221420288\n",
            "Epoch: 6160 | Training Loss: 2.9270787239074707 | Testing Loss: 3.737170457839966\n",
            "Epoch: 6170 | Training Loss: 2.916823148727417 | Testing Loss: 3.7254416942596436\n",
            "Epoch: 6180 | Training Loss: 2.9065678119659424 | Testing Loss: 3.713712692260742\n",
            "Epoch: 6190 | Training Loss: 2.8963124752044678 | Testing Loss: 3.701984167098999\n",
            "Epoch: 6200 | Training Loss: 2.8863542079925537 | Testing Loss: 3.690458297729492\n",
            "Epoch: 6210 | Training Loss: 2.876988172531128 | Testing Loss: 3.679236650466919\n",
            "Epoch: 6220 | Training Loss: 2.8676223754882812 | Testing Loss: 3.6680150032043457\n",
            "Epoch: 6230 | Training Loss: 2.8582565784454346 | Testing Loss: 3.6567933559417725\n",
            "Epoch: 6240 | Training Loss: 2.848890781402588 | Testing Loss: 3.64557147026062\n",
            "Epoch: 6250 | Training Loss: 2.839524745941162 | Testing Loss: 3.634349822998047\n",
            "Epoch: 6260 | Training Loss: 2.8301587104797363 | Testing Loss: 3.6231281757354736\n",
            "Epoch: 6270 | Training Loss: 2.8207929134368896 | Testing Loss: 3.6119065284729004\n",
            "Epoch: 6280 | Training Loss: 2.811426877975464 | Testing Loss: 3.600684881210327\n",
            "Epoch: 6290 | Training Loss: 2.802060842514038 | Testing Loss: 3.589462995529175\n",
            "Epoch: 6300 | Training Loss: 2.7926952838897705 | Testing Loss: 3.5782413482666016\n",
            "Epoch: 6310 | Training Loss: 2.7833292484283447 | Testing Loss: 3.5670197010040283\n",
            "Epoch: 6320 | Training Loss: 2.773963212966919 | Testing Loss: 3.555798053741455\n",
            "Epoch: 6330 | Training Loss: 2.7645974159240723 | Testing Loss: 3.544576406478882\n",
            "Epoch: 6340 | Training Loss: 2.7552316188812256 | Testing Loss: 3.5333545207977295\n",
            "Epoch: 6350 | Training Loss: 2.745868682861328 | Testing Loss: 3.5221848487854004\n",
            "Epoch: 6360 | Training Loss: 2.737356662750244 | Testing Loss: 3.511482000350952\n",
            "Epoch: 6370 | Training Loss: 2.728844404220581 | Testing Loss: 3.500778913497925\n",
            "Epoch: 6380 | Training Loss: 2.720332384109497 | Testing Loss: 3.4900765419006348\n",
            "Epoch: 6390 | Training Loss: 2.711820363998413 | Testing Loss: 3.4793732166290283\n",
            "Epoch: 6400 | Training Loss: 2.703307867050171 | Testing Loss: 3.468670606613159\n",
            "Epoch: 6410 | Training Loss: 2.694795846939087 | Testing Loss: 3.457967758178711\n",
            "Epoch: 6420 | Training Loss: 2.686283826828003 | Testing Loss: 3.4472649097442627\n",
            "Epoch: 6430 | Training Loss: 2.6777713298797607 | Testing Loss: 3.4365622997283936\n",
            "Epoch: 6440 | Training Loss: 2.6692593097686768 | Testing Loss: 3.4258594512939453\n",
            "Epoch: 6450 | Training Loss: 2.6607472896575928 | Testing Loss: 3.415156602859497\n",
            "Epoch: 6460 | Training Loss: 2.6522350311279297 | Testing Loss: 3.404453992843628\n",
            "Epoch: 6470 | Training Loss: 2.6437230110168457 | Testing Loss: 3.3937509059906006\n",
            "Epoch: 6480 | Training Loss: 2.6352107524871826 | Testing Loss: 3.3830482959747314\n",
            "Epoch: 6490 | Training Loss: 2.6266987323760986 | Testing Loss: 3.372345447540283\n",
            "Epoch: 6500 | Training Loss: 2.6181862354278564 | Testing Loss: 3.361642599105835\n",
            "Epoch: 6510 | Training Loss: 2.6096997261047363 | Testing Loss: 3.35099196434021\n",
            "Epoch: 6520 | Training Loss: 2.601999044418335 | Testing Loss: 3.3408126831054688\n",
            "Epoch: 6530 | Training Loss: 2.5942978858947754 | Testing Loss: 3.3306329250335693\n",
            "Epoch: 6540 | Training Loss: 2.586597204208374 | Testing Loss: 3.320453405380249\n",
            "Epoch: 6550 | Training Loss: 2.5788962841033936 | Testing Loss: 3.3102734088897705\n",
            "Epoch: 6560 | Training Loss: 2.571195363998413 | Testing Loss: 3.30009388923645\n",
            "Epoch: 6570 | Training Loss: 2.5634946823120117 | Testing Loss: 3.28991436958313\n",
            "Epoch: 6580 | Training Loss: 2.5557937622070312 | Testing Loss: 3.2797343730926514\n",
            "Epoch: 6590 | Training Loss: 2.548092842102051 | Testing Loss: 3.269554853439331\n",
            "Epoch: 6600 | Training Loss: 2.5403923988342285 | Testing Loss: 3.2593750953674316\n",
            "Epoch: 6610 | Training Loss: 2.532691478729248 | Testing Loss: 3.249195098876953\n",
            "Epoch: 6620 | Training Loss: 2.5249905586242676 | Testing Loss: 3.239015579223633\n",
            "Epoch: 6630 | Training Loss: 2.517289876937866 | Testing Loss: 3.2288360595703125\n",
            "Epoch: 6640 | Training Loss: 2.5095887184143066 | Testing Loss: 3.218656539916992\n",
            "Epoch: 6650 | Training Loss: 2.5018880367279053 | Testing Loss: 3.2084767818450928\n",
            "Epoch: 6660 | Training Loss: 2.494187116622925 | Testing Loss: 3.1982970237731934\n",
            "Epoch: 6670 | Training Loss: 2.4864864349365234 | Testing Loss: 3.188117504119873\n",
            "Epoch: 6680 | Training Loss: 2.479123592376709 | Testing Loss: 3.1782028675079346\n",
            "Epoch: 6690 | Training Loss: 2.472193479537964 | Testing Loss: 3.168553113937378\n",
            "Epoch: 6700 | Training Loss: 2.4652633666992188 | Testing Loss: 3.1589035987854004\n",
            "Epoch: 6710 | Training Loss: 2.4583334922790527 | Testing Loss: 3.1492538452148438\n",
            "Epoch: 6720 | Training Loss: 2.4514036178588867 | Testing Loss: 3.1396045684814453\n",
            "Epoch: 6730 | Training Loss: 2.4444735050201416 | Testing Loss: 3.1299550533294678\n",
            "Epoch: 6740 | Training Loss: 2.4375436305999756 | Testing Loss: 3.120305299758911\n",
            "Epoch: 6750 | Training Loss: 2.4306135177612305 | Testing Loss: 3.1106555461883545\n",
            "Epoch: 6760 | Training Loss: 2.4236836433410645 | Testing Loss: 3.101006269454956\n",
            "Epoch: 6770 | Training Loss: 2.4167535305023193 | Testing Loss: 3.0913567543029785\n",
            "Epoch: 6780 | Training Loss: 2.409823417663574 | Testing Loss: 3.081707000732422\n",
            "Epoch: 6790 | Training Loss: 2.4028937816619873 | Testing Loss: 3.0720577239990234\n",
            "Epoch: 6800 | Training Loss: 2.395963668823242 | Testing Loss: 3.062408208847046\n",
            "Epoch: 6810 | Training Loss: 2.389033555984497 | Testing Loss: 3.0527584552764893\n",
            "Epoch: 6820 | Training Loss: 2.382103681564331 | Testing Loss: 3.0431087017059326\n",
            "Epoch: 6830 | Training Loss: 2.375173568725586 | Testing Loss: 3.033459424972534\n",
            "Epoch: 6840 | Training Loss: 2.368243932723999 | Testing Loss: 3.0238099098205566\n",
            "Epoch: 6850 | Training Loss: 2.361415147781372 | Testing Loss: 3.0142674446105957\n",
            "Epoch: 6860 | Training Loss: 2.355215311050415 | Testing Loss: 3.005155324935913\n",
            "Epoch: 6870 | Training Loss: 2.349015235900879 | Testing Loss: 2.9960427284240723\n",
            "Epoch: 6880 | Training Loss: 2.342815399169922 | Testing Loss: 2.9869303703308105\n",
            "Epoch: 6890 | Training Loss: 2.3366153240203857 | Testing Loss: 2.977818012237549\n",
            "Epoch: 6900 | Training Loss: 2.3304154872894287 | Testing Loss: 2.968705415725708\n",
            "Epoch: 6910 | Training Loss: 2.3242156505584717 | Testing Loss: 2.9595930576324463\n",
            "Epoch: 6920 | Training Loss: 2.3180155754089355 | Testing Loss: 2.9504804611206055\n",
            "Epoch: 6930 | Training Loss: 2.3118157386779785 | Testing Loss: 2.9413681030273438\n",
            "Epoch: 6940 | Training Loss: 2.3056156635284424 | Testing Loss: 2.932255506515503\n",
            "Epoch: 6950 | Training Loss: 2.2994158267974854 | Testing Loss: 2.923143148422241\n",
            "Epoch: 6960 | Training Loss: 2.293215751647949 | Testing Loss: 2.9140307903289795\n",
            "Epoch: 6970 | Training Loss: 2.287015676498413 | Testing Loss: 2.9049184322357178\n",
            "Epoch: 6980 | Training Loss: 2.280815839767456 | Testing Loss: 2.895806074142456\n",
            "Epoch: 6990 | Training Loss: 2.274616003036499 | Testing Loss: 2.8866937160491943\n",
            "Epoch: 7000 | Training Loss: 2.268415927886963 | Testing Loss: 2.8775811195373535\n",
            "Epoch: 7010 | Training Loss: 2.262216091156006 | Testing Loss: 2.868468761444092\n",
            "Epoch: 7020 | Training Loss: 2.2560160160064697 | Testing Loss: 2.859356164932251\n",
            "Epoch: 7030 | Training Loss: 2.2498435974121094 | Testing Loss: 2.850297689437866\n",
            "Epoch: 7040 | Training Loss: 2.244330406188965 | Testing Loss: 2.841726779937744\n",
            "Epoch: 7050 | Training Loss: 2.2388172149658203 | Testing Loss: 2.833155632019043\n",
            "Epoch: 7060 | Training Loss: 2.2333037853240967 | Testing Loss: 2.824584722518921\n",
            "Epoch: 7070 | Training Loss: 2.227790594100952 | Testing Loss: 2.8160135746002197\n",
            "Epoch: 7080 | Training Loss: 2.2222774028778076 | Testing Loss: 2.8074424266815186\n",
            "Epoch: 7090 | Training Loss: 2.216764211654663 | Testing Loss: 2.7988712787628174\n",
            "Epoch: 7100 | Training Loss: 2.2112510204315186 | Testing Loss: 2.790300130844116\n",
            "Epoch: 7110 | Training Loss: 2.205737829208374 | Testing Loss: 2.781728982925415\n",
            "Epoch: 7120 | Training Loss: 2.2002246379852295 | Testing Loss: 2.773157835006714\n",
            "Epoch: 7130 | Training Loss: 2.194711446762085 | Testing Loss: 2.764586925506592\n",
            "Epoch: 7140 | Training Loss: 2.1891982555389404 | Testing Loss: 2.7560157775878906\n",
            "Epoch: 7150 | Training Loss: 2.183685064315796 | Testing Loss: 2.7474448680877686\n",
            "Epoch: 7160 | Training Loss: 2.1781716346740723 | Testing Loss: 2.7388737201690674\n",
            "Epoch: 7170 | Training Loss: 2.1726584434509277 | Testing Loss: 2.730302572250366\n",
            "Epoch: 7180 | Training Loss: 2.167145252227783 | Testing Loss: 2.721731185913086\n",
            "Epoch: 7190 | Training Loss: 2.1616322994232178 | Testing Loss: 2.713160276412964\n",
            "Epoch: 7200 | Training Loss: 2.156118869781494 | Testing Loss: 2.7045891284942627\n",
            "Epoch: 7210 | Training Loss: 2.1506056785583496 | Testing Loss: 2.6960179805755615\n",
            "Epoch: 7220 | Training Loss: 2.145118236541748 | Testing Loss: 2.6875016689300537\n",
            "Epoch: 7230 | Training Loss: 2.1402499675750732 | Testing Loss: 2.6794791221618652\n",
            "Epoch: 7240 | Training Loss: 2.1353821754455566 | Testing Loss: 2.6714560985565186\n",
            "Epoch: 7250 | Training Loss: 2.130513906478882 | Testing Loss: 2.663433790206909\n",
            "Epoch: 7260 | Training Loss: 2.125645875930786 | Testing Loss: 2.6554107666015625\n",
            "Epoch: 7270 | Training Loss: 2.1207778453826904 | Testing Loss: 2.647388219833374\n",
            "Epoch: 7280 | Training Loss: 2.1159098148345947 | Testing Loss: 2.6393654346466064\n",
            "Epoch: 7290 | Training Loss: 2.111041784286499 | Testing Loss: 2.631342649459839\n",
            "Epoch: 7300 | Training Loss: 2.106173276901245 | Testing Loss: 2.6233198642730713\n",
            "Epoch: 7310 | Training Loss: 2.1013054847717285 | Testing Loss: 2.6152970790863037\n",
            "Epoch: 7320 | Training Loss: 2.0964372158050537 | Testing Loss: 2.607274293899536\n",
            "Epoch: 7330 | Training Loss: 2.091569185256958 | Testing Loss: 2.5992515087127686\n",
            "Epoch: 7340 | Training Loss: 2.0867011547088623 | Testing Loss: 2.59122896194458\n",
            "Epoch: 7350 | Training Loss: 2.0818328857421875 | Testing Loss: 2.5832059383392334\n",
            "Epoch: 7360 | Training Loss: 2.076965093612671 | Testing Loss: 2.575183391571045\n",
            "Epoch: 7370 | Training Loss: 2.072096824645996 | Testing Loss: 2.5671603679656982\n",
            "Epoch: 7380 | Training Loss: 2.0672287940979004 | Testing Loss: 2.5591378211975098\n",
            "Epoch: 7390 | Training Loss: 2.0623605251312256 | Testing Loss: 2.551115036010742\n",
            "Epoch: 7400 | Training Loss: 2.05749249458313 | Testing Loss: 2.5430924892425537\n",
            "Epoch: 7410 | Training Loss: 2.052624225616455 | Testing Loss: 2.535069227218628\n",
            "Epoch: 7420 | Training Loss: 2.0477705001831055 | Testing Loss: 2.5271012783050537\n",
            "Epoch: 7430 | Training Loss: 2.043501138687134 | Testing Loss: 2.5196266174316406\n",
            "Epoch: 7440 | Training Loss: 2.039231538772583 | Testing Loss: 2.5121514797210693\n",
            "Epoch: 7450 | Training Loss: 2.0349621772766113 | Testing Loss: 2.504676580429077\n",
            "Epoch: 7460 | Training Loss: 2.0306925773620605 | Testing Loss: 2.497201681137085\n",
            "Epoch: 7470 | Training Loss: 2.0264229774475098 | Testing Loss: 2.4897265434265137\n",
            "Epoch: 7480 | Training Loss: 2.022153615951538 | Testing Loss: 2.4822518825531006\n",
            "Epoch: 7490 | Training Loss: 2.0178840160369873 | Testing Loss: 2.4747767448425293\n",
            "Epoch: 7500 | Training Loss: 2.0136146545410156 | Testing Loss: 2.467301607131958\n",
            "Epoch: 7510 | Training Loss: 2.009345054626465 | Testing Loss: 2.459826707839966\n",
            "Epoch: 7520 | Training Loss: 2.005075454711914 | Testing Loss: 2.4523515701293945\n",
            "Epoch: 7530 | Training Loss: 2.0008060932159424 | Testing Loss: 2.4448769092559814\n",
            "Epoch: 7540 | Training Loss: 1.9965366125106812 | Testing Loss: 2.43740177154541\n",
            "Epoch: 7550 | Training Loss: 1.9922668933868408 | Testing Loss: 2.429926633834839\n",
            "Epoch: 7560 | Training Loss: 1.9879974126815796 | Testing Loss: 2.4224517345428467\n",
            "Epoch: 7570 | Training Loss: 1.983728051185608 | Testing Loss: 2.4149768352508545\n",
            "Epoch: 7580 | Training Loss: 1.9794584512710571 | Testing Loss: 2.4075019359588623\n",
            "Epoch: 7590 | Training Loss: 1.9751890897750854 | Testing Loss: 2.40002703666687\n",
            "Epoch: 7600 | Training Loss: 1.9709194898605347 | Testing Loss: 2.392551898956299\n",
            "Epoch: 7610 | Training Loss: 1.9666500091552734 | Testing Loss: 2.3850769996643066\n",
            "Epoch: 7620 | Training Loss: 1.962380290031433 | Testing Loss: 2.3776018619537354\n",
            "Epoch: 7630 | Training Loss: 1.9581108093261719 | Testing Loss: 2.3701272010803223\n",
            "Epoch: 7640 | Training Loss: 1.9542999267578125 | Testing Loss: 2.3631558418273926\n",
            "Epoch: 7650 | Training Loss: 1.9505890607833862 | Testing Loss: 2.3562400341033936\n",
            "Epoch: 7660 | Training Loss: 1.9468780755996704 | Testing Loss: 2.349324941635132\n",
            "Epoch: 7670 | Training Loss: 1.9431670904159546 | Testing Loss: 2.342409133911133\n",
            "Epoch: 7680 | Training Loss: 1.9394562244415283 | Testing Loss: 2.335493803024292\n",
            "Epoch: 7690 | Training Loss: 1.9357452392578125 | Testing Loss: 2.3285787105560303\n",
            "Epoch: 7700 | Training Loss: 1.9320343732833862 | Testing Loss: 2.3216633796691895\n",
            "Epoch: 7710 | Training Loss: 1.9283233880996704 | Testing Loss: 2.3147475719451904\n",
            "Epoch: 7720 | Training Loss: 1.9246124029159546 | Testing Loss: 2.3078324794769287\n",
            "Epoch: 7730 | Training Loss: 1.9209015369415283 | Testing Loss: 2.300916910171509\n",
            "Epoch: 7740 | Training Loss: 1.9171905517578125 | Testing Loss: 2.294001340866089\n",
            "Epoch: 7750 | Training Loss: 1.9134796857833862 | Testing Loss: 2.287086248397827\n",
            "Epoch: 7760 | Training Loss: 1.9097687005996704 | Testing Loss: 2.2801706790924072\n",
            "Epoch: 7770 | Training Loss: 1.9060577154159546 | Testing Loss: 2.2732551097869873\n",
            "Epoch: 7780 | Training Loss: 1.9023468494415283 | Testing Loss: 2.2663400173187256\n",
            "Epoch: 7790 | Training Loss: 1.8986358642578125 | Testing Loss: 2.2594244480133057\n",
            "Epoch: 7800 | Training Loss: 1.8949249982833862 | Testing Loss: 2.252509117126465\n",
            "Epoch: 7810 | Training Loss: 1.8912140130996704 | Testing Loss: 2.245593547821045\n",
            "Epoch: 7820 | Training Loss: 1.8875030279159546 | Testing Loss: 2.238677978515625\n",
            "Epoch: 7830 | Training Loss: 1.8837921619415283 | Testing Loss: 2.231762647628784\n",
            "Epoch: 7840 | Training Loss: 1.8800811767578125 | Testing Loss: 2.2248475551605225\n",
            "Epoch: 7850 | Training Loss: 1.8763703107833862 | Testing Loss: 2.2179319858551025\n",
            "Epoch: 7860 | Training Loss: 1.8727737665176392 | Testing Loss: 2.211186170578003\n",
            "Epoch: 7870 | Training Loss: 1.8695770502090454 | Testing Loss: 2.2048356533050537\n",
            "Epoch: 7880 | Training Loss: 1.8663806915283203 | Testing Loss: 2.1984856128692627\n",
            "Epoch: 7890 | Training Loss: 1.8631839752197266 | Testing Loss: 2.1921353340148926\n",
            "Epoch: 7900 | Training Loss: 1.8599872589111328 | Testing Loss: 2.1857850551605225\n",
            "Epoch: 7910 | Training Loss: 1.856790542602539 | Testing Loss: 2.1794347763061523\n",
            "Epoch: 7920 | Training Loss: 1.8535940647125244 | Testing Loss: 2.1730844974517822\n",
            "Epoch: 7930 | Training Loss: 1.8503974676132202 | Testing Loss: 2.166734457015991\n",
            "Epoch: 7940 | Training Loss: 1.8472007513046265 | Testing Loss: 2.160383939743042\n",
            "Epoch: 7950 | Training Loss: 1.8440040349960327 | Testing Loss: 2.154033660888672\n",
            "Epoch: 7960 | Training Loss: 1.840807557106018 | Testing Loss: 2.147683620452881\n",
            "Epoch: 7970 | Training Loss: 1.8376108407974243 | Testing Loss: 2.1413333415985107\n",
            "Epoch: 7980 | Training Loss: 1.8344143629074097 | Testing Loss: 2.1349830627441406\n",
            "Epoch: 7990 | Training Loss: 1.831217646598816 | Testing Loss: 2.1286327838897705\n",
            "Epoch: 8000 | Training Loss: 1.8280210494995117 | Testing Loss: 2.1222825050354004\n",
            "Epoch: 8010 | Training Loss: 1.824824571609497 | Testing Loss: 2.1159322261810303\n",
            "Epoch: 8020 | Training Loss: 1.8216278553009033 | Testing Loss: 2.10958194732666\n",
            "Epoch: 8030 | Training Loss: 1.8184311389923096 | Testing Loss: 2.103231906890869\n",
            "Epoch: 8040 | Training Loss: 1.8152345418930054 | Testing Loss: 2.096881628036499\n",
            "Epoch: 8050 | Training Loss: 1.8120378255844116 | Testing Loss: 2.09053111076355\n",
            "Epoch: 8060 | Training Loss: 1.808841347694397 | Testing Loss: 2.084181070327759\n",
            "Epoch: 8070 | Training Loss: 1.8056446313858032 | Testing Loss: 2.0778307914733887\n",
            "Epoch: 8080 | Training Loss: 1.8024481534957886 | Testing Loss: 2.0714805126190186\n",
            "Epoch: 8090 | Training Loss: 1.7992515563964844 | Testing Loss: 2.0651302337646484\n",
            "Epoch: 8100 | Training Loss: 1.7960548400878906 | Testing Loss: 2.0587799549102783\n",
            "Epoch: 8110 | Training Loss: 1.793304681777954 | Testing Loss: 2.053001880645752\n",
            "Epoch: 8120 | Training Loss: 1.7905782461166382 | Testing Loss: 2.0472238063812256\n",
            "Epoch: 8130 | Training Loss: 1.787852168083191 | Testing Loss: 2.041445732116699\n",
            "Epoch: 8140 | Training Loss: 1.785125732421875 | Testing Loss: 2.035667896270752\n",
            "Epoch: 8150 | Training Loss: 1.7823995351791382 | Testing Loss: 2.0298895835876465\n",
            "Epoch: 8160 | Training Loss: 1.7796732187271118 | Testing Loss: 2.02411150932312\n",
            "Epoch: 8170 | Training Loss: 1.776947021484375 | Testing Loss: 2.0183334350585938\n",
            "Epoch: 8180 | Training Loss: 1.7742207050323486 | Testing Loss: 2.0125555992126465\n",
            "Epoch: 8190 | Training Loss: 1.7714942693710327 | Testing Loss: 2.006777286529541\n",
            "Epoch: 8200 | Training Loss: 1.7687679529190063 | Testing Loss: 2.0009992122650146\n",
            "Epoch: 8210 | Training Loss: 1.7660417556762695 | Testing Loss: 1.9952211380004883\n",
            "Epoch: 8220 | Training Loss: 1.7633154392242432 | Testing Loss: 1.9894431829452515\n",
            "Epoch: 8230 | Training Loss: 1.7605892419815063 | Testing Loss: 1.983664870262146\n",
            "Epoch: 8240 | Training Loss: 1.75786292552948 | Testing Loss: 1.9778870344161987\n",
            "Epoch: 8250 | Training Loss: 1.755136489868164 | Testing Loss: 1.9721088409423828\n",
            "Epoch: 8260 | Training Loss: 1.7524102926254272 | Testing Loss: 1.966330885887146\n",
            "Epoch: 8270 | Training Loss: 1.74968421459198 | Testing Loss: 1.9605525732040405\n",
            "Epoch: 8280 | Training Loss: 1.7469576597213745 | Testing Loss: 1.9547747373580933\n",
            "Epoch: 8290 | Training Loss: 1.7442314624786377 | Testing Loss: 1.9489965438842773\n",
            "Epoch: 8300 | Training Loss: 1.7415050268173218 | Testing Loss: 1.9432185888290405\n",
            "Epoch: 8310 | Training Loss: 1.7387789487838745 | Testing Loss: 1.937440276145935\n",
            "Epoch: 8320 | Training Loss: 1.7360527515411377 | Testing Loss: 1.9316624402999878\n",
            "Epoch: 8330 | Training Loss: 1.7333263158798218 | Testing Loss: 1.9258842468261719\n",
            "Epoch: 8340 | Training Loss: 1.7305999994277954 | Testing Loss: 1.920106291770935\n",
            "Epoch: 8350 | Training Loss: 1.727873682975769 | Testing Loss: 1.9143279790878296\n",
            "Epoch: 8360 | Training Loss: 1.7251474857330322 | Testing Loss: 1.9085501432418823\n",
            "Epoch: 8370 | Training Loss: 1.7224212884902954 | Testing Loss: 1.9035345315933228\n",
            "Epoch: 8380 | Training Loss: 1.719694972038269 | Testing Loss: 1.898658037185669\n",
            "Epoch: 8390 | Training Loss: 1.7169685363769531 | Testing Loss: 1.893781304359436\n",
            "Epoch: 8400 | Training Loss: 1.7142423391342163 | Testing Loss: 1.8889048099517822\n",
            "Epoch: 8410 | Training Loss: 1.71151602268219 | Testing Loss: 1.8840280771255493\n",
            "Epoch: 8420 | Training Loss: 1.7087898254394531 | Testing Loss: 1.8791515827178955\n",
            "Epoch: 8430 | Training Loss: 1.7060635089874268 | Testing Loss: 1.8742748498916626\n",
            "Epoch: 8440 | Training Loss: 1.7033370733261108 | Testing Loss: 1.8693983554840088\n",
            "Epoch: 8450 | Training Loss: 1.7006107568740845 | Testing Loss: 1.8645216226577759\n",
            "Epoch: 8460 | Training Loss: 1.6978845596313477 | Testing Loss: 1.859645128250122\n",
            "Epoch: 8470 | Training Loss: 1.6951583623886108 | Testing Loss: 1.8547683954238892\n",
            "Epoch: 8480 | Training Loss: 1.6924320459365845 | Testing Loss: 1.8498919010162354\n",
            "Epoch: 8490 | Training Loss: 1.689705729484558 | Testing Loss: 1.8450151681900024\n",
            "Epoch: 8500 | Training Loss: 1.6869792938232422 | Testing Loss: 1.8401386737823486\n",
            "Epoch: 8510 | Training Loss: 1.6842533349990845 | Testing Loss: 1.8352619409561157\n",
            "Epoch: 8520 | Training Loss: 1.681526780128479 | Testing Loss: 1.830385446548462\n",
            "Epoch: 8530 | Training Loss: 1.6788005828857422 | Testing Loss: 1.825508713722229\n",
            "Epoch: 8540 | Training Loss: 1.6760742664337158 | Testing Loss: 1.8206322193145752\n",
            "Epoch: 8550 | Training Loss: 1.673348069190979 | Testing Loss: 1.8157554864883423\n",
            "Epoch: 8560 | Training Loss: 1.6706217527389526 | Testing Loss: 1.8108789920806885\n",
            "Epoch: 8570 | Training Loss: 1.6678955554962158 | Testing Loss: 1.8060022592544556\n",
            "Epoch: 8580 | Training Loss: 1.6651691198349 | Testing Loss: 1.8011257648468018\n",
            "Epoch: 8590 | Training Loss: 1.6624428033828735 | Testing Loss: 1.7962490320205688\n",
            "Epoch: 8600 | Training Loss: 1.6597164869308472 | Testing Loss: 1.791372537612915\n",
            "Epoch: 8610 | Training Loss: 1.6569902896881104 | Testing Loss: 1.7864958047866821\n",
            "Epoch: 8620 | Training Loss: 1.6544103622436523 | Testing Loss: 1.7818092107772827\n",
            "Epoch: 8630 | Training Loss: 1.6521152257919312 | Testing Loss: 1.7774070501327515\n",
            "Epoch: 8640 | Training Loss: 1.6498199701309204 | Testing Loss: 1.7730051279067993\n",
            "Epoch: 8650 | Training Loss: 1.6475248336791992 | Testing Loss: 1.7686032056808472\n",
            "Epoch: 8660 | Training Loss: 1.645229697227478 | Testing Loss: 1.7642015218734741\n",
            "Epoch: 8670 | Training Loss: 1.6429346799850464 | Testing Loss: 1.7597993612289429\n",
            "Epoch: 8680 | Training Loss: 1.640639305114746 | Testing Loss: 1.7553974390029907\n",
            "Epoch: 8690 | Training Loss: 1.638344168663025 | Testing Loss: 1.7509952783584595\n",
            "Epoch: 8700 | Training Loss: 1.6360489130020142 | Testing Loss: 1.7465934753417969\n",
            "Epoch: 8710 | Training Loss: 1.633753776550293 | Testing Loss: 1.7421915531158447\n",
            "Epoch: 8720 | Training Loss: 1.6314585208892822 | Testing Loss: 1.7377897500991821\n",
            "Epoch: 8730 | Training Loss: 1.629163384437561 | Testing Loss: 1.7333875894546509\n",
            "Epoch: 8740 | Training Loss: 1.6268681287765503 | Testing Loss: 1.7289856672286987\n",
            "Epoch: 8750 | Training Loss: 1.624572992324829 | Testing Loss: 1.7245838642120361\n",
            "Epoch: 8760 | Training Loss: 1.622277855873108 | Testing Loss: 1.7201818227767944\n",
            "Epoch: 8770 | Training Loss: 1.6199826002120972 | Testing Loss: 1.7157796621322632\n",
            "Epoch: 8780 | Training Loss: 1.6176872253417969 | Testing Loss: 1.7113779783248901\n",
            "Epoch: 8790 | Training Loss: 1.6153923273086548 | Testing Loss: 1.7069758176803589\n",
            "Epoch: 8800 | Training Loss: 1.613097071647644 | Testing Loss: 1.7025737762451172\n",
            "Epoch: 8810 | Training Loss: 1.6108016967773438 | Testing Loss: 1.6981719732284546\n",
            "Epoch: 8820 | Training Loss: 1.6085065603256226 | Testing Loss: 1.6937702894210815\n",
            "Epoch: 8830 | Training Loss: 1.606211543083191 | Testing Loss: 1.6893682479858398\n",
            "Epoch: 8840 | Training Loss: 1.6039161682128906 | Testing Loss: 1.6849660873413086\n",
            "Epoch: 8850 | Training Loss: 1.6016209125518799 | Testing Loss: 1.6805641651153564\n",
            "Epoch: 8860 | Training Loss: 1.5993257761001587 | Testing Loss: 1.676162600517273\n",
            "Epoch: 8870 | Training Loss: 1.5970306396484375 | Testing Loss: 1.6717602014541626\n",
            "Epoch: 8880 | Training Loss: 1.5947355031967163 | Testing Loss: 1.6673583984375\n",
            "Epoch: 8890 | Training Loss: 1.5924403667449951 | Testing Loss: 1.6629562377929688\n",
            "Epoch: 8900 | Training Loss: 1.5903820991516113 | Testing Loss: 1.6588910818099976\n",
            "Epoch: 8910 | Training Loss: 1.5884720087051392 | Testing Loss: 1.6549701690673828\n",
            "Epoch: 8920 | Training Loss: 1.5865617990493774 | Testing Loss: 1.6510494947433472\n",
            "Epoch: 8930 | Training Loss: 1.5846515893936157 | Testing Loss: 1.6471284627914429\n",
            "Epoch: 8940 | Training Loss: 1.5827414989471436 | Testing Loss: 1.6432075500488281\n",
            "Epoch: 8950 | Training Loss: 1.5808312892913818 | Testing Loss: 1.6392866373062134\n",
            "Epoch: 8960 | Training Loss: 1.5789211988449097 | Testing Loss: 1.6353657245635986\n",
            "Epoch: 8970 | Training Loss: 1.577010989189148 | Testing Loss: 1.6314449310302734\n",
            "Epoch: 8980 | Training Loss: 1.5751007795333862 | Testing Loss: 1.6275237798690796\n",
            "Epoch: 8990 | Training Loss: 1.5731905698776245 | Testing Loss: 1.623603105545044\n",
            "Epoch: 9000 | Training Loss: 1.5712803602218628 | Testing Loss: 1.6196821928024292\n",
            "Epoch: 9010 | Training Loss: 1.5693702697753906 | Testing Loss: 1.615761160850525\n",
            "Epoch: 9020 | Training Loss: 1.567460060119629 | Testing Loss: 1.6118404865264893\n",
            "Epoch: 9030 | Training Loss: 1.5655498504638672 | Testing Loss: 1.6079193353652954\n",
            "Epoch: 9040 | Training Loss: 1.563639760017395 | Testing Loss: 1.6039985418319702\n",
            "Epoch: 9050 | Training Loss: 1.5617295503616333 | Testing Loss: 1.6000776290893555\n",
            "Epoch: 9060 | Training Loss: 1.5598194599151611 | Testing Loss: 1.5961568355560303\n",
            "Epoch: 9070 | Training Loss: 1.5579092502593994 | Testing Loss: 1.5922359228134155\n",
            "Epoch: 9080 | Training Loss: 1.5559990406036377 | Testing Loss: 1.5883148908615112\n",
            "Epoch: 9090 | Training Loss: 1.5540889501571655 | Testing Loss: 1.5843939781188965\n",
            "Epoch: 9100 | Training Loss: 1.5521787405014038 | Testing Loss: 1.5804731845855713\n",
            "Epoch: 9110 | Training Loss: 1.550268530845642 | Testing Loss: 1.5765522718429565\n",
            "Epoch: 9120 | Training Loss: 1.5483583211898804 | Testing Loss: 1.5726313591003418\n",
            "Epoch: 9130 | Training Loss: 1.5464481115341187 | Testing Loss: 1.5687103271484375\n",
            "Epoch: 9140 | Training Loss: 1.5445380210876465 | Testing Loss: 1.5647895336151123\n",
            "Epoch: 9150 | Training Loss: 1.5426278114318848 | Testing Loss: 1.5608686208724976\n",
            "Epoch: 9160 | Training Loss: 1.5407177209854126 | Testing Loss: 1.5569478273391724\n",
            "Epoch: 9170 | Training Loss: 1.5388075113296509 | Testing Loss: 1.5530269145965576\n",
            "Epoch: 9180 | Training Loss: 1.5368973016738892 | Testing Loss: 1.5491058826446533\n",
            "Epoch: 9190 | Training Loss: 1.534987211227417 | Testing Loss: 1.5451849699020386\n",
            "Epoch: 9200 | Training Loss: 1.5330770015716553 | Testing Loss: 1.5412641763687134\n",
            "Epoch: 9210 | Training Loss: 1.5314435958862305 | Testing Loss: 1.5377811193466187\n",
            "Epoch: 9220 | Training Loss: 1.529870629310608 | Testing Loss: 1.5343462228775024\n",
            "Epoch: 9230 | Training Loss: 1.5282975435256958 | Testing Loss: 1.5309112071990967\n",
            "Epoch: 9240 | Training Loss: 1.5267245769500732 | Testing Loss: 1.5274765491485596\n",
            "Epoch: 9250 | Training Loss: 1.5251514911651611 | Testing Loss: 1.524042010307312\n",
            "Epoch: 9260 | Training Loss: 1.5235785245895386 | Testing Loss: 1.5206072330474854\n",
            "Epoch: 9270 | Training Loss: 1.5220054388046265 | Testing Loss: 1.5171725749969482\n",
            "Epoch: 9280 | Training Loss: 1.5204325914382935 | Testing Loss: 1.5137379169464111\n",
            "Epoch: 9290 | Training Loss: 1.5188595056533813 | Testing Loss: 1.5103031396865845\n",
            "Epoch: 9300 | Training Loss: 1.5172865390777588 | Testing Loss: 1.5068682432174683\n",
            "Epoch: 9310 | Training Loss: 1.5157135725021362 | Testing Loss: 1.5034334659576416\n",
            "Epoch: 9320 | Training Loss: 1.5141403675079346 | Testing Loss: 1.499998927116394\n",
            "Epoch: 9330 | Training Loss: 1.512567400932312 | Testing Loss: 1.4965641498565674\n",
            "Epoch: 9340 | Training Loss: 1.5109943151474 | Testing Loss: 1.4931293725967407\n",
            "Epoch: 9350 | Training Loss: 1.5094213485717773 | Testing Loss: 1.489694595336914\n",
            "Epoch: 9360 | Training Loss: 1.5078483819961548 | Testing Loss: 1.4862598180770874\n",
            "Epoch: 9370 | Training Loss: 1.5062751770019531 | Testing Loss: 1.4828251600265503\n",
            "Epoch: 9380 | Training Loss: 1.5047023296356201 | Testing Loss: 1.4793903827667236\n",
            "Epoch: 9390 | Training Loss: 1.5031293630599976 | Testing Loss: 1.475955605506897\n",
            "Epoch: 9400 | Training Loss: 1.5015562772750854 | Testing Loss: 1.4725209474563599\n",
            "Epoch: 9410 | Training Loss: 1.4999831914901733 | Testing Loss: 1.4690864086151123\n",
            "Epoch: 9420 | Training Loss: 1.4984102249145508 | Testing Loss: 1.4656516313552856\n",
            "Epoch: 9430 | Training Loss: 1.4968371391296387 | Testing Loss: 1.4622164964675903\n",
            "Epoch: 9440 | Training Loss: 1.4952641725540161 | Testing Loss: 1.4587819576263428\n",
            "Epoch: 9450 | Training Loss: 1.493691086769104 | Testing Loss: 1.4553472995758057\n",
            "Epoch: 9460 | Training Loss: 1.4921181201934814 | Testing Loss: 1.4519124031066895\n",
            "Epoch: 9470 | Training Loss: 1.4905451536178589 | Testing Loss: 1.448477864265442\n",
            "Epoch: 9480 | Training Loss: 1.4889720678329468 | Testing Loss: 1.4450432062149048\n",
            "Epoch: 9490 | Training Loss: 1.4873991012573242 | Testing Loss: 1.4416083097457886\n",
            "Epoch: 9500 | Training Loss: 1.4858261346817017 | Testing Loss: 1.4381736516952515\n",
            "Epoch: 9510 | Training Loss: 1.484253168106079 | Testing Loss: 1.4347387552261353\n",
            "Epoch: 9520 | Training Loss: 1.4826799631118774 | Testing Loss: 1.4313043355941772\n",
            "Epoch: 9530 | Training Loss: 1.4811069965362549 | Testing Loss: 1.427869439125061\n",
            "Epoch: 9540 | Training Loss: 1.4795340299606323 | Testing Loss: 1.4244345426559448\n",
            "Epoch: 9550 | Training Loss: 1.4779609441757202 | Testing Loss: 1.4210907220840454\n",
            "Epoch: 9560 | Training Loss: 1.4763879776000977 | Testing Loss: 1.4182885885238647\n",
            "Epoch: 9570 | Training Loss: 1.474815011024475 | Testing Loss: 1.415486454963684\n",
            "Epoch: 9580 | Training Loss: 1.473241925239563 | Testing Loss: 1.4126843214035034\n",
            "Epoch: 9590 | Training Loss: 1.4716689586639404 | Testing Loss: 1.4098819494247437\n",
            "Epoch: 9600 | Training Loss: 1.4700958728790283 | Testing Loss: 1.407080054283142\n",
            "Epoch: 9610 | Training Loss: 1.4685229063034058 | Testing Loss: 1.4042779207229614\n",
            "Epoch: 9620 | Training Loss: 1.4669498205184937 | Testing Loss: 1.4014757871627808\n",
            "Epoch: 9630 | Training Loss: 1.4653767347335815 | Testing Loss: 1.398673176765442\n",
            "Epoch: 9640 | Training Loss: 1.463803768157959 | Testing Loss: 1.3958710432052612\n",
            "Epoch: 9650 | Training Loss: 1.4622306823730469 | Testing Loss: 1.3930691480636597\n",
            "Epoch: 9660 | Training Loss: 1.4606577157974243 | Testing Loss: 1.3902667760849\n",
            "Epoch: 9670 | Training Loss: 1.4590847492218018 | Testing Loss: 1.3874648809432983\n",
            "Epoch: 9680 | Training Loss: 1.4575117826461792 | Testing Loss: 1.3846626281738281\n",
            "Epoch: 9690 | Training Loss: 1.455938696861267 | Testing Loss: 1.3818604946136475\n",
            "Epoch: 9700 | Training Loss: 1.454365611076355 | Testing Loss: 1.3790581226348877\n",
            "Epoch: 9710 | Training Loss: 1.4527926445007324 | Testing Loss: 1.376255989074707\n",
            "Epoch: 9720 | Training Loss: 1.4512195587158203 | Testing Loss: 1.373453974723816\n",
            "Epoch: 9730 | Training Loss: 1.4496465921401978 | Testing Loss: 1.3706516027450562\n",
            "Epoch: 9740 | Training Loss: 1.4480736255645752 | Testing Loss: 1.367849588394165\n",
            "Epoch: 9750 | Training Loss: 1.4465006589889526 | Testing Loss: 1.3650473356246948\n",
            "Epoch: 9760 | Training Loss: 1.4449275732040405 | Testing Loss: 1.3622452020645142\n",
            "Epoch: 9770 | Training Loss: 1.4433544874191284 | Testing Loss: 1.3594430685043335\n",
            "Epoch: 9780 | Training Loss: 1.4417814016342163 | Testing Loss: 1.3566408157348633\n",
            "Epoch: 9790 | Training Loss: 1.4402084350585938 | Testing Loss: 1.353838562965393\n",
            "Epoch: 9800 | Training Loss: 1.4386353492736816 | Testing Loss: 1.351036548614502\n",
            "Epoch: 9810 | Training Loss: 1.437062382698059 | Testing Loss: 1.3482345342636108\n",
            "Epoch: 9820 | Training Loss: 1.4354894161224365 | Testing Loss: 1.3454324007034302\n",
            "Epoch: 9830 | Training Loss: 1.4339163303375244 | Testing Loss: 1.3426297903060913\n",
            "Epoch: 9840 | Training Loss: 1.4323433637619019 | Testing Loss: 1.3398276567459106\n",
            "Epoch: 9850 | Training Loss: 1.4307702779769897 | Testing Loss: 1.3370256423950195\n",
            "Epoch: 9860 | Training Loss: 1.4291973114013672 | Testing Loss: 1.3342232704162598\n",
            "Epoch: 9870 | Training Loss: 1.4276243448257446 | Testing Loss: 1.3314213752746582\n",
            "Epoch: 9880 | Training Loss: 1.426051378250122 | Testing Loss: 1.3286192417144775\n",
            "Epoch: 9890 | Training Loss: 1.424687385559082 | Testing Loss: 1.326122760772705\n",
            "Epoch: 9900 | Training Loss: 1.423409104347229 | Testing Loss: 1.3237030506134033\n",
            "Epoch: 9910 | Training Loss: 1.4221309423446655 | Testing Loss: 1.321282982826233\n",
            "Epoch: 9920 | Training Loss: 1.4208526611328125 | Testing Loss: 1.3188633918762207\n",
            "Epoch: 9930 | Training Loss: 1.4195743799209595 | Testing Loss: 1.3164433240890503\n",
            "Epoch: 9940 | Training Loss: 1.4182960987091064 | Testing Loss: 1.3140236139297485\n",
            "Epoch: 9950 | Training Loss: 1.4170178174972534 | Testing Loss: 1.3116039037704468\n",
            "Epoch: 9960 | Training Loss: 1.4157394170761108 | Testing Loss: 1.3091840744018555\n",
            "Epoch: 9970 | Training Loss: 1.4144611358642578 | Testing Loss: 1.3067643642425537\n",
            "Epoch: 9980 | Training Loss: 1.4131829738616943 | Testing Loss: 1.3043444156646729\n",
            "Epoch: 9990 | Training Loss: 1.4119046926498413 | Testing Loss: 1.3019245862960815\n",
            "Epoch: 10000 | Training Loss: 1.4106264114379883 | Testing Loss: 1.2995048761367798\n",
            "Epoch: 10010 | Training Loss: 1.4093483686447144 | Testing Loss: 1.2970848083496094\n",
            "Epoch: 10020 | Training Loss: 1.4080699682235718 | Testing Loss: 1.2946652173995972\n",
            "Epoch: 10030 | Training Loss: 1.4067915678024292 | Testing Loss: 1.2922452688217163\n",
            "Epoch: 10040 | Training Loss: 1.4055134057998657 | Testing Loss: 1.289825439453125\n",
            "Epoch: 10050 | Training Loss: 1.4042350053787231 | Testing Loss: 1.2874056100845337\n",
            "Epoch: 10060 | Training Loss: 1.4029568433761597 | Testing Loss: 1.2849857807159424\n",
            "Epoch: 10070 | Training Loss: 1.4016786813735962 | Testing Loss: 1.282565951347351\n",
            "Epoch: 10080 | Training Loss: 1.4004004001617432 | Testing Loss: 1.2801460027694702\n",
            "Epoch: 10090 | Training Loss: 1.3991221189498901 | Testing Loss: 1.2777262926101685\n",
            "Epoch: 10100 | Training Loss: 1.397843837738037 | Testing Loss: 1.2753065824508667\n",
            "Epoch: 10110 | Training Loss: 1.3965654373168945 | Testing Loss: 1.2728865146636963\n",
            "Epoch: 10120 | Training Loss: 1.3952873945236206 | Testing Loss: 1.270466923713684\n",
            "Epoch: 10130 | Training Loss: 1.394008994102478 | Testing Loss: 1.2680468559265137\n",
            "Epoch: 10140 | Training Loss: 1.392730712890625 | Testing Loss: 1.2656270265579224\n",
            "Epoch: 10150 | Training Loss: 1.391452431678772 | Testing Loss: 1.2632073163986206\n",
            "Epoch: 10160 | Training Loss: 1.390174150466919 | Testing Loss: 1.2607876062393188\n",
            "Epoch: 10170 | Training Loss: 1.388895869255066 | Testing Loss: 1.258367896080017\n",
            "Epoch: 10180 | Training Loss: 1.387617588043213 | Testing Loss: 1.2559479475021362\n",
            "Epoch: 10190 | Training Loss: 1.3863394260406494 | Testing Loss: 1.2535279989242554\n",
            "Epoch: 10200 | Training Loss: 1.3850610256195068 | Testing Loss: 1.2511084079742432\n",
            "Epoch: 10210 | Training Loss: 1.3837827444076538 | Testing Loss: 1.2486883401870728\n",
            "Epoch: 10220 | Training Loss: 1.3825045824050903 | Testing Loss: 1.246268630027771\n",
            "Epoch: 10230 | Training Loss: 1.3812261819839478 | Testing Loss: 1.2438488006591797\n",
            "Epoch: 10240 | Training Loss: 1.3799480199813843 | Testing Loss: 1.2414289712905884\n",
            "Epoch: 10250 | Training Loss: 1.3786696195602417 | Testing Loss: 1.239009141921997\n",
            "Epoch: 10260 | Training Loss: 1.3773914575576782 | Testing Loss: 1.2365893125534058\n",
            "Epoch: 10270 | Training Loss: 1.3761132955551147 | Testing Loss: 1.2343239784240723\n",
            "Epoch: 10280 | Training Loss: 1.3748348951339722 | Testing Loss: 1.23245108127594\n",
            "Epoch: 10290 | Training Loss: 1.3735566139221191 | Testing Loss: 1.2305783033370972\n",
            "Epoch: 10300 | Training Loss: 1.3722784519195557 | Testing Loss: 1.228705644607544\n",
            "Epoch: 10310 | Training Loss: 1.3710001707077026 | Testing Loss: 1.226832628250122\n",
            "Epoch: 10320 | Training Loss: 1.36972177028656 | Testing Loss: 1.2249600887298584\n",
            "Epoch: 10330 | Training Loss: 1.3684437274932861 | Testing Loss: 1.2230870723724365\n",
            "Epoch: 10340 | Training Loss: 1.367165446281433 | Testing Loss: 1.2212142944335938\n",
            "Epoch: 10350 | Training Loss: 1.3658870458602905 | Testing Loss: 1.2193413972854614\n",
            "Epoch: 10360 | Training Loss: 1.3646087646484375 | Testing Loss: 1.2174686193466187\n",
            "Epoch: 10370 | Training Loss: 1.3633304834365845 | Testing Loss: 1.215596079826355\n",
            "Epoch: 10380 | Training Loss: 1.362052321434021 | Testing Loss: 1.213723063468933\n",
            "Epoch: 10390 | Training Loss: 1.3607739210128784 | Testing Loss: 1.2118501663208008\n",
            "Epoch: 10400 | Training Loss: 1.359495759010315 | Testing Loss: 1.2099775075912476\n",
            "Epoch: 10410 | Training Loss: 1.358217477798462 | Testing Loss: 1.2081044912338257\n",
            "Epoch: 10420 | Training Loss: 1.3569391965866089 | Testing Loss: 1.206231951713562\n",
            "Epoch: 10430 | Training Loss: 1.3556607961654663 | Testing Loss: 1.2043590545654297\n",
            "Epoch: 10440 | Training Loss: 1.3543827533721924 | Testing Loss: 1.202486276626587\n",
            "Epoch: 10450 | Training Loss: 1.3531043529510498 | Testing Loss: 1.2006134986877441\n",
            "Epoch: 10460 | Training Loss: 1.3518260717391968 | Testing Loss: 1.1987404823303223\n",
            "Epoch: 10470 | Training Loss: 1.3505480289459229 | Testing Loss: 1.196867823600769\n",
            "Epoch: 10480 | Training Loss: 1.3492696285247803 | Testing Loss: 1.1949950456619263\n",
            "Epoch: 10490 | Training Loss: 1.3479913473129272 | Testing Loss: 1.193122148513794\n",
            "Epoch: 10500 | Training Loss: 1.3467129468917847 | Testing Loss: 1.1912496089935303\n",
            "Epoch: 10510 | Training Loss: 1.3454347848892212 | Testing Loss: 1.1893765926361084\n",
            "Epoch: 10520 | Training Loss: 1.3441565036773682 | Testing Loss: 1.1875039339065552\n",
            "Epoch: 10530 | Training Loss: 1.3428782224655151 | Testing Loss: 1.1856309175491333\n",
            "Epoch: 10540 | Training Loss: 1.3416000604629517 | Testing Loss: 1.1837581396102905\n",
            "Epoch: 10550 | Training Loss: 1.340321660041809 | Testing Loss: 1.1818852424621582\n",
            "Epoch: 10560 | Training Loss: 1.339043378829956 | Testing Loss: 1.180012583732605\n",
            "Epoch: 10570 | Training Loss: 1.337765097618103 | Testing Loss: 1.1781398057937622\n",
            "Epoch: 10580 | Training Loss: 1.33648681640625 | Testing Loss: 1.1762670278549194\n",
            "Epoch: 10590 | Training Loss: 1.335208773612976 | Testing Loss: 1.1743940114974976\n",
            "Epoch: 10600 | Training Loss: 1.3339303731918335 | Testing Loss: 1.1725214719772339\n",
            "Epoch: 10610 | Training Loss: 1.3326520919799805 | Testing Loss: 1.170648455619812\n",
            "Epoch: 10620 | Training Loss: 1.331373929977417 | Testing Loss: 1.1687757968902588\n",
            "Epoch: 10630 | Training Loss: 1.3300954103469849 | Testing Loss: 1.1669028997421265\n",
            "Epoch: 10640 | Training Loss: 1.328921914100647 | Testing Loss: 1.165169596672058\n",
            "Epoch: 10650 | Training Loss: 1.327894687652588 | Testing Loss: 1.1635754108428955\n",
            "Epoch: 10660 | Training Loss: 1.3268674612045288 | Testing Loss: 1.1619813442230225\n",
            "Epoch: 10670 | Training Loss: 1.3258403539657593 | Testing Loss: 1.160387396812439\n",
            "Epoch: 10680 | Training Loss: 1.3248130083084106 | Testing Loss: 1.158793330192566\n",
            "Epoch: 10690 | Training Loss: 1.3237859010696411 | Testing Loss: 1.1571992635726929\n",
            "Epoch: 10700 | Training Loss: 1.322758674621582 | Testing Loss: 1.1556053161621094\n",
            "Epoch: 10710 | Training Loss: 1.321731448173523 | Testing Loss: 1.1540114879608154\n",
            "Epoch: 10720 | Training Loss: 1.3207043409347534 | Testing Loss: 1.1524174213409424\n",
            "Epoch: 10730 | Training Loss: 1.3196762800216675 | Testing Loss: 1.1508222818374634\n",
            "Epoch: 10740 | Training Loss: 1.318648338317871 | Testing Loss: 1.1492273807525635\n",
            "Epoch: 10750 | Training Loss: 1.3176205158233643 | Testing Loss: 1.147632360458374\n",
            "Epoch: 10760 | Training Loss: 1.3165924549102783 | Testing Loss: 1.1460373401641846\n",
            "Epoch: 10770 | Training Loss: 1.315564513206482 | Testing Loss: 1.1444425582885742\n",
            "Epoch: 10780 | Training Loss: 1.314536690711975 | Testing Loss: 1.1428474187850952\n",
            "Epoch: 10790 | Training Loss: 1.3135086297988892 | Testing Loss: 1.1412523984909058\n",
            "Epoch: 10800 | Training Loss: 1.3124808073043823 | Testing Loss: 1.1396576166152954\n",
            "Epoch: 10810 | Training Loss: 1.3114527463912964 | Testing Loss: 1.138062596321106\n",
            "Epoch: 10820 | Training Loss: 1.3104246854782104 | Testing Loss: 1.1364678144454956\n",
            "Epoch: 10830 | Training Loss: 1.3093969821929932 | Testing Loss: 1.134872555732727\n",
            "Epoch: 10840 | Training Loss: 1.3083689212799072 | Testing Loss: 1.1332777738571167\n",
            "Epoch: 10850 | Training Loss: 1.3073409795761108 | Testing Loss: 1.1316827535629272\n",
            "Epoch: 10860 | Training Loss: 1.306312918663025 | Testing Loss: 1.1300877332687378\n",
            "Epoch: 10870 | Training Loss: 1.3052852153778076 | Testing Loss: 1.128492832183838\n",
            "Epoch: 10880 | Training Loss: 1.3042571544647217 | Testing Loss: 1.1268978118896484\n",
            "Epoch: 10890 | Training Loss: 1.3032292127609253 | Testing Loss: 1.1253029108047485\n",
            "Epoch: 10900 | Training Loss: 1.302201271057129 | Testing Loss: 1.1237080097198486\n",
            "Epoch: 10910 | Training Loss: 1.301173210144043 | Testing Loss: 1.1221128702163696\n",
            "Epoch: 10920 | Training Loss: 1.3001452684402466 | Testing Loss: 1.1205180883407593\n",
            "Epoch: 10930 | Training Loss: 1.2991174459457397 | Testing Loss: 1.1189230680465698\n",
            "Epoch: 10940 | Training Loss: 1.2980893850326538 | Testing Loss: 1.1173280477523804\n",
            "Epoch: 10950 | Training Loss: 1.297061562538147 | Testing Loss: 1.115733027458191\n",
            "Epoch: 10960 | Training Loss: 1.296033501625061 | Testing Loss: 1.1141380071640015\n",
            "Epoch: 10970 | Training Loss: 1.2950056791305542 | Testing Loss: 1.1125433444976807\n",
            "Epoch: 10980 | Training Loss: 1.2939777374267578 | Testing Loss: 1.1109482049942017\n",
            "Epoch: 10990 | Training Loss: 1.2929496765136719 | Testing Loss: 1.1093531847000122\n",
            "Epoch: 11000 | Training Loss: 1.291921854019165 | Testing Loss: 1.1077584028244019\n",
            "Epoch: 11010 | Training Loss: 1.2908939123153687 | Testing Loss: 1.1061633825302124\n",
            "Epoch: 11020 | Training Loss: 1.2898658514022827 | Testing Loss: 1.1045684814453125\n",
            "Epoch: 11030 | Training Loss: 1.2888380289077759 | Testing Loss: 1.1029733419418335\n",
            "Epoch: 11040 | Training Loss: 1.2878100872039795 | Testing Loss: 1.1013784408569336\n",
            "Epoch: 11050 | Training Loss: 1.286782145500183 | Testing Loss: 1.0997835397720337\n",
            "Epoch: 11060 | Training Loss: 1.2857542037963867 | Testing Loss: 1.0981884002685547\n",
            "Epoch: 11070 | Training Loss: 1.2847262620925903 | Testing Loss: 1.0965936183929443\n",
            "Epoch: 11080 | Training Loss: 1.2837656736373901 | Testing Loss: 1.095111608505249\n",
            "Epoch: 11090 | Training Loss: 1.2829363346099854 | Testing Loss: 1.0937989950180054\n",
            "Epoch: 11100 | Training Loss: 1.282106876373291 | Testing Loss: 1.0924867391586304\n",
            "Epoch: 11110 | Training Loss: 1.2812775373458862 | Testing Loss: 1.0911742448806763\n",
            "Epoch: 11120 | Training Loss: 1.2804481983184814 | Testing Loss: 1.0898617506027222\n",
            "Epoch: 11130 | Training Loss: 1.2796188592910767 | Testing Loss: 1.088549256324768\n",
            "Epoch: 11140 | Training Loss: 1.2787894010543823 | Testing Loss: 1.0872368812561035\n",
            "Epoch: 11150 | Training Loss: 1.2779600620269775 | Testing Loss: 1.085924506187439\n",
            "Epoch: 11160 | Training Loss: 1.2771306037902832 | Testing Loss: 1.0846120119094849\n",
            "Epoch: 11170 | Training Loss: 1.2763012647628784 | Testing Loss: 1.0832995176315308\n",
            "Epoch: 11180 | Training Loss: 1.2754719257354736 | Testing Loss: 1.0819871425628662\n",
            "Epoch: 11190 | Training Loss: 1.2746425867080688 | Testing Loss: 1.080674648284912\n",
            "Epoch: 11200 | Training Loss: 1.2738131284713745 | Testing Loss: 1.0793622732162476\n",
            "Epoch: 11210 | Training Loss: 1.2729837894439697 | Testing Loss: 1.078049659729004\n",
            "Epoch: 11220 | Training Loss: 1.2721543312072754 | Testing Loss: 1.0767372846603394\n",
            "Epoch: 11230 | Training Loss: 1.2713249921798706 | Testing Loss: 1.0754249095916748\n",
            "Epoch: 11240 | Training Loss: 1.2704956531524658 | Testing Loss: 1.0741124153137207\n",
            "Epoch: 11250 | Training Loss: 1.269666314125061 | Testing Loss: 1.0727999210357666\n",
            "Epoch: 11260 | Training Loss: 1.2688368558883667 | Testing Loss: 1.071487545967102\n",
            "Epoch: 11270 | Training Loss: 1.2680076360702515 | Testing Loss: 1.070175051689148\n",
            "Epoch: 11280 | Training Loss: 1.2671781778335571 | Testing Loss: 1.0688626766204834\n",
            "Epoch: 11290 | Training Loss: 1.2663488388061523 | Testing Loss: 1.0675500631332397\n",
            "Epoch: 11300 | Training Loss: 1.265519380569458 | Testing Loss: 1.0662378072738647\n",
            "Epoch: 11310 | Training Loss: 1.2646900415420532 | Testing Loss: 1.0649253129959106\n",
            "Epoch: 11320 | Training Loss: 1.2638605833053589 | Testing Loss: 1.0636128187179565\n",
            "Epoch: 11330 | Training Loss: 1.2630313634872437 | Testing Loss: 1.0623003244400024\n",
            "Epoch: 11340 | Training Loss: 1.2622019052505493 | Testing Loss: 1.060987949371338\n",
            "Epoch: 11350 | Training Loss: 1.2613725662231445 | Testing Loss: 1.0596755743026733\n",
            "Epoch: 11360 | Training Loss: 1.2605432271957397 | Testing Loss: 1.0583630800247192\n",
            "Epoch: 11370 | Training Loss: 1.2597137689590454 | Testing Loss: 1.0570505857467651\n",
            "Epoch: 11380 | Training Loss: 1.258884310722351 | Testing Loss: 1.0557382106781006\n",
            "Epoch: 11390 | Training Loss: 1.2580550909042358 | Testing Loss: 1.0544257164001465\n",
            "Epoch: 11400 | Training Loss: 1.2572256326675415 | Testing Loss: 1.053113341331482\n",
            "Epoch: 11410 | Training Loss: 1.2563962936401367 | Testing Loss: 1.0518007278442383\n",
            "Epoch: 11420 | Training Loss: 1.255566954612732 | Testing Loss: 1.0504883527755737\n",
            "Epoch: 11430 | Training Loss: 1.2547376155853271 | Testing Loss: 1.0491759777069092\n",
            "Epoch: 11440 | Training Loss: 1.2539081573486328 | Testing Loss: 1.047863483428955\n",
            "Epoch: 11450 | Training Loss: 1.253078818321228 | Testing Loss: 1.046550989151001\n",
            "Epoch: 11460 | Training Loss: 1.2522493600845337 | Testing Loss: 1.0452386140823364\n",
            "Epoch: 11470 | Training Loss: 1.251420021057129 | Testing Loss: 1.0439261198043823\n",
            "Epoch: 11480 | Training Loss: 1.2505906820297241 | Testing Loss: 1.0426137447357178\n",
            "Epoch: 11490 | Training Loss: 1.2497613430023193 | Testing Loss: 1.0413011312484741\n",
            "Epoch: 11500 | Training Loss: 1.248931884765625 | Testing Loss: 1.0399888753890991\n",
            "Epoch: 11510 | Training Loss: 1.2481025457382202 | Testing Loss: 1.038676381111145\n",
            "Epoch: 11520 | Training Loss: 1.2472732067108154 | Testing Loss: 1.037363886833191\n",
            "Epoch: 11530 | Training Loss: 1.246443748474121 | Testing Loss: 1.0360513925552368\n",
            "Epoch: 11540 | Training Loss: 1.2456144094467163 | Testing Loss: 1.0347390174865723\n",
            "Epoch: 11550 | Training Loss: 1.2447850704193115 | Testing Loss: 1.0334266424179077\n",
            "Epoch: 11560 | Training Loss: 1.2439556121826172 | Testing Loss: 1.0321141481399536\n",
            "Epoch: 11570 | Training Loss: 1.2431262731552124 | Testing Loss: 1.0308016538619995\n",
            "Epoch: 11580 | Training Loss: 1.2422969341278076 | Testing Loss: 1.029489278793335\n",
            "Epoch: 11590 | Training Loss: 1.2414675951004028 | Testing Loss: 1.0281767845153809\n",
            "Epoch: 11600 | Training Loss: 1.2406381368637085 | Testing Loss: 1.0268644094467163\n",
            "Epoch: 11610 | Training Loss: 1.2398089170455933 | Testing Loss: 1.0255517959594727\n",
            "Epoch: 11620 | Training Loss: 1.2389793395996094 | Testing Loss: 1.024239420890808\n",
            "Epoch: 11630 | Training Loss: 1.2381500005722046 | Testing Loss: 1.0229270458221436\n",
            "Epoch: 11640 | Training Loss: 1.2373840808868408 | Testing Loss: 1.0217586755752563\n",
            "Epoch: 11650 | Training Loss: 1.2367006540298462 | Testing Loss: 1.020734190940857\n",
            "Epoch: 11660 | Training Loss: 1.2360174655914307 | Testing Loss: 1.0197100639343262\n",
            "Epoch: 11670 | Training Loss: 1.235334038734436 | Testing Loss: 1.0186855792999268\n",
            "Epoch: 11680 | Training Loss: 1.2346508502960205 | Testing Loss: 1.0176613330841064\n",
            "Epoch: 11690 | Training Loss: 1.2339674234390259 | Testing Loss: 1.0166370868682861\n",
            "Epoch: 11700 | Training Loss: 1.2332842350006104 | Testing Loss: 1.0156126022338867\n",
            "Epoch: 11710 | Training Loss: 1.2326008081436157 | Testing Loss: 1.014588475227356\n",
            "Epoch: 11720 | Training Loss: 1.231917381286621 | Testing Loss: 1.0135639905929565\n",
            "Epoch: 11730 | Training Loss: 1.2312341928482056 | Testing Loss: 1.0125397443771362\n",
            "Epoch: 11740 | Training Loss: 1.2305508852005005 | Testing Loss: 1.011515498161316\n",
            "Epoch: 11750 | Training Loss: 1.2298675775527954 | Testing Loss: 1.0104910135269165\n",
            "Epoch: 11760 | Training Loss: 1.2291842699050903 | Testing Loss: 1.0094668865203857\n",
            "Epoch: 11770 | Training Loss: 1.2285008430480957 | Testing Loss: 1.0084422826766968\n",
            "Epoch: 11780 | Training Loss: 1.2278175354003906 | Testing Loss: 1.0074180364608765\n",
            "Epoch: 11790 | Training Loss: 1.227134346961975 | Testing Loss: 1.0063937902450562\n",
            "Epoch: 11800 | Training Loss: 1.2264509201049805 | Testing Loss: 1.0053694248199463\n",
            "Epoch: 11810 | Training Loss: 1.2257676124572754 | Testing Loss: 1.0043452978134155\n",
            "Epoch: 11820 | Training Loss: 1.2250841856002808 | Testing Loss: 1.0033206939697266\n",
            "Epoch: 11830 | Training Loss: 1.2244009971618652 | Testing Loss: 1.0022964477539062\n",
            "Epoch: 11840 | Training Loss: 1.2237176895141602 | Testing Loss: 1.001272201538086\n",
            "Epoch: 11850 | Training Loss: 1.2230342626571655 | Testing Loss: 1.000247836112976\n",
            "Epoch: 11860 | Training Loss: 1.22235107421875 | Testing Loss: 0.9992236495018005\n",
            "Epoch: 11870 | Training Loss: 1.2216676473617554 | Testing Loss: 0.9981991052627563\n",
            "Epoch: 11880 | Training Loss: 1.2209844589233398 | Testing Loss: 0.997174859046936\n",
            "Epoch: 11890 | Training Loss: 1.2203010320663452 | Testing Loss: 0.9961506128311157\n",
            "Epoch: 11900 | Training Loss: 1.2196178436279297 | Testing Loss: 0.9951261878013611\n",
            "Epoch: 11910 | Training Loss: 1.218934416770935 | Testing Loss: 0.9941020011901855\n",
            "Epoch: 11920 | Training Loss: 1.21825110912323 | Testing Loss: 0.9930774569511414\n",
            "Epoch: 11930 | Training Loss: 1.217567801475525 | Testing Loss: 0.992053210735321\n",
            "Epoch: 11940 | Training Loss: 1.2168844938278198 | Testing Loss: 0.9910289645195007\n",
            "Epoch: 11950 | Training Loss: 1.2162011861801147 | Testing Loss: 0.9900045394897461\n",
            "Epoch: 11960 | Training Loss: 1.2155178785324097 | Testing Loss: 0.9889804124832153\n",
            "Epoch: 11970 | Training Loss: 1.2148345708847046 | Testing Loss: 0.9879558682441711\n",
            "Epoch: 11980 | Training Loss: 1.2141512632369995 | Testing Loss: 0.9869316220283508\n",
            "Epoch: 11990 | Training Loss: 1.2134679555892944 | Testing Loss: 0.9859073758125305\n",
            "Epoch: 12000 | Training Loss: 1.2127846479415894 | Testing Loss: 0.9848829507827759\n",
            "Epoch: 12010 | Training Loss: 1.2121013402938843 | Testing Loss: 0.9838587641716003\n",
            "Epoch: 12020 | Training Loss: 1.2114179134368896 | Testing Loss: 0.9828342795372009\n",
            "Epoch: 12030 | Training Loss: 1.2107347249984741 | Testing Loss: 0.9818100333213806\n",
            "Epoch: 12040 | Training Loss: 1.210051417350769 | Testing Loss: 0.9807857871055603\n",
            "Epoch: 12050 | Training Loss: 1.2093679904937744 | Testing Loss: 0.9797613024711609\n",
            "Epoch: 12060 | Training Loss: 1.2086848020553589 | Testing Loss: 0.9787371754646301\n",
            "Epoch: 12070 | Training Loss: 1.2080013751983643 | Testing Loss: 0.9777126312255859\n",
            "Epoch: 12080 | Training Loss: 1.2073181867599487 | Testing Loss: 0.9766883850097656\n",
            "Epoch: 12090 | Training Loss: 1.206634759902954 | Testing Loss: 0.9756641387939453\n",
            "Epoch: 12100 | Training Loss: 1.2059515714645386 | Testing Loss: 0.9746397137641907\n",
            "Epoch: 12110 | Training Loss: 1.205268144607544 | Testing Loss: 0.9736155867576599\n",
            "Epoch: 12120 | Training Loss: 1.2045847177505493 | Testing Loss: 0.9725910425186157\n",
            "Epoch: 12130 | Training Loss: 1.2039015293121338 | Testing Loss: 0.9715667963027954\n",
            "Epoch: 12140 | Training Loss: 1.2032182216644287 | Testing Loss: 0.9705425500869751\n",
            "Epoch: 12150 | Training Loss: 1.2025349140167236 | Testing Loss: 0.9695181250572205\n",
            "Epoch: 12160 | Training Loss: 1.2018516063690186 | Testing Loss: 0.9684939384460449\n",
            "Epoch: 12170 | Training Loss: 1.201168179512024 | Testing Loss: 0.9674693942070007\n",
            "Epoch: 12180 | Training Loss: 1.2004848718643188 | Testing Loss: 0.9664451479911804\n",
            "Epoch: 12190 | Training Loss: 1.1998016834259033 | Testing Loss: 0.9654209017753601\n",
            "Epoch: 12200 | Training Loss: 1.1991182565689087 | Testing Loss: 0.9643964767456055\n",
            "Epoch: 12210 | Training Loss: 1.1984349489212036 | Testing Loss: 0.9633723497390747\n",
            "Epoch: 12220 | Training Loss: 1.197751522064209 | Testing Loss: 0.9623478055000305\n",
            "Epoch: 12230 | Training Loss: 1.1970683336257935 | Testing Loss: 0.9613235592842102\n",
            "Epoch: 12240 | Training Loss: 1.1963850259780884 | Testing Loss: 0.9602993130683899\n",
            "Epoch: 12250 | Training Loss: 1.1957015991210938 | Testing Loss: 0.9592748880386353\n",
            "Epoch: 12260 | Training Loss: 1.1950184106826782 | Testing Loss: 0.9582507014274597\n",
            "Epoch: 12270 | Training Loss: 1.1943349838256836 | Testing Loss: 0.9572262167930603\n",
            "Epoch: 12280 | Training Loss: 1.193651795387268 | Testing Loss: 0.95620197057724\n",
            "Epoch: 12290 | Training Loss: 1.1929683685302734 | Testing Loss: 0.9551777243614197\n",
            "Epoch: 12300 | Training Loss: 1.192285180091858 | Testing Loss: 0.9541532397270203\n",
            "Epoch: 12310 | Training Loss: 1.1916017532348633 | Testing Loss: 0.9531291127204895\n",
            "Epoch: 12320 | Training Loss: 1.1909183263778687 | Testing Loss: 0.9521045684814453\n",
            "Epoch: 12330 | Training Loss: 1.1902351379394531 | Testing Loss: 0.951080322265625\n",
            "Epoch: 12340 | Training Loss: 1.189551830291748 | Testing Loss: 0.9500560760498047\n",
            "Epoch: 12350 | Training Loss: 1.188868522644043 | Testing Loss: 0.94903165102005\n",
            "Epoch: 12360 | Training Loss: 1.188185214996338 | Testing Loss: 0.9480075240135193\n",
            "Epoch: 12370 | Training Loss: 1.1875017881393433 | Testing Loss: 0.9469829797744751\n",
            "Epoch: 12380 | Training Loss: 1.1868184804916382 | Testing Loss: 0.9459587335586548\n",
            "Epoch: 12390 | Training Loss: 1.1861352920532227 | Testing Loss: 0.9449344873428345\n",
            "Epoch: 12400 | Training Loss: 1.185451865196228 | Testing Loss: 0.9439100623130798\n",
            "Epoch: 12410 | Training Loss: 1.184768557548523 | Testing Loss: 0.9428858757019043\n",
            "Epoch: 12420 | Training Loss: 1.1840852499008179 | Testing Loss: 0.9418613314628601\n",
            "Epoch: 12430 | Training Loss: 1.1834667921066284 | Testing Loss: 0.9410691261291504\n",
            "Epoch: 12440 | Training Loss: 1.182873010635376 | Testing Loss: 0.9403347969055176\n",
            "Epoch: 12450 | Training Loss: 1.1822794675827026 | Testing Loss: 0.9396004676818848\n",
            "Epoch: 12460 | Training Loss: 1.1816856861114502 | Testing Loss: 0.938866138458252\n",
            "Epoch: 12470 | Training Loss: 1.1810921430587769 | Testing Loss: 0.9381318092346191\n",
            "Epoch: 12480 | Training Loss: 1.1804983615875244 | Testing Loss: 0.9373974800109863\n",
            "Epoch: 12490 | Training Loss: 1.179904818534851 | Testing Loss: 0.9366631507873535\n",
            "Epoch: 12500 | Training Loss: 1.1793110370635986 | Testing Loss: 0.9359288215637207\n",
            "Epoch: 12510 | Training Loss: 1.1787174940109253 | Testing Loss: 0.9351944923400879\n",
            "Epoch: 12520 | Training Loss: 1.1781237125396729 | Testing Loss: 0.9344601631164551\n",
            "Epoch: 12530 | Training Loss: 1.1775301694869995 | Testing Loss: 0.9337258338928223\n",
            "Epoch: 12540 | Training Loss: 1.176936388015747 | Testing Loss: 0.9329915046691895\n",
            "Epoch: 12550 | Training Loss: 1.1763428449630737 | Testing Loss: 0.9322571754455566\n",
            "Epoch: 12560 | Training Loss: 1.1757490634918213 | Testing Loss: 0.9315228462219238\n",
            "Epoch: 12570 | Training Loss: 1.175155520439148 | Testing Loss: 0.930788516998291\n",
            "Epoch: 12580 | Training Loss: 1.1745617389678955 | Testing Loss: 0.9300541877746582\n",
            "Epoch: 12590 | Training Loss: 1.1739681959152222 | Testing Loss: 0.9293198585510254\n",
            "Epoch: 12600 | Training Loss: 1.1733744144439697 | Testing Loss: 0.9285855293273926\n",
            "Epoch: 12610 | Training Loss: 1.1727808713912964 | Testing Loss: 0.9278512001037598\n",
            "Epoch: 12620 | Training Loss: 1.172187089920044 | Testing Loss: 0.927116870880127\n",
            "Epoch: 12630 | Training Loss: 1.1715935468673706 | Testing Loss: 0.9263825416564941\n",
            "Epoch: 12640 | Training Loss: 1.1709997653961182 | Testing Loss: 0.9256482124328613\n",
            "Epoch: 12650 | Training Loss: 1.1704062223434448 | Testing Loss: 0.9249138832092285\n",
            "Epoch: 12660 | Training Loss: 1.1698124408721924 | Testing Loss: 0.9241795539855957\n",
            "Epoch: 12670 | Training Loss: 1.169218897819519 | Testing Loss: 0.9234452247619629\n",
            "Epoch: 12680 | Training Loss: 1.1686251163482666 | Testing Loss: 0.9227108955383301\n",
            "Epoch: 12690 | Training Loss: 1.1680315732955933 | Testing Loss: 0.9219765663146973\n",
            "Epoch: 12700 | Training Loss: 1.1674377918243408 | Testing Loss: 0.9212422370910645\n",
            "Epoch: 12710 | Training Loss: 1.1668442487716675 | Testing Loss: 0.9205079078674316\n",
            "Epoch: 12720 | Training Loss: 1.166250467300415 | Testing Loss: 0.9197735786437988\n",
            "Epoch: 12730 | Training Loss: 1.1656569242477417 | Testing Loss: 0.919039249420166\n",
            "Epoch: 12740 | Training Loss: 1.1650631427764893 | Testing Loss: 0.9183049201965332\n",
            "Epoch: 12750 | Training Loss: 1.164469599723816 | Testing Loss: 0.9175705909729004\n",
            "Epoch: 12760 | Training Loss: 1.1638758182525635 | Testing Loss: 0.9168362617492676\n",
            "Epoch: 12770 | Training Loss: 1.1632822751998901 | Testing Loss: 0.9161019325256348\n",
            "Epoch: 12780 | Training Loss: 1.1626884937286377 | Testing Loss: 0.915367603302002\n",
            "Epoch: 12790 | Training Loss: 1.1620949506759644 | Testing Loss: 0.9146332740783691\n",
            "Epoch: 12800 | Training Loss: 1.161501169204712 | Testing Loss: 0.9138989448547363\n",
            "Epoch: 12810 | Training Loss: 1.1609076261520386 | Testing Loss: 0.9131646156311035\n",
            "Epoch: 12820 | Training Loss: 1.1603138446807861 | Testing Loss: 0.9124302864074707\n",
            "Epoch: 12830 | Training Loss: 1.1597203016281128 | Testing Loss: 0.9116959571838379\n",
            "Epoch: 12840 | Training Loss: 1.1591265201568604 | Testing Loss: 0.9109616279602051\n",
            "Epoch: 12850 | Training Loss: 1.158532977104187 | Testing Loss: 0.9102272987365723\n",
            "Epoch: 12860 | Training Loss: 1.1579391956329346 | Testing Loss: 0.9094929695129395\n",
            "Epoch: 12870 | Training Loss: 1.1573456525802612 | Testing Loss: 0.9087586402893066\n",
            "Epoch: 12880 | Training Loss: 1.1567518711090088 | Testing Loss: 0.9080243110656738\n",
            "Epoch: 12890 | Training Loss: 1.1561583280563354 | Testing Loss: 0.907289981842041\n",
            "Epoch: 12900 | Training Loss: 1.155564546585083 | Testing Loss: 0.9065556526184082\n",
            "Epoch: 12910 | Training Loss: 1.1549710035324097 | Testing Loss: 0.9058213233947754\n",
            "Epoch: 12920 | Training Loss: 1.1543772220611572 | Testing Loss: 0.9050869941711426\n",
            "Epoch: 12930 | Training Loss: 1.1537836790084839 | Testing Loss: 0.9043526649475098\n",
            "Epoch: 12940 | Training Loss: 1.1531898975372314 | Testing Loss: 0.903618335723877\n",
            "Epoch: 12950 | Training Loss: 1.152596354484558 | Testing Loss: 0.9028840065002441\n",
            "Epoch: 12960 | Training Loss: 1.1520025730133057 | Testing Loss: 0.9021496772766113\n",
            "Epoch: 12970 | Training Loss: 1.1514090299606323 | Testing Loss: 0.9014153480529785\n",
            "Epoch: 12980 | Training Loss: 1.1508152484893799 | Testing Loss: 0.9006810188293457\n",
            "Epoch: 12990 | Training Loss: 1.1502217054367065 | Testing Loss: 0.8999466896057129\n",
            "Epoch: 13000 | Training Loss: 1.149627923965454 | Testing Loss: 0.8992123603820801\n",
            "Epoch: 13010 | Training Loss: 1.1490343809127808 | Testing Loss: 0.8984780311584473\n",
            "Epoch: 13020 | Training Loss: 1.1484405994415283 | Testing Loss: 0.8977437019348145\n",
            "Epoch: 13030 | Training Loss: 1.147847056388855 | Testing Loss: 0.8970093727111816\n",
            "Epoch: 13040 | Training Loss: 1.1472532749176025 | Testing Loss: 0.8962750434875488\n",
            "Epoch: 13050 | Training Loss: 1.1466597318649292 | Testing Loss: 0.895540714263916\n",
            "Epoch: 13060 | Training Loss: 1.1460659503936768 | Testing Loss: 0.8948063850402832\n",
            "Epoch: 13070 | Training Loss: 1.1454724073410034 | Testing Loss: 0.8940720558166504\n",
            "Epoch: 13080 | Training Loss: 1.144878625869751 | Testing Loss: 0.8933377265930176\n",
            "Epoch: 13090 | Training Loss: 1.1442850828170776 | Testing Loss: 0.8926033973693848\n",
            "Epoch: 13100 | Training Loss: 1.1436913013458252 | Testing Loss: 0.891869068145752\n",
            "Epoch: 13110 | Training Loss: 1.1430977582931519 | Testing Loss: 0.8911347389221191\n",
            "Epoch: 13120 | Training Loss: 1.1425039768218994 | Testing Loss: 0.8904004096984863\n",
            "Epoch: 13130 | Training Loss: 1.141910433769226 | Testing Loss: 0.8896660804748535\n",
            "Epoch: 13140 | Training Loss: 1.1413166522979736 | Testing Loss: 0.8889317512512207\n",
            "Epoch: 13150 | Training Loss: 1.1407231092453003 | Testing Loss: 0.8881974220275879\n",
            "Epoch: 13160 | Training Loss: 1.1401293277740479 | Testing Loss: 0.8874630928039551\n",
            "Epoch: 13170 | Training Loss: 1.1395357847213745 | Testing Loss: 0.8867287635803223\n",
            "Epoch: 13180 | Training Loss: 1.138942003250122 | Testing Loss: 0.8859944343566895\n",
            "Epoch: 13190 | Training Loss: 1.1383484601974487 | Testing Loss: 0.8852601051330566\n",
            "Epoch: 13200 | Training Loss: 1.1377546787261963 | Testing Loss: 0.8845257759094238\n",
            "Epoch: 13210 | Training Loss: 1.137161135673523 | Testing Loss: 0.883791446685791\n",
            "Epoch: 13220 | Training Loss: 1.1365673542022705 | Testing Loss: 0.8830571174621582\n",
            "Epoch: 13230 | Training Loss: 1.1359738111495972 | Testing Loss: 0.8823227882385254\n",
            "Epoch: 13240 | Training Loss: 1.1353800296783447 | Testing Loss: 0.8815884590148926\n",
            "Epoch: 13250 | Training Loss: 1.1347863674163818 | Testing Loss: 0.8808541297912598\n",
            "Epoch: 13260 | Training Loss: 1.134192705154419 | Testing Loss: 0.880119800567627\n",
            "Epoch: 13270 | Training Loss: 1.133599042892456 | Testing Loss: 0.8793854713439941\n",
            "Epoch: 13280 | Training Loss: 1.1330053806304932 | Testing Loss: 0.8786511421203613\n",
            "Epoch: 13290 | Training Loss: 1.1324117183685303 | Testing Loss: 0.8779168128967285\n",
            "Epoch: 13300 | Training Loss: 1.1318180561065674 | Testing Loss: 0.8771824836730957\n",
            "Epoch: 13310 | Training Loss: 1.1312243938446045 | Testing Loss: 0.8764481544494629\n",
            "Epoch: 13320 | Training Loss: 1.1306307315826416 | Testing Loss: 0.8757138252258301\n",
            "Epoch: 13330 | Training Loss: 1.1300370693206787 | Testing Loss: 0.8749794960021973\n",
            "Epoch: 13340 | Training Loss: 1.1294434070587158 | Testing Loss: 0.8742451667785645\n",
            "Epoch: 13350 | Training Loss: 1.128849744796753 | Testing Loss: 0.8735108375549316\n",
            "Epoch: 13360 | Training Loss: 1.12825608253479 | Testing Loss: 0.8727765083312988\n",
            "Epoch: 13370 | Training Loss: 1.1276624202728271 | Testing Loss: 0.872042179107666\n",
            "Epoch: 13380 | Training Loss: 1.1270687580108643 | Testing Loss: 0.8713078498840332\n",
            "Epoch: 13390 | Training Loss: 1.1264750957489014 | Testing Loss: 0.8705735206604004\n",
            "Epoch: 13400 | Training Loss: 1.1258814334869385 | Testing Loss: 0.8698391914367676\n",
            "Epoch: 13410 | Training Loss: 1.1252877712249756 | Testing Loss: 0.8691048622131348\n",
            "Epoch: 13420 | Training Loss: 1.1246941089630127 | Testing Loss: 0.868370532989502\n",
            "Epoch: 13430 | Training Loss: 1.1241004467010498 | Testing Loss: 0.8676362037658691\n",
            "Epoch: 13440 | Training Loss: 1.123506784439087 | Testing Loss: 0.8669018745422363\n",
            "Epoch: 13450 | Training Loss: 1.122913122177124 | Testing Loss: 0.8661675453186035\n",
            "Epoch: 13460 | Training Loss: 1.1223194599151611 | Testing Loss: 0.8654332160949707\n",
            "Epoch: 13470 | Training Loss: 1.1217257976531982 | Testing Loss: 0.8646988868713379\n",
            "Epoch: 13480 | Training Loss: 1.1211321353912354 | Testing Loss: 0.8639645576477051\n",
            "Epoch: 13490 | Training Loss: 1.1205384731292725 | Testing Loss: 0.8632302284240723\n",
            "Epoch: 13500 | Training Loss: 1.1199448108673096 | Testing Loss: 0.8624958992004395\n",
            "Epoch: 13510 | Training Loss: 1.1193511486053467 | Testing Loss: 0.8617615699768066\n",
            "Epoch: 13520 | Training Loss: 1.1187574863433838 | Testing Loss: 0.8610272407531738\n",
            "Epoch: 13530 | Training Loss: 1.118163824081421 | Testing Loss: 0.860292911529541\n",
            "Epoch: 13540 | Training Loss: 1.117570161819458 | Testing Loss: 0.8595585823059082\n",
            "Epoch: 13550 | Training Loss: 1.1169764995574951 | Testing Loss: 0.8588242530822754\n",
            "Epoch: 13560 | Training Loss: 1.1163828372955322 | Testing Loss: 0.8580899238586426\n",
            "Epoch: 13570 | Training Loss: 1.1157891750335693 | Testing Loss: 0.8573555946350098\n",
            "Epoch: 13580 | Training Loss: 1.1151955127716064 | Testing Loss: 0.856621265411377\n",
            "Epoch: 13590 | Training Loss: 1.1146018505096436 | Testing Loss: 0.8558869361877441\n",
            "Epoch: 13600 | Training Loss: 1.1140081882476807 | Testing Loss: 0.8551526069641113\n",
            "Epoch: 13610 | Training Loss: 1.1134145259857178 | Testing Loss: 0.8544182777404785\n",
            "Epoch: 13620 | Training Loss: 1.1128208637237549 | Testing Loss: 0.8536839485168457\n",
            "Epoch: 13630 | Training Loss: 1.112227201461792 | Testing Loss: 0.8529496192932129\n",
            "Epoch: 13640 | Training Loss: 1.111633539199829 | Testing Loss: 0.8522152900695801\n",
            "Epoch: 13650 | Training Loss: 1.1110398769378662 | Testing Loss: 0.8514809608459473\n",
            "Epoch: 13660 | Training Loss: 1.1104462146759033 | Testing Loss: 0.8507466316223145\n",
            "Epoch: 13670 | Training Loss: 1.1098525524139404 | Testing Loss: 0.8500123023986816\n",
            "Epoch: 13680 | Training Loss: 1.1092588901519775 | Testing Loss: 0.8492779731750488\n",
            "Epoch: 13690 | Training Loss: 1.1086652278900146 | Testing Loss: 0.848543643951416\n",
            "Epoch: 13700 | Training Loss: 1.1080715656280518 | Testing Loss: 0.8478093147277832\n",
            "Epoch: 13710 | Training Loss: 1.1074779033660889 | Testing Loss: 0.8470749855041504\n",
            "Epoch: 13720 | Training Loss: 1.106884241104126 | Testing Loss: 0.8463406562805176\n",
            "Epoch: 13730 | Training Loss: 1.106290578842163 | Testing Loss: 0.8456063270568848\n",
            "Epoch: 13740 | Training Loss: 1.1056969165802002 | Testing Loss: 0.844871997833252\n",
            "Epoch: 13750 | Training Loss: 1.1051032543182373 | Testing Loss: 0.8441376686096191\n",
            "Epoch: 13760 | Training Loss: 1.1045095920562744 | Testing Loss: 0.8434033393859863\n",
            "Epoch: 13770 | Training Loss: 1.1039159297943115 | Testing Loss: 0.8426690101623535\n",
            "Epoch: 13780 | Training Loss: 1.1033222675323486 | Testing Loss: 0.8419346809387207\n",
            "Epoch: 13790 | Training Loss: 1.1027286052703857 | Testing Loss: 0.8412003517150879\n",
            "Epoch: 13800 | Training Loss: 1.1021349430084229 | Testing Loss: 0.8404660224914551\n",
            "Epoch: 13810 | Training Loss: 1.10154128074646 | Testing Loss: 0.8397316932678223\n",
            "Epoch: 13820 | Training Loss: 1.100947618484497 | Testing Loss: 0.8389973640441895\n",
            "Epoch: 13830 | Training Loss: 1.1003572940826416 | Testing Loss: 0.8383218050003052\n",
            "Epoch: 13840 | Training Loss: 1.0997968912124634 | Testing Loss: 0.8378812074661255\n",
            "Epoch: 13850 | Training Loss: 1.0992364883422852 | Testing Loss: 0.8374406099319458\n",
            "Epoch: 13860 | Training Loss: 1.098676085472107 | Testing Loss: 0.8370000123977661\n",
            "Epoch: 13870 | Training Loss: 1.0981156826019287 | Testing Loss: 0.8365594148635864\n",
            "Epoch: 13880 | Training Loss: 1.0975552797317505 | Testing Loss: 0.8361188173294067\n",
            "Epoch: 13890 | Training Loss: 1.0969948768615723 | Testing Loss: 0.835678219795227\n",
            "Epoch: 13900 | Training Loss: 1.096434473991394 | Testing Loss: 0.8352376222610474\n",
            "Epoch: 13910 | Training Loss: 1.0958740711212158 | Testing Loss: 0.8347970247268677\n",
            "Epoch: 13920 | Training Loss: 1.0953136682510376 | Testing Loss: 0.834356427192688\n",
            "Epoch: 13930 | Training Loss: 1.0947532653808594 | Testing Loss: 0.8339158296585083\n",
            "Epoch: 13940 | Training Loss: 1.0941928625106812 | Testing Loss: 0.8334752321243286\n",
            "Epoch: 13950 | Training Loss: 1.093632459640503 | Testing Loss: 0.8330346345901489\n",
            "Epoch: 13960 | Training Loss: 1.0930720567703247 | Testing Loss: 0.8325940370559692\n",
            "Epoch: 13970 | Training Loss: 1.0925116539001465 | Testing Loss: 0.8321534395217896\n",
            "Epoch: 13980 | Training Loss: 1.0919512510299683 | Testing Loss: 0.8317128419876099\n",
            "Epoch: 13990 | Training Loss: 1.09139084815979 | Testing Loss: 0.8312722444534302\n",
            "Epoch: 14000 | Training Loss: 1.0908304452896118 | Testing Loss: 0.8308316469192505\n",
            "Epoch: 14010 | Training Loss: 1.0902700424194336 | Testing Loss: 0.8303910493850708\n",
            "Epoch: 14020 | Training Loss: 1.0897096395492554 | Testing Loss: 0.8299504518508911\n",
            "Epoch: 14030 | Training Loss: 1.0891492366790771 | Testing Loss: 0.8295098543167114\n",
            "Epoch: 14040 | Training Loss: 1.088588833808899 | Testing Loss: 0.8290692567825317\n",
            "Epoch: 14050 | Training Loss: 1.0880284309387207 | Testing Loss: 0.828628659248352\n",
            "Epoch: 14060 | Training Loss: 1.0874680280685425 | Testing Loss: 0.8281880617141724\n",
            "Epoch: 14070 | Training Loss: 1.0869076251983643 | Testing Loss: 0.8277474641799927\n",
            "Epoch: 14080 | Training Loss: 1.086347222328186 | Testing Loss: 0.827306866645813\n",
            "Epoch: 14090 | Training Loss: 1.0857868194580078 | Testing Loss: 0.8268662691116333\n",
            "Epoch: 14100 | Training Loss: 1.0852264165878296 | Testing Loss: 0.8264256715774536\n",
            "Epoch: 14110 | Training Loss: 1.0846660137176514 | Testing Loss: 0.8259850740432739\n",
            "Epoch: 14120 | Training Loss: 1.0841056108474731 | Testing Loss: 0.8255444765090942\n",
            "Epoch: 14130 | Training Loss: 1.083545207977295 | Testing Loss: 0.8251038789749146\n",
            "Epoch: 14140 | Training Loss: 1.0829848051071167 | Testing Loss: 0.8246632814407349\n",
            "Epoch: 14150 | Training Loss: 1.0824244022369385 | Testing Loss: 0.8242226839065552\n",
            "Epoch: 14160 | Training Loss: 1.0818639993667603 | Testing Loss: 0.8237820863723755\n",
            "Epoch: 14170 | Training Loss: 1.081303596496582 | Testing Loss: 0.8233414888381958\n",
            "Epoch: 14180 | Training Loss: 1.0807431936264038 | Testing Loss: 0.8229008913040161\n",
            "Epoch: 14190 | Training Loss: 1.0801827907562256 | Testing Loss: 0.8224602937698364\n",
            "Epoch: 14200 | Training Loss: 1.0796223878860474 | Testing Loss: 0.8220196962356567\n",
            "Epoch: 14210 | Training Loss: 1.0790619850158691 | Testing Loss: 0.821579098701477\n",
            "Epoch: 14220 | Training Loss: 1.078501582145691 | Testing Loss: 0.8211385011672974\n",
            "Epoch: 14230 | Training Loss: 1.0779411792755127 | Testing Loss: 0.8206979036331177\n",
            "Epoch: 14240 | Training Loss: 1.0773807764053345 | Testing Loss: 0.820257306098938\n",
            "Epoch: 14250 | Training Loss: 1.0768203735351562 | Testing Loss: 0.8198167085647583\n",
            "Epoch: 14260 | Training Loss: 1.076259970664978 | Testing Loss: 0.8193761110305786\n",
            "Epoch: 14270 | Training Loss: 1.0756995677947998 | Testing Loss: 0.8189355134963989\n",
            "Epoch: 14280 | Training Loss: 1.0751391649246216 | Testing Loss: 0.8184949159622192\n",
            "Epoch: 14290 | Training Loss: 1.0745787620544434 | Testing Loss: 0.8180543184280396\n",
            "Epoch: 14300 | Training Loss: 1.0740183591842651 | Testing Loss: 0.8176137208938599\n",
            "Epoch: 14310 | Training Loss: 1.073457956314087 | Testing Loss: 0.8171731233596802\n",
            "Epoch: 14320 | Training Loss: 1.0728975534439087 | Testing Loss: 0.8167325258255005\n",
            "Epoch: 14330 | Training Loss: 1.0723371505737305 | Testing Loss: 0.8162919282913208\n",
            "Epoch: 14340 | Training Loss: 1.0717767477035522 | Testing Loss: 0.8158513307571411\n",
            "Epoch: 14350 | Training Loss: 1.071216344833374 | Testing Loss: 0.8154107332229614\n",
            "Epoch: 14360 | Training Loss: 1.0706559419631958 | Testing Loss: 0.8149701356887817\n",
            "Epoch: 14370 | Training Loss: 1.0700955390930176 | Testing Loss: 0.814529538154602\n",
            "Epoch: 14380 | Training Loss: 1.0695351362228394 | Testing Loss: 0.8140889406204224\n",
            "Epoch: 14390 | Training Loss: 1.0689747333526611 | Testing Loss: 0.8136483430862427\n",
            "Epoch: 14400 | Training Loss: 1.068414330482483 | Testing Loss: 0.813207745552063\n",
            "Epoch: 14410 | Training Loss: 1.0678539276123047 | Testing Loss: 0.8127671480178833\n",
            "Epoch: 14420 | Training Loss: 1.0672935247421265 | Testing Loss: 0.8123265504837036\n",
            "Epoch: 14430 | Training Loss: 1.0667331218719482 | Testing Loss: 0.8118859529495239\n",
            "Epoch: 14440 | Training Loss: 1.06617271900177 | Testing Loss: 0.8114453554153442\n",
            "Epoch: 14450 | Training Loss: 1.0656123161315918 | Testing Loss: 0.8110047578811646\n",
            "Epoch: 14460 | Training Loss: 1.0650519132614136 | Testing Loss: 0.8105641603469849\n",
            "Epoch: 14470 | Training Loss: 1.0644915103912354 | Testing Loss: 0.8101235628128052\n",
            "Epoch: 14480 | Training Loss: 1.0639311075210571 | Testing Loss: 0.8096829652786255\n",
            "Epoch: 14490 | Training Loss: 1.063370704650879 | Testing Loss: 0.8092423677444458\n",
            "Epoch: 14500 | Training Loss: 1.0628103017807007 | Testing Loss: 0.8088017702102661\n",
            "Epoch: 14510 | Training Loss: 1.0622498989105225 | Testing Loss: 0.8083611726760864\n",
            "Epoch: 14520 | Training Loss: 1.0616894960403442 | Testing Loss: 0.8079205751419067\n",
            "Epoch: 14530 | Training Loss: 1.061129093170166 | Testing Loss: 0.807479977607727\n",
            "Epoch: 14540 | Training Loss: 1.0605686902999878 | Testing Loss: 0.8070393800735474\n",
            "Epoch: 14550 | Training Loss: 1.0600082874298096 | Testing Loss: 0.8065987825393677\n",
            "Epoch: 14560 | Training Loss: 1.0594478845596313 | Testing Loss: 0.806158185005188\n",
            "Epoch: 14570 | Training Loss: 1.0588874816894531 | Testing Loss: 0.8057175874710083\n",
            "Epoch: 14580 | Training Loss: 1.058327078819275 | Testing Loss: 0.8052769899368286\n",
            "Epoch: 14590 | Training Loss: 1.0577666759490967 | Testing Loss: 0.8048363924026489\n",
            "Epoch: 14600 | Training Loss: 1.0572062730789185 | Testing Loss: 0.8043957948684692\n",
            "Epoch: 14610 | Training Loss: 1.0566458702087402 | Testing Loss: 0.8039551973342896\n",
            "Epoch: 14620 | Training Loss: 1.056085467338562 | Testing Loss: 0.8035145998001099\n",
            "Epoch: 14630 | Training Loss: 1.0555250644683838 | Testing Loss: 0.8030740022659302\n",
            "Epoch: 14640 | Training Loss: 1.0549646615982056 | Testing Loss: 0.8026334047317505\n",
            "Epoch: 14650 | Training Loss: 1.0544042587280273 | Testing Loss: 0.8021928071975708\n",
            "Epoch: 14660 | Training Loss: 1.0538438558578491 | Testing Loss: 0.8017522096633911\n",
            "Epoch: 14670 | Training Loss: 1.053283452987671 | Testing Loss: 0.8013116121292114\n",
            "Epoch: 14680 | Training Loss: 1.0527230501174927 | Testing Loss: 0.8008710145950317\n",
            "Epoch: 14690 | Training Loss: 1.0521626472473145 | Testing Loss: 0.800430417060852\n",
            "Epoch: 14700 | Training Loss: 1.0516022443771362 | Testing Loss: 0.7999898195266724\n",
            "Epoch: 14710 | Training Loss: 1.051041841506958 | Testing Loss: 0.7995492219924927\n",
            "Epoch: 14720 | Training Loss: 1.0504814386367798 | Testing Loss: 0.799108624458313\n",
            "Epoch: 14730 | Training Loss: 1.0499210357666016 | Testing Loss: 0.7986680269241333\n",
            "Epoch: 14740 | Training Loss: 1.0493606328964233 | Testing Loss: 0.7982274293899536\n",
            "Epoch: 14750 | Training Loss: 1.0488002300262451 | Testing Loss: 0.7977868318557739\n",
            "Epoch: 14760 | Training Loss: 1.048239827156067 | Testing Loss: 0.7973462343215942\n",
            "Epoch: 14770 | Training Loss: 1.0476794242858887 | Testing Loss: 0.7969056367874146\n",
            "Epoch: 14780 | Training Loss: 1.0471190214157104 | Testing Loss: 0.7964650392532349\n",
            "Epoch: 14790 | Training Loss: 1.0465586185455322 | Testing Loss: 0.7960244417190552\n",
            "Epoch: 14800 | Training Loss: 1.045998215675354 | Testing Loss: 0.7955838441848755\n",
            "Epoch: 14810 | Training Loss: 1.0454378128051758 | Testing Loss: 0.7951432466506958\n",
            "Epoch: 14820 | Training Loss: 1.0448774099349976 | Testing Loss: 0.7947026491165161\n",
            "Epoch: 14830 | Training Loss: 1.0443170070648193 | Testing Loss: 0.7942620515823364\n",
            "Epoch: 14840 | Training Loss: 1.0437566041946411 | Testing Loss: 0.7938214540481567\n",
            "Epoch: 14850 | Training Loss: 1.043196201324463 | Testing Loss: 0.793380856513977\n",
            "Epoch: 14860 | Training Loss: 1.0426357984542847 | Testing Loss: 0.7929402589797974\n",
            "Epoch: 14870 | Training Loss: 1.0420753955841064 | Testing Loss: 0.7924996614456177\n",
            "Epoch: 14880 | Training Loss: 1.0415149927139282 | Testing Loss: 0.792059063911438\n",
            "Epoch: 14890 | Training Loss: 1.04095458984375 | Testing Loss: 0.7916184663772583\n",
            "Epoch: 14900 | Training Loss: 1.0403941869735718 | Testing Loss: 0.7911778688430786\n",
            "Epoch: 14910 | Training Loss: 1.0398337841033936 | Testing Loss: 0.7907372713088989\n",
            "Epoch: 14920 | Training Loss: 1.0392733812332153 | Testing Loss: 0.7902966737747192\n",
            "Epoch: 14930 | Training Loss: 1.038712978363037 | Testing Loss: 0.7898560762405396\n",
            "Epoch: 14940 | Training Loss: 1.0381525754928589 | Testing Loss: 0.7894154787063599\n",
            "Epoch: 14950 | Training Loss: 1.0375921726226807 | Testing Loss: 0.7889748811721802\n",
            "Epoch: 14960 | Training Loss: 1.0370317697525024 | Testing Loss: 0.7885342836380005\n",
            "Epoch: 14970 | Training Loss: 1.0364713668823242 | Testing Loss: 0.7880936861038208\n",
            "Epoch: 14980 | Training Loss: 1.035910964012146 | Testing Loss: 0.7876530885696411\n",
            "Epoch: 14990 | Training Loss: 1.0353505611419678 | Testing Loss: 0.7872124910354614\n",
            "Epoch: 15000 | Training Loss: 1.0347901582717896 | Testing Loss: 0.7867718935012817\n",
            "Epoch: 15010 | Training Loss: 1.0342297554016113 | Testing Loss: 0.786331295967102\n",
            "Epoch: 15020 | Training Loss: 1.033669352531433 | Testing Loss: 0.7858906984329224\n",
            "Epoch: 15030 | Training Loss: 1.0331089496612549 | Testing Loss: 0.7854501008987427\n",
            "Epoch: 15040 | Training Loss: 1.0325485467910767 | Testing Loss: 0.785009503364563\n",
            "Epoch: 15050 | Training Loss: 1.0319881439208984 | Testing Loss: 0.7845689058303833\n",
            "Epoch: 15060 | Training Loss: 1.0314277410507202 | Testing Loss: 0.7841283082962036\n",
            "Epoch: 15070 | Training Loss: 1.030867338180542 | Testing Loss: 0.7836877107620239\n",
            "Epoch: 15080 | Training Loss: 1.0303069353103638 | Testing Loss: 0.7832471132278442\n",
            "Epoch: 15090 | Training Loss: 1.0297465324401855 | Testing Loss: 0.7828065156936646\n",
            "Epoch: 15100 | Training Loss: 1.0291861295700073 | Testing Loss: 0.7823659181594849\n",
            "Epoch: 15110 | Training Loss: 1.028625726699829 | Testing Loss: 0.7819253206253052\n",
            "Epoch: 15120 | Training Loss: 1.0280653238296509 | Testing Loss: 0.7814847230911255\n",
            "Epoch: 15130 | Training Loss: 1.0275049209594727 | Testing Loss: 0.7810441255569458\n",
            "Epoch: 15140 | Training Loss: 1.0269445180892944 | Testing Loss: 0.7806035280227661\n",
            "Epoch: 15150 | Training Loss: 1.0263841152191162 | Testing Loss: 0.7801629304885864\n",
            "Epoch: 15160 | Training Loss: 1.025823712348938 | Testing Loss: 0.7797223329544067\n",
            "Epoch: 15170 | Training Loss: 1.0252633094787598 | Testing Loss: 0.779281735420227\n",
            "Epoch: 15180 | Training Loss: 1.0247029066085815 | Testing Loss: 0.7788411378860474\n",
            "Epoch: 15190 | Training Loss: 1.0241425037384033 | Testing Loss: 0.7784005403518677\n",
            "Epoch: 15200 | Training Loss: 1.023582100868225 | Testing Loss: 0.777959942817688\n",
            "Epoch: 15210 | Training Loss: 1.0230216979980469 | Testing Loss: 0.7775193452835083\n",
            "Epoch: 15220 | Training Loss: 1.0224612951278687 | Testing Loss: 0.7770787477493286\n",
            "Epoch: 15230 | Training Loss: 1.0219008922576904 | Testing Loss: 0.7766381502151489\n",
            "Epoch: 15240 | Training Loss: 1.0213404893875122 | Testing Loss: 0.7761975526809692\n",
            "Epoch: 15250 | Training Loss: 1.020780086517334 | Testing Loss: 0.7757569551467896\n",
            "Epoch: 15260 | Training Loss: 1.0202196836471558 | Testing Loss: 0.7753163576126099\n",
            "Epoch: 15270 | Training Loss: 1.0196592807769775 | Testing Loss: 0.7748757600784302\n",
            "Epoch: 15280 | Training Loss: 1.0190988779067993 | Testing Loss: 0.7744351625442505\n",
            "Epoch: 15290 | Training Loss: 1.018538475036621 | Testing Loss: 0.7739945650100708\n",
            "Epoch: 15300 | Training Loss: 1.0179780721664429 | Testing Loss: 0.7735539674758911\n",
            "Epoch: 15310 | Training Loss: 1.0174176692962646 | Testing Loss: 0.7731133699417114\n",
            "Epoch: 15320 | Training Loss: 1.0168572664260864 | Testing Loss: 0.7726727724075317\n",
            "Epoch: 15330 | Training Loss: 1.0162968635559082 | Testing Loss: 0.772232174873352\n",
            "Epoch: 15340 | Training Loss: 1.01573646068573 | Testing Loss: 0.7717915773391724\n",
            "Epoch: 15350 | Training Loss: 1.0151760578155518 | Testing Loss: 0.7713509798049927\n",
            "Epoch: 15360 | Training Loss: 1.0146156549453735 | Testing Loss: 0.770910382270813\n",
            "Epoch: 15370 | Training Loss: 1.0140552520751953 | Testing Loss: 0.7704697847366333\n",
            "Epoch: 15380 | Training Loss: 1.013494849205017 | Testing Loss: 0.7700291872024536\n",
            "Epoch: 15390 | Training Loss: 1.0129344463348389 | Testing Loss: 0.7695885896682739\n",
            "Epoch: 15400 | Training Loss: 1.0123740434646606 | Testing Loss: 0.7691479921340942\n",
            "Epoch: 15410 | Training Loss: 1.0118136405944824 | Testing Loss: 0.7687073945999146\n",
            "Epoch: 15420 | Training Loss: 1.0112532377243042 | Testing Loss: 0.7682667970657349\n",
            "Epoch: 15430 | Training Loss: 1.010692834854126 | Testing Loss: 0.7678261995315552\n",
            "Epoch: 15440 | Training Loss: 1.0101324319839478 | Testing Loss: 0.7673856019973755\n",
            "Epoch: 15450 | Training Loss: 1.0095720291137695 | Testing Loss: 0.7669450044631958\n",
            "Epoch: 15460 | Training Loss: 1.0090116262435913 | Testing Loss: 0.7665044069290161\n",
            "Epoch: 15470 | Training Loss: 1.008451223373413 | Testing Loss: 0.7660638093948364\n",
            "Epoch: 15480 | Training Loss: 1.0078908205032349 | Testing Loss: 0.7656232118606567\n",
            "Epoch: 15490 | Training Loss: 1.0073304176330566 | Testing Loss: 0.765182614326477\n",
            "Epoch: 15500 | Training Loss: 1.0067700147628784 | Testing Loss: 0.7647420167922974\n",
            "Epoch: 15510 | Training Loss: 1.0062096118927002 | Testing Loss: 0.7643014192581177\n",
            "Epoch: 15520 | Training Loss: 1.005649209022522 | Testing Loss: 0.763860821723938\n",
            "Epoch: 15530 | Training Loss: 1.0050888061523438 | Testing Loss: 0.7634202241897583\n",
            "Epoch: 15540 | Training Loss: 1.0045284032821655 | Testing Loss: 0.7629796266555786\n",
            "Epoch: 15550 | Training Loss: 1.0039680004119873 | Testing Loss: 0.7625390291213989\n",
            "Epoch: 15560 | Training Loss: 1.003407597541809 | Testing Loss: 0.7620984315872192\n",
            "Epoch: 15570 | Training Loss: 1.0028471946716309 | Testing Loss: 0.7616578340530396\n",
            "Epoch: 15580 | Training Loss: 1.0022867918014526 | Testing Loss: 0.7612172365188599\n",
            "Epoch: 15590 | Training Loss: 1.0017263889312744 | Testing Loss: 0.7607766389846802\n",
            "Epoch: 15600 | Training Loss: 1.0011659860610962 | Testing Loss: 0.7603360414505005\n",
            "Epoch: 15610 | Training Loss: 1.000605583190918 | Testing Loss: 0.7598954439163208\n",
            "Epoch: 15620 | Training Loss: 1.0000451803207397 | Testing Loss: 0.7594548463821411\n",
            "Epoch: 15630 | Training Loss: 0.9994847178459167 | Testing Loss: 0.7590142488479614\n",
            "Epoch: 15640 | Training Loss: 0.9989243745803833 | Testing Loss: 0.7585736513137817\n",
            "Epoch: 15650 | Training Loss: 0.9983639717102051 | Testing Loss: 0.758133053779602\n",
            "Epoch: 15660 | Training Loss: 0.9978035092353821 | Testing Loss: 0.7576924562454224\n",
            "Epoch: 15670 | Training Loss: 0.9972431063652039 | Testing Loss: 0.7572518587112427\n",
            "Epoch: 15680 | Training Loss: 0.9966827630996704 | Testing Loss: 0.756811261177063\n",
            "Epoch: 15690 | Training Loss: 0.9961223602294922 | Testing Loss: 0.7563706636428833\n",
            "Epoch: 15700 | Training Loss: 0.9955618977546692 | Testing Loss: 0.7559300661087036\n",
            "Epoch: 15710 | Training Loss: 0.995001494884491 | Testing Loss: 0.7554894685745239\n",
            "Epoch: 15720 | Training Loss: 0.9944411516189575 | Testing Loss: 0.7550488710403442\n",
            "Epoch: 15730 | Training Loss: 0.9938807487487793 | Testing Loss: 0.7546082735061646\n",
            "Epoch: 15740 | Training Loss: 0.9933202862739563 | Testing Loss: 0.7541676759719849\n",
            "Epoch: 15750 | Training Loss: 0.9927598834037781 | Testing Loss: 0.7537270784378052\n",
            "Epoch: 15760 | Training Loss: 0.9921995401382446 | Testing Loss: 0.7532864809036255\n",
            "Epoch: 15770 | Training Loss: 0.9916391372680664 | Testing Loss: 0.7528458833694458\n",
            "Epoch: 15780 | Training Loss: 0.9910786747932434 | Testing Loss: 0.7524052858352661\n",
            "Epoch: 15790 | Training Loss: 0.9905182719230652 | Testing Loss: 0.7519646883010864\n",
            "Epoch: 15800 | Training Loss: 0.9899579286575317 | Testing Loss: 0.7515240907669067\n",
            "Epoch: 15810 | Training Loss: 0.9893975257873535 | Testing Loss: 0.751083493232727\n",
            "Epoch: 15820 | Training Loss: 0.9888370633125305 | Testing Loss: 0.7506428956985474\n",
            "Epoch: 15830 | Training Loss: 0.9882766604423523 | Testing Loss: 0.7502022981643677\n",
            "Epoch: 15840 | Training Loss: 0.9877163171768188 | Testing Loss: 0.749761700630188\n",
            "Epoch: 15850 | Training Loss: 0.9871559143066406 | Testing Loss: 0.7493211030960083\n",
            "Epoch: 15860 | Training Loss: 0.9865954518318176 | Testing Loss: 0.7488805055618286\n",
            "Epoch: 15870 | Training Loss: 0.9860350489616394 | Testing Loss: 0.7484399080276489\n",
            "Epoch: 15880 | Training Loss: 0.985474705696106 | Testing Loss: 0.7479993104934692\n",
            "Epoch: 15890 | Training Loss: 0.9849143028259277 | Testing Loss: 0.7475587129592896\n",
            "Epoch: 15900 | Training Loss: 0.9843538403511047 | Testing Loss: 0.7471181154251099\n",
            "Epoch: 15910 | Training Loss: 0.9837934374809265 | Testing Loss: 0.7466775178909302\n",
            "Epoch: 15920 | Training Loss: 0.9832330942153931 | Testing Loss: 0.7462369203567505\n",
            "Epoch: 15930 | Training Loss: 0.9826726913452148 | Testing Loss: 0.7457963228225708\n",
            "Epoch: 15940 | Training Loss: 0.9821122288703918 | Testing Loss: 0.7453557252883911\n",
            "Epoch: 15950 | Training Loss: 0.9815518260002136 | Testing Loss: 0.7449151277542114\n",
            "Epoch: 15960 | Training Loss: 0.9809914827346802 | Testing Loss: 0.7444745302200317\n",
            "Epoch: 15970 | Training Loss: 0.980431079864502 | Testing Loss: 0.744033932685852\n",
            "Epoch: 15980 | Training Loss: 0.979870617389679 | Testing Loss: 0.7435933351516724\n",
            "Epoch: 15990 | Training Loss: 0.9793102145195007 | Testing Loss: 0.7431527376174927\n",
            "Epoch: 16000 | Training Loss: 0.9787498712539673 | Testing Loss: 0.742712140083313\n",
            "Epoch: 16010 | Training Loss: 0.9781894683837891 | Testing Loss: 0.7422715425491333\n",
            "Epoch: 16020 | Training Loss: 0.9776290059089661 | Testing Loss: 0.7418309450149536\n",
            "Epoch: 16030 | Training Loss: 0.9770686030387878 | Testing Loss: 0.7413903474807739\n",
            "Epoch: 16040 | Training Loss: 0.9765082597732544 | Testing Loss: 0.7409497499465942\n",
            "Epoch: 16050 | Training Loss: 0.9759478569030762 | Testing Loss: 0.7405091524124146\n",
            "Epoch: 16060 | Training Loss: 0.9753873944282532 | Testing Loss: 0.7400685548782349\n",
            "Epoch: 16070 | Training Loss: 0.974826991558075 | Testing Loss: 0.7396279573440552\n",
            "Epoch: 16080 | Training Loss: 0.9742666482925415 | Testing Loss: 0.7391873598098755\n",
            "Epoch: 16090 | Training Loss: 0.9737062454223633 | Testing Loss: 0.7387467622756958\n",
            "Epoch: 16100 | Training Loss: 0.9731457829475403 | Testing Loss: 0.7383061647415161\n",
            "Epoch: 16110 | Training Loss: 0.9725853800773621 | Testing Loss: 0.7378655672073364\n",
            "Epoch: 16120 | Training Loss: 0.9720250368118286 | Testing Loss: 0.7374249696731567\n",
            "Epoch: 16130 | Training Loss: 0.9714646339416504 | Testing Loss: 0.736984372138977\n",
            "Epoch: 16140 | Training Loss: 0.9709041714668274 | Testing Loss: 0.7365437746047974\n",
            "Epoch: 16150 | Training Loss: 0.9703437685966492 | Testing Loss: 0.7361031770706177\n",
            "Epoch: 16160 | Training Loss: 0.9697834253311157 | Testing Loss: 0.735662579536438\n",
            "Epoch: 16170 | Training Loss: 0.9692230224609375 | Testing Loss: 0.7352219820022583\n",
            "Epoch: 16180 | Training Loss: 0.9686625599861145 | Testing Loss: 0.7347813844680786\n",
            "Epoch: 16190 | Training Loss: 0.9681021571159363 | Testing Loss: 0.7343407869338989\n",
            "Epoch: 16200 | Training Loss: 0.9675418138504028 | Testing Loss: 0.7339001893997192\n",
            "Epoch: 16210 | Training Loss: 0.9669814109802246 | Testing Loss: 0.7334595918655396\n",
            "Epoch: 16220 | Training Loss: 0.9664209485054016 | Testing Loss: 0.7330189943313599\n",
            "Epoch: 16230 | Training Loss: 0.9658605456352234 | Testing Loss: 0.7325783967971802\n",
            "Epoch: 16240 | Training Loss: 0.9653002023696899 | Testing Loss: 0.7321377992630005\n",
            "Epoch: 16250 | Training Loss: 0.9647397994995117 | Testing Loss: 0.7316972017288208\n",
            "Epoch: 16260 | Training Loss: 0.9641793370246887 | Testing Loss: 0.7312566041946411\n",
            "Epoch: 16270 | Training Loss: 0.9636189341545105 | Testing Loss: 0.7308160066604614\n",
            "Epoch: 16280 | Training Loss: 0.963058590888977 | Testing Loss: 0.7303754091262817\n",
            "Epoch: 16290 | Training Loss: 0.9624981880187988 | Testing Loss: 0.729934811592102\n",
            "Epoch: 16300 | Training Loss: 0.9619377255439758 | Testing Loss: 0.7294942140579224\n",
            "Epoch: 16310 | Training Loss: 0.9613773226737976 | Testing Loss: 0.7290536165237427\n",
            "Epoch: 16320 | Training Loss: 0.9608169794082642 | Testing Loss: 0.728613018989563\n",
            "Epoch: 16330 | Training Loss: 0.9602565765380859 | Testing Loss: 0.7281724214553833\n",
            "Epoch: 16340 | Training Loss: 0.9596961140632629 | Testing Loss: 0.7277318239212036\n",
            "Epoch: 16350 | Training Loss: 0.9591357111930847 | Testing Loss: 0.7272912263870239\n",
            "Epoch: 16360 | Training Loss: 0.9585753679275513 | Testing Loss: 0.7268506288528442\n",
            "Epoch: 16370 | Training Loss: 0.958014965057373 | Testing Loss: 0.7264100313186646\n",
            "Epoch: 16380 | Training Loss: 0.95745450258255 | Testing Loss: 0.7259694337844849\n",
            "Epoch: 16390 | Training Loss: 0.9568940997123718 | Testing Loss: 0.7255288362503052\n",
            "Epoch: 16400 | Training Loss: 0.9563337564468384 | Testing Loss: 0.7250882387161255\n",
            "Epoch: 16410 | Training Loss: 0.9557733535766602 | Testing Loss: 0.7246476411819458\n",
            "Epoch: 16420 | Training Loss: 0.9552128911018372 | Testing Loss: 0.7242070436477661\n",
            "Epoch: 16430 | Training Loss: 0.9546524882316589 | Testing Loss: 0.7237664461135864\n",
            "Epoch: 16440 | Training Loss: 0.9540921449661255 | Testing Loss: 0.7233258485794067\n",
            "Epoch: 16450 | Training Loss: 0.9535317420959473 | Testing Loss: 0.722885251045227\n",
            "Epoch: 16460 | Training Loss: 0.9529712796211243 | Testing Loss: 0.7224446535110474\n",
            "Epoch: 16470 | Training Loss: 0.952410876750946 | Testing Loss: 0.7220040559768677\n",
            "Epoch: 16480 | Training Loss: 0.9518505334854126 | Testing Loss: 0.721563458442688\n",
            "Epoch: 16490 | Training Loss: 0.9512901306152344 | Testing Loss: 0.7211228609085083\n",
            "Epoch: 16500 | Training Loss: 0.9507296681404114 | Testing Loss: 0.7206822633743286\n",
            "Epoch: 16510 | Training Loss: 0.9501692652702332 | Testing Loss: 0.7202416658401489\n",
            "Epoch: 16520 | Training Loss: 0.9496089220046997 | Testing Loss: 0.7198010683059692\n",
            "Epoch: 16530 | Training Loss: 0.9490485191345215 | Testing Loss: 0.7193604707717896\n",
            "Epoch: 16540 | Training Loss: 0.9484880566596985 | Testing Loss: 0.7189198732376099\n",
            "Epoch: 16550 | Training Loss: 0.9479276537895203 | Testing Loss: 0.7184792757034302\n",
            "Epoch: 16560 | Training Loss: 0.9473673105239868 | Testing Loss: 0.7180386781692505\n",
            "Epoch: 16570 | Training Loss: 0.9468069076538086 | Testing Loss: 0.7175980806350708\n",
            "Epoch: 16580 | Training Loss: 0.9462464451789856 | Testing Loss: 0.7171574831008911\n",
            "Epoch: 16590 | Training Loss: 0.9456860423088074 | Testing Loss: 0.7167168855667114\n",
            "Epoch: 16600 | Training Loss: 0.9451256990432739 | Testing Loss: 0.7162762880325317\n",
            "Epoch: 16610 | Training Loss: 0.9445652961730957 | Testing Loss: 0.715835690498352\n",
            "Epoch: 16620 | Training Loss: 0.9440048336982727 | Testing Loss: 0.7153950929641724\n",
            "Epoch: 16630 | Training Loss: 0.9434444308280945 | Testing Loss: 0.7149544954299927\n",
            "Epoch: 16640 | Training Loss: 0.942884087562561 | Testing Loss: 0.714513897895813\n",
            "Epoch: 16650 | Training Loss: 0.9423236846923828 | Testing Loss: 0.7140733003616333\n",
            "Epoch: 16660 | Training Loss: 0.9417632222175598 | Testing Loss: 0.7136327028274536\n",
            "Epoch: 16670 | Training Loss: 0.9412028193473816 | Testing Loss: 0.7131921052932739\n",
            "Epoch: 16680 | Training Loss: 0.9406424760818481 | Testing Loss: 0.7127515077590942\n",
            "Epoch: 16690 | Training Loss: 0.9400820732116699 | Testing Loss: 0.7123109102249146\n",
            "Epoch: 16700 | Training Loss: 0.9395216107368469 | Testing Loss: 0.7118703126907349\n",
            "Epoch: 16710 | Training Loss: 0.9389612078666687 | Testing Loss: 0.7114297151565552\n",
            "Epoch: 16720 | Training Loss: 0.9384008646011353 | Testing Loss: 0.7109891176223755\n",
            "Epoch: 16730 | Training Loss: 0.937840461730957 | Testing Loss: 0.7105485200881958\n",
            "Epoch: 16740 | Training Loss: 0.937279999256134 | Testing Loss: 0.7101079225540161\n",
            "Epoch: 16750 | Training Loss: 0.9367195963859558 | Testing Loss: 0.7096673250198364\n",
            "Epoch: 16760 | Training Loss: 0.9361592531204224 | Testing Loss: 0.7092267274856567\n",
            "Epoch: 16770 | Training Loss: 0.9355988502502441 | Testing Loss: 0.708786129951477\n",
            "Epoch: 16780 | Training Loss: 0.9350383877754211 | Testing Loss: 0.7083455324172974\n",
            "Epoch: 16790 | Training Loss: 0.9344779849052429 | Testing Loss: 0.7079049348831177\n",
            "Epoch: 16800 | Training Loss: 0.9339176416397095 | Testing Loss: 0.707464337348938\n",
            "Epoch: 16810 | Training Loss: 0.9333572387695312 | Testing Loss: 0.7070237398147583\n",
            "Epoch: 16820 | Training Loss: 0.9327967762947083 | Testing Loss: 0.7065831422805786\n",
            "Epoch: 16830 | Training Loss: 0.93223637342453 | Testing Loss: 0.7061425447463989\n",
            "Epoch: 16840 | Training Loss: 0.9316760301589966 | Testing Loss: 0.7057019472122192\n",
            "Epoch: 16850 | Training Loss: 0.9311156272888184 | Testing Loss: 0.7052613496780396\n",
            "Epoch: 16860 | Training Loss: 0.9305551648139954 | Testing Loss: 0.7048207521438599\n",
            "Epoch: 16870 | Training Loss: 0.9299947619438171 | Testing Loss: 0.7043801546096802\n",
            "Epoch: 16880 | Training Loss: 0.9294344186782837 | Testing Loss: 0.7039395570755005\n",
            "Epoch: 16890 | Training Loss: 0.9288740158081055 | Testing Loss: 0.7034989595413208\n",
            "Epoch: 16900 | Training Loss: 0.9283135533332825 | Testing Loss: 0.7030583620071411\n",
            "Epoch: 16910 | Training Loss: 0.9277531504631042 | Testing Loss: 0.7026177644729614\n",
            "Epoch: 16920 | Training Loss: 0.9271928071975708 | Testing Loss: 0.7021771669387817\n",
            "Epoch: 16930 | Training Loss: 0.9266324043273926 | Testing Loss: 0.701736569404602\n",
            "Epoch: 16940 | Training Loss: 0.9260719418525696 | Testing Loss: 0.7012959718704224\n",
            "Epoch: 16950 | Training Loss: 0.9255115389823914 | Testing Loss: 0.7008553743362427\n",
            "Epoch: 16960 | Training Loss: 0.9249511957168579 | Testing Loss: 0.700414776802063\n",
            "Epoch: 16970 | Training Loss: 0.9243907928466797 | Testing Loss: 0.6999741792678833\n",
            "Epoch: 16980 | Training Loss: 0.9238303303718567 | Testing Loss: 0.6995335817337036\n",
            "Epoch: 16990 | Training Loss: 0.9232699275016785 | Testing Loss: 0.6990929841995239\n",
            "Epoch: 17000 | Training Loss: 0.922709584236145 | Testing Loss: 0.6986523866653442\n",
            "Epoch: 17010 | Training Loss: 0.9221491813659668 | Testing Loss: 0.6982117891311646\n",
            "Epoch: 17020 | Training Loss: 0.9215887188911438 | Testing Loss: 0.6977711915969849\n",
            "Epoch: 17030 | Training Loss: 0.9210283160209656 | Testing Loss: 0.6973305940628052\n",
            "Epoch: 17040 | Training Loss: 0.9204679727554321 | Testing Loss: 0.6968899965286255\n",
            "Epoch: 17050 | Training Loss: 0.9199075698852539 | Testing Loss: 0.6964493989944458\n",
            "Epoch: 17060 | Training Loss: 0.9193471074104309 | Testing Loss: 0.6960088014602661\n",
            "Epoch: 17070 | Training Loss: 0.9187867045402527 | Testing Loss: 0.6955682039260864\n",
            "Epoch: 17080 | Training Loss: 0.9182263612747192 | Testing Loss: 0.6951276063919067\n",
            "Epoch: 17090 | Training Loss: 0.917665958404541 | Testing Loss: 0.694687008857727\n",
            "Epoch: 17100 | Training Loss: 0.917105495929718 | Testing Loss: 0.6942464113235474\n",
            "Epoch: 17110 | Training Loss: 0.9165450930595398 | Testing Loss: 0.6938058137893677\n",
            "Epoch: 17120 | Training Loss: 0.9159847497940063 | Testing Loss: 0.693365216255188\n",
            "Epoch: 17130 | Training Loss: 0.9154243469238281 | Testing Loss: 0.6929246187210083\n",
            "Epoch: 17140 | Training Loss: 0.9148638844490051 | Testing Loss: 0.6924840211868286\n",
            "Epoch: 17150 | Training Loss: 0.9143034815788269 | Testing Loss: 0.6920434236526489\n",
            "Epoch: 17160 | Training Loss: 0.9137431383132935 | Testing Loss: 0.6916028261184692\n",
            "Epoch: 17170 | Training Loss: 0.9131827354431152 | Testing Loss: 0.6911622285842896\n",
            "Epoch: 17180 | Training Loss: 0.9126222729682922 | Testing Loss: 0.6907216310501099\n",
            "Epoch: 17190 | Training Loss: 0.912061870098114 | Testing Loss: 0.6902810335159302\n",
            "Epoch: 17200 | Training Loss: 0.9115015268325806 | Testing Loss: 0.6898404359817505\n",
            "Epoch: 17210 | Training Loss: 0.9109411239624023 | Testing Loss: 0.6893998384475708\n",
            "Epoch: 17220 | Training Loss: 0.9103806614875793 | Testing Loss: 0.6889592409133911\n",
            "Epoch: 17230 | Training Loss: 0.9098202586174011 | Testing Loss: 0.6885186433792114\n",
            "Epoch: 17240 | Training Loss: 0.9092599153518677 | Testing Loss: 0.6880780458450317\n",
            "Epoch: 17250 | Training Loss: 0.9086994528770447 | Testing Loss: 0.687637448310852\n",
            "Epoch: 17260 | Training Loss: 0.9081390500068665 | Testing Loss: 0.6871968507766724\n",
            "Epoch: 17270 | Training Loss: 0.9075786471366882 | Testing Loss: 0.6867562532424927\n",
            "Epoch: 17280 | Training Loss: 0.9070183038711548 | Testing Loss: 0.686315655708313\n",
            "Epoch: 17290 | Training Loss: 0.9064578413963318 | Testing Loss: 0.6858750581741333\n",
            "Epoch: 17300 | Training Loss: 0.9058974385261536 | Testing Loss: 0.6854344606399536\n",
            "Epoch: 17310 | Training Loss: 0.9053370356559753 | Testing Loss: 0.6849938631057739\n",
            "Epoch: 17320 | Training Loss: 0.9047766923904419 | Testing Loss: 0.6845532655715942\n",
            "Epoch: 17330 | Training Loss: 0.9042162299156189 | Testing Loss: 0.6841126680374146\n",
            "Epoch: 17340 | Training Loss: 0.9036558270454407 | Testing Loss: 0.6836720705032349\n",
            "Epoch: 17350 | Training Loss: 0.9030954241752625 | Testing Loss: 0.6832314729690552\n",
            "Epoch: 17360 | Training Loss: 0.902535080909729 | Testing Loss: 0.6827908754348755\n",
            "Epoch: 17370 | Training Loss: 0.901974618434906 | Testing Loss: 0.6823502779006958\n",
            "Epoch: 17380 | Training Loss: 0.9014142155647278 | Testing Loss: 0.6819096803665161\n",
            "Epoch: 17390 | Training Loss: 0.9008538126945496 | Testing Loss: 0.6814690828323364\n",
            "Epoch: 17400 | Training Loss: 0.9002934694290161 | Testing Loss: 0.6810284852981567\n",
            "Epoch: 17410 | Training Loss: 0.8997330069541931 | Testing Loss: 0.680587887763977\n",
            "Epoch: 17420 | Training Loss: 0.8991726040840149 | Testing Loss: 0.6801472902297974\n",
            "Epoch: 17430 | Training Loss: 0.8986122012138367 | Testing Loss: 0.6797066926956177\n",
            "Epoch: 17440 | Training Loss: 0.8980518579483032 | Testing Loss: 0.679266095161438\n",
            "Epoch: 17450 | Training Loss: 0.8974913954734802 | Testing Loss: 0.6788254976272583\n",
            "Epoch: 17460 | Training Loss: 0.896930992603302 | Testing Loss: 0.6783849000930786\n",
            "Epoch: 17470 | Training Loss: 0.8963705897331238 | Testing Loss: 0.6779443025588989\n",
            "Epoch: 17480 | Training Loss: 0.8958102464675903 | Testing Loss: 0.6775037050247192\n",
            "Epoch: 17490 | Training Loss: 0.8952497839927673 | Testing Loss: 0.6770631074905396\n",
            "Epoch: 17500 | Training Loss: 0.8946893811225891 | Testing Loss: 0.6766225099563599\n",
            "Epoch: 17510 | Training Loss: 0.8941289782524109 | Testing Loss: 0.6761819124221802\n",
            "Epoch: 17520 | Training Loss: 0.8935686349868774 | Testing Loss: 0.6757413148880005\n",
            "Epoch: 17530 | Training Loss: 0.8930081725120544 | Testing Loss: 0.6753007173538208\n",
            "Epoch: 17540 | Training Loss: 0.8924477696418762 | Testing Loss: 0.6748601198196411\n",
            "Epoch: 17550 | Training Loss: 0.891887366771698 | Testing Loss: 0.6744195222854614\n",
            "Epoch: 17560 | Training Loss: 0.8913270235061646 | Testing Loss: 0.6739789247512817\n",
            "Epoch: 17570 | Training Loss: 0.8907665610313416 | Testing Loss: 0.673538327217102\n",
            "Epoch: 17580 | Training Loss: 0.8902061581611633 | Testing Loss: 0.6730977296829224\n",
            "Epoch: 17590 | Training Loss: 0.8896457552909851 | Testing Loss: 0.6726571321487427\n",
            "Epoch: 17600 | Training Loss: 0.8890854120254517 | Testing Loss: 0.672216534614563\n",
            "Epoch: 17610 | Training Loss: 0.8885249495506287 | Testing Loss: 0.6717759370803833\n",
            "Epoch: 17620 | Training Loss: 0.8879645466804504 | Testing Loss: 0.6713353395462036\n",
            "Epoch: 17630 | Training Loss: 0.8874041438102722 | Testing Loss: 0.6708947420120239\n",
            "Epoch: 17640 | Training Loss: 0.8868438005447388 | Testing Loss: 0.6704541444778442\n",
            "Epoch: 17650 | Training Loss: 0.8862833380699158 | Testing Loss: 0.6700135469436646\n",
            "Epoch: 17660 | Training Loss: 0.8857229351997375 | Testing Loss: 0.6695729494094849\n",
            "Epoch: 17670 | Training Loss: 0.8851625323295593 | Testing Loss: 0.6691323518753052\n",
            "Epoch: 17680 | Training Loss: 0.8846021890640259 | Testing Loss: 0.6686917543411255\n",
            "Epoch: 17690 | Training Loss: 0.8840417265892029 | Testing Loss: 0.6682511568069458\n",
            "Epoch: 17700 | Training Loss: 0.8834813237190247 | Testing Loss: 0.6678105592727661\n",
            "Epoch: 17710 | Training Loss: 0.8829209208488464 | Testing Loss: 0.6673699617385864\n",
            "Epoch: 17720 | Training Loss: 0.882360577583313 | Testing Loss: 0.6669293642044067\n",
            "Epoch: 17730 | Training Loss: 0.88180011510849 | Testing Loss: 0.666488766670227\n",
            "Epoch: 17740 | Training Loss: 0.8812397122383118 | Testing Loss: 0.6660481691360474\n",
            "Epoch: 17750 | Training Loss: 0.8806793093681335 | Testing Loss: 0.6656075716018677\n",
            "Epoch: 17760 | Training Loss: 0.8801189661026001 | Testing Loss: 0.665166974067688\n",
            "Epoch: 17770 | Training Loss: 0.8795585036277771 | Testing Loss: 0.6647263765335083\n",
            "Epoch: 17780 | Training Loss: 0.8789981007575989 | Testing Loss: 0.6642857789993286\n",
            "Epoch: 17790 | Training Loss: 0.8784376978874207 | Testing Loss: 0.6638451814651489\n",
            "Epoch: 17800 | Training Loss: 0.8778773546218872 | Testing Loss: 0.6634045839309692\n",
            "Epoch: 17810 | Training Loss: 0.8773168921470642 | Testing Loss: 0.6629639863967896\n",
            "Epoch: 17820 | Training Loss: 0.876756489276886 | Testing Loss: 0.6625233888626099\n",
            "Epoch: 17830 | Training Loss: 0.8761960864067078 | Testing Loss: 0.6620827913284302\n",
            "Epoch: 17840 | Training Loss: 0.8756357431411743 | Testing Loss: 0.6616421937942505\n",
            "Epoch: 17850 | Training Loss: 0.8750752806663513 | Testing Loss: 0.6612015962600708\n",
            "Epoch: 17860 | Training Loss: 0.8745148777961731 | Testing Loss: 0.6607609987258911\n",
            "Epoch: 17870 | Training Loss: 0.8739544749259949 | Testing Loss: 0.6603204011917114\n",
            "Epoch: 17880 | Training Loss: 0.8733941316604614 | Testing Loss: 0.6598798036575317\n",
            "Epoch: 17890 | Training Loss: 0.8728336691856384 | Testing Loss: 0.659439206123352\n",
            "Epoch: 17900 | Training Loss: 0.8722732663154602 | Testing Loss: 0.6589986085891724\n",
            "Epoch: 17910 | Training Loss: 0.871712863445282 | Testing Loss: 0.6585580110549927\n",
            "Epoch: 17920 | Training Loss: 0.8711525201797485 | Testing Loss: 0.658117413520813\n",
            "Epoch: 17930 | Training Loss: 0.8705920577049255 | Testing Loss: 0.6576768159866333\n",
            "Epoch: 17940 | Training Loss: 0.8700316548347473 | Testing Loss: 0.6572362184524536\n",
            "Epoch: 17950 | Training Loss: 0.8694712519645691 | Testing Loss: 0.6567956209182739\n",
            "Epoch: 17960 | Training Loss: 0.8689109086990356 | Testing Loss: 0.6563550233840942\n",
            "Epoch: 17970 | Training Loss: 0.8683504462242126 | Testing Loss: 0.6559144258499146\n",
            "Epoch: 17980 | Training Loss: 0.8677900433540344 | Testing Loss: 0.6554738283157349\n",
            "Epoch: 17990 | Training Loss: 0.8672296404838562 | Testing Loss: 0.6550332307815552\n",
            "Epoch: 18000 | Training Loss: 0.8666692972183228 | Testing Loss: 0.6545926332473755\n",
            "Epoch: 18010 | Training Loss: 0.8661088347434998 | Testing Loss: 0.6541520357131958\n",
            "Epoch: 18020 | Training Loss: 0.8655484318733215 | Testing Loss: 0.6537114381790161\n",
            "Epoch: 18030 | Training Loss: 0.8649880290031433 | Testing Loss: 0.6532708406448364\n",
            "Epoch: 18040 | Training Loss: 0.8644276857376099 | Testing Loss: 0.6528302431106567\n",
            "Epoch: 18050 | Training Loss: 0.8638672232627869 | Testing Loss: 0.652389645576477\n",
            "Epoch: 18060 | Training Loss: 0.8633068203926086 | Testing Loss: 0.6519490480422974\n",
            "Epoch: 18070 | Training Loss: 0.8627464175224304 | Testing Loss: 0.6515084505081177\n",
            "Epoch: 18080 | Training Loss: 0.862186074256897 | Testing Loss: 0.651067852973938\n",
            "Epoch: 18090 | Training Loss: 0.861625611782074 | Testing Loss: 0.6506272554397583\n",
            "Epoch: 18100 | Training Loss: 0.8610652089118958 | Testing Loss: 0.6501866579055786\n",
            "Epoch: 18110 | Training Loss: 0.8605048060417175 | Testing Loss: 0.6497460603713989\n",
            "Epoch: 18120 | Training Loss: 0.8599444627761841 | Testing Loss: 0.6493054628372192\n",
            "Epoch: 18130 | Training Loss: 0.8593840003013611 | Testing Loss: 0.6488648653030396\n",
            "Epoch: 18140 | Training Loss: 0.8588235974311829 | Testing Loss: 0.6484242677688599\n",
            "Epoch: 18150 | Training Loss: 0.8582631945610046 | Testing Loss: 0.6479836702346802\n",
            "Epoch: 18160 | Training Loss: 0.8577028512954712 | Testing Loss: 0.6475430727005005\n",
            "Epoch: 18170 | Training Loss: 0.8571423888206482 | Testing Loss: 0.6471024751663208\n",
            "Epoch: 18180 | Training Loss: 0.85658198595047 | Testing Loss: 0.6466618776321411\n",
            "Epoch: 18190 | Training Loss: 0.8560215830802917 | Testing Loss: 0.6462212800979614\n",
            "Epoch: 18200 | Training Loss: 0.8554612398147583 | Testing Loss: 0.6457806825637817\n",
            "Epoch: 18210 | Training Loss: 0.8549007773399353 | Testing Loss: 0.645340085029602\n",
            "Epoch: 18220 | Training Loss: 0.8543403744697571 | Testing Loss: 0.6448994874954224\n",
            "Epoch: 18230 | Training Loss: 0.8537799715995789 | Testing Loss: 0.6444588899612427\n",
            "Epoch: 18240 | Training Loss: 0.8532196283340454 | Testing Loss: 0.644018292427063\n",
            "Epoch: 18250 | Training Loss: 0.8526591658592224 | Testing Loss: 0.6435776948928833\n",
            "Epoch: 18260 | Training Loss: 0.8520987629890442 | Testing Loss: 0.6431370973587036\n",
            "Epoch: 18270 | Training Loss: 0.851538360118866 | Testing Loss: 0.6426964998245239\n",
            "Epoch: 18280 | Training Loss: 0.8509780168533325 | Testing Loss: 0.6422559022903442\n",
            "Epoch: 18290 | Training Loss: 0.8504175543785095 | Testing Loss: 0.6418153047561646\n",
            "Epoch: 18300 | Training Loss: 0.8498571515083313 | Testing Loss: 0.6413747072219849\n",
            "Epoch: 18310 | Training Loss: 0.8492967486381531 | Testing Loss: 0.6409341096878052\n",
            "Epoch: 18320 | Training Loss: 0.8487364053726196 | Testing Loss: 0.6404935121536255\n",
            "Epoch: 18330 | Training Loss: 0.8481759428977966 | Testing Loss: 0.6400529146194458\n",
            "Epoch: 18340 | Training Loss: 0.8476155400276184 | Testing Loss: 0.6396123170852661\n",
            "Epoch: 18350 | Training Loss: 0.8470551371574402 | Testing Loss: 0.6391717195510864\n",
            "Epoch: 18360 | Training Loss: 0.8464947938919067 | Testing Loss: 0.6387311220169067\n",
            "Epoch: 18370 | Training Loss: 0.8459343314170837 | Testing Loss: 0.638290524482727\n",
            "Epoch: 18380 | Training Loss: 0.8453739285469055 | Testing Loss: 0.6378499269485474\n",
            "Epoch: 18390 | Training Loss: 0.8448135256767273 | Testing Loss: 0.6374093294143677\n",
            "Epoch: 18400 | Training Loss: 0.8442531824111938 | Testing Loss: 0.636968731880188\n",
            "Epoch: 18410 | Training Loss: 0.8436927199363708 | Testing Loss: 0.6365281343460083\n",
            "Epoch: 18420 | Training Loss: 0.8431323170661926 | Testing Loss: 0.6360875368118286\n",
            "Epoch: 18430 | Training Loss: 0.8425719141960144 | Testing Loss: 0.6356469392776489\n",
            "Epoch: 18440 | Training Loss: 0.842011570930481 | Testing Loss: 0.6352063417434692\n",
            "Epoch: 18450 | Training Loss: 0.841451108455658 | Testing Loss: 0.6347657442092896\n",
            "Epoch: 18460 | Training Loss: 0.8408907055854797 | Testing Loss: 0.6343251466751099\n",
            "Epoch: 18470 | Training Loss: 0.8403303027153015 | Testing Loss: 0.6338845491409302\n",
            "Epoch: 18480 | Training Loss: 0.8397699594497681 | Testing Loss: 0.6334439516067505\n",
            "Epoch: 18490 | Training Loss: 0.8392094969749451 | Testing Loss: 0.6330033540725708\n",
            "Epoch: 18500 | Training Loss: 0.8386490941047668 | Testing Loss: 0.6325627565383911\n",
            "Epoch: 18510 | Training Loss: 0.8380886912345886 | Testing Loss: 0.6321221590042114\n",
            "Epoch: 18520 | Training Loss: 0.8375283479690552 | Testing Loss: 0.6316815614700317\n",
            "Epoch: 18530 | Training Loss: 0.8369678854942322 | Testing Loss: 0.631240963935852\n",
            "Epoch: 18540 | Training Loss: 0.836407482624054 | Testing Loss: 0.6308003664016724\n",
            "Epoch: 18550 | Training Loss: 0.8358470797538757 | Testing Loss: 0.6303597688674927\n",
            "Epoch: 18560 | Training Loss: 0.8352867364883423 | Testing Loss: 0.629919171333313\n",
            "Epoch: 18570 | Training Loss: 0.8347262740135193 | Testing Loss: 0.6294785737991333\n",
            "Epoch: 18580 | Training Loss: 0.8341658711433411 | Testing Loss: 0.6290379762649536\n",
            "Epoch: 18590 | Training Loss: 0.8336054682731628 | Testing Loss: 0.6285973787307739\n",
            "Epoch: 18600 | Training Loss: 0.8330451250076294 | Testing Loss: 0.6281567811965942\n",
            "Epoch: 18610 | Training Loss: 0.8324846625328064 | Testing Loss: 0.6277161836624146\n",
            "Epoch: 18620 | Training Loss: 0.8319242596626282 | Testing Loss: 0.6272755861282349\n",
            "Epoch: 18630 | Training Loss: 0.83136385679245 | Testing Loss: 0.6268349885940552\n",
            "Epoch: 18640 | Training Loss: 0.8308035135269165 | Testing Loss: 0.6263943910598755\n",
            "Epoch: 18650 | Training Loss: 0.8302430510520935 | Testing Loss: 0.6259537935256958\n",
            "Epoch: 18660 | Training Loss: 0.8296826481819153 | Testing Loss: 0.6255131959915161\n",
            "Epoch: 18670 | Training Loss: 0.8291222453117371 | Testing Loss: 0.6250725984573364\n",
            "Epoch: 18680 | Training Loss: 0.8285619020462036 | Testing Loss: 0.6246320009231567\n",
            "Epoch: 18690 | Training Loss: 0.8280014395713806 | Testing Loss: 0.624191403388977\n",
            "Epoch: 18700 | Training Loss: 0.8274410367012024 | Testing Loss: 0.6237508058547974\n",
            "Epoch: 18710 | Training Loss: 0.8268806338310242 | Testing Loss: 0.6233102083206177\n",
            "Epoch: 18720 | Training Loss: 0.8263202905654907 | Testing Loss: 0.622869610786438\n",
            "Epoch: 18730 | Training Loss: 0.8257598280906677 | Testing Loss: 0.6224290132522583\n",
            "Epoch: 18740 | Training Loss: 0.8251994252204895 | Testing Loss: 0.6219884157180786\n",
            "Epoch: 18750 | Training Loss: 0.8246390223503113 | Testing Loss: 0.6215478181838989\n",
            "Epoch: 18760 | Training Loss: 0.8240786790847778 | Testing Loss: 0.6211072206497192\n",
            "Epoch: 18770 | Training Loss: 0.8235182166099548 | Testing Loss: 0.6206666231155396\n",
            "Epoch: 18780 | Training Loss: 0.8229578137397766 | Testing Loss: 0.6202260255813599\n",
            "Epoch: 18790 | Training Loss: 0.8223974108695984 | Testing Loss: 0.6197854280471802\n",
            "Epoch: 18800 | Training Loss: 0.8218370676040649 | Testing Loss: 0.6193448305130005\n",
            "Epoch: 18810 | Training Loss: 0.8212766051292419 | Testing Loss: 0.6189042329788208\n",
            "Epoch: 18820 | Training Loss: 0.8207162022590637 | Testing Loss: 0.6184636354446411\n",
            "Epoch: 18830 | Training Loss: 0.8201557993888855 | Testing Loss: 0.6180230379104614\n",
            "Epoch: 18840 | Training Loss: 0.819595456123352 | Testing Loss: 0.6175824403762817\n",
            "Epoch: 18850 | Training Loss: 0.819034993648529 | Testing Loss: 0.617141842842102\n",
            "Epoch: 18860 | Training Loss: 0.8184745907783508 | Testing Loss: 0.6167012453079224\n",
            "Epoch: 18870 | Training Loss: 0.8179141879081726 | Testing Loss: 0.6162606477737427\n",
            "Epoch: 18880 | Training Loss: 0.8173538446426392 | Testing Loss: 0.615820050239563\n",
            "Epoch: 18890 | Training Loss: 0.8167933821678162 | Testing Loss: 0.6153794527053833\n",
            "Epoch: 18900 | Training Loss: 0.8162329792976379 | Testing Loss: 0.6149388551712036\n",
            "Epoch: 18910 | Training Loss: 0.8156725764274597 | Testing Loss: 0.6144982576370239\n",
            "Epoch: 18920 | Training Loss: 0.8151122331619263 | Testing Loss: 0.6140576601028442\n",
            "Epoch: 18930 | Training Loss: 0.8145517706871033 | Testing Loss: 0.6136170625686646\n",
            "Epoch: 18940 | Training Loss: 0.813991367816925 | Testing Loss: 0.6131764650344849\n",
            "Epoch: 18950 | Training Loss: 0.8134309649467468 | Testing Loss: 0.6127358675003052\n",
            "Epoch: 18960 | Training Loss: 0.8128706216812134 | Testing Loss: 0.6122952699661255\n",
            "Epoch: 18970 | Training Loss: 0.8123101592063904 | Testing Loss: 0.6118546724319458\n",
            "Epoch: 18980 | Training Loss: 0.8117497563362122 | Testing Loss: 0.6114140748977661\n",
            "Epoch: 18990 | Training Loss: 0.8111893534660339 | Testing Loss: 0.6109734773635864\n",
            "Epoch: 19000 | Training Loss: 0.8106290102005005 | Testing Loss: 0.6105328798294067\n",
            "Epoch: 19010 | Training Loss: 0.8100685477256775 | Testing Loss: 0.610092282295227\n",
            "Epoch: 19020 | Training Loss: 0.8095081448554993 | Testing Loss: 0.6096516847610474\n",
            "Epoch: 19030 | Training Loss: 0.808947741985321 | Testing Loss: 0.6092110872268677\n",
            "Epoch: 19040 | Training Loss: 0.8083873987197876 | Testing Loss: 0.608770489692688\n",
            "Epoch: 19050 | Training Loss: 0.8078269362449646 | Testing Loss: 0.6083298921585083\n",
            "Epoch: 19060 | Training Loss: 0.8072665333747864 | Testing Loss: 0.6078892946243286\n",
            "Epoch: 19070 | Training Loss: 0.8067061305046082 | Testing Loss: 0.6074486970901489\n",
            "Epoch: 19080 | Training Loss: 0.8061457872390747 | Testing Loss: 0.6070080995559692\n",
            "Epoch: 19090 | Training Loss: 0.8055853247642517 | Testing Loss: 0.6065675020217896\n",
            "Epoch: 19100 | Training Loss: 0.8050249218940735 | Testing Loss: 0.6061269044876099\n",
            "Epoch: 19110 | Training Loss: 0.8044645190238953 | Testing Loss: 0.6056863069534302\n",
            "Epoch: 19120 | Training Loss: 0.8039041757583618 | Testing Loss: 0.6052457094192505\n",
            "Epoch: 19130 | Training Loss: 0.8033437132835388 | Testing Loss: 0.6048051118850708\n",
            "Epoch: 19140 | Training Loss: 0.8027833104133606 | Testing Loss: 0.6043645143508911\n",
            "Epoch: 19150 | Training Loss: 0.8022229075431824 | Testing Loss: 0.6039239168167114\n",
            "Epoch: 19160 | Training Loss: 0.8016625642776489 | Testing Loss: 0.6034833192825317\n",
            "Epoch: 19170 | Training Loss: 0.8011021018028259 | Testing Loss: 0.603042721748352\n",
            "Epoch: 19180 | Training Loss: 0.8005416989326477 | Testing Loss: 0.6026021242141724\n",
            "Epoch: 19190 | Training Loss: 0.7999812960624695 | Testing Loss: 0.6021615266799927\n",
            "Epoch: 19200 | Training Loss: 0.799420952796936 | Testing Loss: 0.601720929145813\n",
            "Epoch: 19210 | Training Loss: 0.798860490322113 | Testing Loss: 0.6012803316116333\n",
            "Epoch: 19220 | Training Loss: 0.7983000874519348 | Testing Loss: 0.6008397340774536\n",
            "Epoch: 19230 | Training Loss: 0.7977396845817566 | Testing Loss: 0.6003991365432739\n",
            "Epoch: 19240 | Training Loss: 0.7971793413162231 | Testing Loss: 0.5999585390090942\n",
            "Epoch: 19250 | Training Loss: 0.7966188788414001 | Testing Loss: 0.5995179414749146\n",
            "Epoch: 19260 | Training Loss: 0.7960584759712219 | Testing Loss: 0.5990773439407349\n",
            "Epoch: 19270 | Training Loss: 0.7954980731010437 | Testing Loss: 0.5986367464065552\n",
            "Epoch: 19280 | Training Loss: 0.7949377298355103 | Testing Loss: 0.5981961488723755\n",
            "Epoch: 19290 | Training Loss: 0.7943772673606873 | Testing Loss: 0.5977555513381958\n",
            "Epoch: 19300 | Training Loss: 0.793816864490509 | Testing Loss: 0.5973149538040161\n",
            "Epoch: 19310 | Training Loss: 0.7932564616203308 | Testing Loss: 0.5968743562698364\n",
            "Epoch: 19320 | Training Loss: 0.7926961183547974 | Testing Loss: 0.5964337587356567\n",
            "Epoch: 19330 | Training Loss: 0.7921356558799744 | Testing Loss: 0.595993161201477\n",
            "Epoch: 19340 | Training Loss: 0.7915752530097961 | Testing Loss: 0.5955525636672974\n",
            "Epoch: 19350 | Training Loss: 0.7910148501396179 | Testing Loss: 0.5951119661331177\n",
            "Epoch: 19360 | Training Loss: 0.7904545068740845 | Testing Loss: 0.594671368598938\n",
            "Epoch: 19370 | Training Loss: 0.7898940443992615 | Testing Loss: 0.5942307710647583\n",
            "Epoch: 19380 | Training Loss: 0.7893336415290833 | Testing Loss: 0.5937901735305786\n",
            "Epoch: 19390 | Training Loss: 0.788773238658905 | Testing Loss: 0.5933495759963989\n",
            "Epoch: 19400 | Training Loss: 0.7882128953933716 | Testing Loss: 0.5929089784622192\n",
            "Epoch: 19410 | Training Loss: 0.7876524329185486 | Testing Loss: 0.5924683809280396\n",
            "Epoch: 19420 | Training Loss: 0.7870920300483704 | Testing Loss: 0.5920277833938599\n",
            "Epoch: 19430 | Training Loss: 0.7865316271781921 | Testing Loss: 0.5915871858596802\n",
            "Epoch: 19440 | Training Loss: 0.7859712839126587 | Testing Loss: 0.5911465883255005\n",
            "Epoch: 19450 | Training Loss: 0.7854108214378357 | Testing Loss: 0.5907059907913208\n",
            "Epoch: 19460 | Training Loss: 0.7848504185676575 | Testing Loss: 0.5902653932571411\n",
            "Epoch: 19470 | Training Loss: 0.7842900156974792 | Testing Loss: 0.5898247957229614\n",
            "Epoch: 19480 | Training Loss: 0.7837296724319458 | Testing Loss: 0.5893841981887817\n",
            "Epoch: 19490 | Training Loss: 0.7831692099571228 | Testing Loss: 0.588943600654602\n",
            "Epoch: 19500 | Training Loss: 0.7826088070869446 | Testing Loss: 0.5885030031204224\n",
            "Epoch: 19510 | Training Loss: 0.7820484042167664 | Testing Loss: 0.5880624055862427\n",
            "Epoch: 19520 | Training Loss: 0.7814880609512329 | Testing Loss: 0.587621808052063\n",
            "Epoch: 19530 | Training Loss: 0.7809275984764099 | Testing Loss: 0.5871812105178833\n",
            "Epoch: 19540 | Training Loss: 0.7803671956062317 | Testing Loss: 0.5867406129837036\n",
            "Epoch: 19550 | Training Loss: 0.7798067927360535 | Testing Loss: 0.5863000154495239\n",
            "Epoch: 19560 | Training Loss: 0.77924644947052 | Testing Loss: 0.5858594179153442\n",
            "Epoch: 19570 | Training Loss: 0.778685986995697 | Testing Loss: 0.5854188203811646\n",
            "Epoch: 19580 | Training Loss: 0.7781255841255188 | Testing Loss: 0.5849782228469849\n",
            "Epoch: 19590 | Training Loss: 0.7775651812553406 | Testing Loss: 0.5845376253128052\n",
            "Epoch: 19600 | Training Loss: 0.7770048379898071 | Testing Loss: 0.5840970277786255\n",
            "Epoch: 19610 | Training Loss: 0.7764443755149841 | Testing Loss: 0.5836564302444458\n",
            "Epoch: 19620 | Training Loss: 0.7758839726448059 | Testing Loss: 0.5832158327102661\n",
            "Epoch: 19630 | Training Loss: 0.7753235697746277 | Testing Loss: 0.5827752351760864\n",
            "Epoch: 19640 | Training Loss: 0.7747632265090942 | Testing Loss: 0.5823346376419067\n",
            "Epoch: 19650 | Training Loss: 0.7742027640342712 | Testing Loss: 0.581894040107727\n",
            "Epoch: 19660 | Training Loss: 0.773642361164093 | Testing Loss: 0.5814534425735474\n",
            "Epoch: 19670 | Training Loss: 0.7730819582939148 | Testing Loss: 0.5810128450393677\n",
            "Epoch: 19680 | Training Loss: 0.7725216150283813 | Testing Loss: 0.580572247505188\n",
            "Epoch: 19690 | Training Loss: 0.7719611525535583 | Testing Loss: 0.5801316499710083\n",
            "Epoch: 19700 | Training Loss: 0.7714007496833801 | Testing Loss: 0.5796910524368286\n",
            "Epoch: 19710 | Training Loss: 0.7708403468132019 | Testing Loss: 0.5792504549026489\n",
            "Epoch: 19720 | Training Loss: 0.7702800035476685 | Testing Loss: 0.5788098573684692\n",
            "Epoch: 19730 | Training Loss: 0.7697195410728455 | Testing Loss: 0.5783692598342896\n",
            "Epoch: 19740 | Training Loss: 0.7691591382026672 | Testing Loss: 0.5779286623001099\n",
            "Epoch: 19750 | Training Loss: 0.768598735332489 | Testing Loss: 0.5774880051612854\n",
            "Epoch: 19760 | Training Loss: 0.7680383920669556 | Testing Loss: 0.5770474076271057\n",
            "Epoch: 19770 | Training Loss: 0.7674779295921326 | Testing Loss: 0.576606810092926\n",
            "Epoch: 19780 | Training Loss: 0.7669175267219543 | Testing Loss: 0.5761662125587463\n",
            "Epoch: 19790 | Training Loss: 0.7663571238517761 | Testing Loss: 0.5757256150245667\n",
            "Epoch: 19800 | Training Loss: 0.7657967805862427 | Testing Loss: 0.575285017490387\n",
            "Epoch: 19810 | Training Loss: 0.7652363181114197 | Testing Loss: 0.5748444199562073\n",
            "Epoch: 19820 | Training Loss: 0.7646759152412415 | Testing Loss: 0.5744038224220276\n",
            "Epoch: 19830 | Training Loss: 0.7641155123710632 | Testing Loss: 0.5739632248878479\n",
            "Epoch: 19840 | Training Loss: 0.7635551691055298 | Testing Loss: 0.5735226273536682\n",
            "Epoch: 19850 | Training Loss: 0.7629947066307068 | Testing Loss: 0.5730820298194885\n",
            "Epoch: 19860 | Training Loss: 0.7624343037605286 | Testing Loss: 0.5726414322853088\n",
            "Epoch: 19870 | Training Loss: 0.7618739008903503 | Testing Loss: 0.5722008347511292\n",
            "Epoch: 19880 | Training Loss: 0.7613135576248169 | Testing Loss: 0.5717602372169495\n",
            "Epoch: 19890 | Training Loss: 0.7607530951499939 | Testing Loss: 0.5713196396827698\n",
            "Epoch: 19900 | Training Loss: 0.7601926922798157 | Testing Loss: 0.5708790421485901\n",
            "Epoch: 19910 | Training Loss: 0.7596322894096375 | Testing Loss: 0.5704384446144104\n",
            "Epoch: 19920 | Training Loss: 0.759071946144104 | Testing Loss: 0.5699978470802307\n",
            "Epoch: 19930 | Training Loss: 0.758511483669281 | Testing Loss: 0.569557249546051\n",
            "Epoch: 19940 | Training Loss: 0.7579510807991028 | Testing Loss: 0.5691166520118713\n",
            "Epoch: 19950 | Training Loss: 0.7573906779289246 | Testing Loss: 0.5686760544776917\n",
            "Epoch: 19960 | Training Loss: 0.7568303346633911 | Testing Loss: 0.568235456943512\n",
            "Epoch: 19970 | Training Loss: 0.7562698721885681 | Testing Loss: 0.5677948594093323\n",
            "Epoch: 19980 | Training Loss: 0.7557094693183899 | Testing Loss: 0.5673542618751526\n",
            "Epoch: 19990 | Training Loss: 0.7551490664482117 | Testing Loss: 0.5669136643409729\n",
            "Epoch: 20000 | Training Loss: 0.7545887231826782 | Testing Loss: 0.5664730668067932\n",
            "Epoch: 20010 | Training Loss: 0.7540282607078552 | Testing Loss: 0.5660324692726135\n",
            "Epoch: 20020 | Training Loss: 0.753467857837677 | Testing Loss: 0.5655918717384338\n",
            "Epoch: 20030 | Training Loss: 0.7529074549674988 | Testing Loss: 0.5651512742042542\n",
            "Epoch: 20040 | Training Loss: 0.7523471117019653 | Testing Loss: 0.5647106766700745\n",
            "Epoch: 20050 | Training Loss: 0.7517866492271423 | Testing Loss: 0.5642700791358948\n",
            "Epoch: 20060 | Training Loss: 0.7512262463569641 | Testing Loss: 0.5638294816017151\n",
            "Epoch: 20070 | Training Loss: 0.7506658434867859 | Testing Loss: 0.5633888840675354\n",
            "Epoch: 20080 | Training Loss: 0.7501055002212524 | Testing Loss: 0.5629482865333557\n",
            "Epoch: 20090 | Training Loss: 0.7495450377464294 | Testing Loss: 0.562507688999176\n",
            "Epoch: 20100 | Training Loss: 0.7489846348762512 | Testing Loss: 0.5620670914649963\n",
            "Epoch: 20110 | Training Loss: 0.748424232006073 | Testing Loss: 0.5616264939308167\n",
            "Epoch: 20120 | Training Loss: 0.7478638887405396 | Testing Loss: 0.561185896396637\n",
            "Epoch: 20130 | Training Loss: 0.7473034262657166 | Testing Loss: 0.5607452988624573\n",
            "Epoch: 20140 | Training Loss: 0.7467430233955383 | Testing Loss: 0.5603047013282776\n",
            "Epoch: 20150 | Training Loss: 0.7461826205253601 | Testing Loss: 0.5598641037940979\n",
            "Epoch: 20160 | Training Loss: 0.7456222772598267 | Testing Loss: 0.5594235062599182\n",
            "Epoch: 20170 | Training Loss: 0.7450618147850037 | Testing Loss: 0.5589829087257385\n",
            "Epoch: 20180 | Training Loss: 0.7445014119148254 | Testing Loss: 0.5585423111915588\n",
            "Epoch: 20190 | Training Loss: 0.7439410090446472 | Testing Loss: 0.5581017136573792\n",
            "Epoch: 20200 | Training Loss: 0.7433806657791138 | Testing Loss: 0.5576611161231995\n",
            "Epoch: 20210 | Training Loss: 0.7428202033042908 | Testing Loss: 0.5572205185890198\n",
            "Epoch: 20220 | Training Loss: 0.7422598004341125 | Testing Loss: 0.5567799210548401\n",
            "Epoch: 20230 | Training Loss: 0.7416993975639343 | Testing Loss: 0.5563393235206604\n",
            "Epoch: 20240 | Training Loss: 0.7411390542984009 | Testing Loss: 0.5558987259864807\n",
            "Epoch: 20250 | Training Loss: 0.7405785918235779 | Testing Loss: 0.555458128452301\n",
            "Epoch: 20260 | Training Loss: 0.7400181889533997 | Testing Loss: 0.5550175309181213\n",
            "Epoch: 20270 | Training Loss: 0.7394577860832214 | Testing Loss: 0.5545769333839417\n",
            "Epoch: 20280 | Training Loss: 0.738897442817688 | Testing Loss: 0.554136335849762\n",
            "Epoch: 20290 | Training Loss: 0.738336980342865 | Testing Loss: 0.5536957383155823\n",
            "Epoch: 20300 | Training Loss: 0.7377765774726868 | Testing Loss: 0.5532551407814026\n",
            "Epoch: 20310 | Training Loss: 0.7372161746025085 | Testing Loss: 0.5528145432472229\n",
            "Epoch: 20320 | Training Loss: 0.7366558313369751 | Testing Loss: 0.5523739457130432\n",
            "Epoch: 20330 | Training Loss: 0.7360953688621521 | Testing Loss: 0.5519333481788635\n",
            "Epoch: 20340 | Training Loss: 0.7355349659919739 | Testing Loss: 0.5514927506446838\n",
            "Epoch: 20350 | Training Loss: 0.7349745631217957 | Testing Loss: 0.5510521531105042\n",
            "Epoch: 20360 | Training Loss: 0.7344142198562622 | Testing Loss: 0.5506115555763245\n",
            "Epoch: 20370 | Training Loss: 0.7338537573814392 | Testing Loss: 0.5501709580421448\n",
            "Epoch: 20380 | Training Loss: 0.733293354511261 | Testing Loss: 0.5497303605079651\n",
            "Epoch: 20390 | Training Loss: 0.7327329516410828 | Testing Loss: 0.5492897629737854\n",
            "Epoch: 20400 | Training Loss: 0.7321726083755493 | Testing Loss: 0.5488491654396057\n",
            "Epoch: 20410 | Training Loss: 0.7316121459007263 | Testing Loss: 0.548408567905426\n",
            "Epoch: 20420 | Training Loss: 0.7310517430305481 | Testing Loss: 0.5479679703712463\n",
            "Epoch: 20430 | Training Loss: 0.7304913401603699 | Testing Loss: 0.5475273728370667\n",
            "Epoch: 20440 | Training Loss: 0.7299309968948364 | Testing Loss: 0.547086775302887\n",
            "Epoch: 20450 | Training Loss: 0.7293705344200134 | Testing Loss: 0.5466461777687073\n",
            "Epoch: 20460 | Training Loss: 0.7288101315498352 | Testing Loss: 0.5462055802345276\n",
            "Epoch: 20470 | Training Loss: 0.728249728679657 | Testing Loss: 0.5457649827003479\n",
            "Epoch: 20480 | Training Loss: 0.7276893854141235 | Testing Loss: 0.5453243851661682\n",
            "Epoch: 20490 | Training Loss: 0.7271289229393005 | Testing Loss: 0.5448837876319885\n",
            "Epoch: 20500 | Training Loss: 0.7265685200691223 | Testing Loss: 0.5444431900978088\n",
            "Epoch: 20510 | Training Loss: 0.7260081171989441 | Testing Loss: 0.5440025925636292\n",
            "Epoch: 20520 | Training Loss: 0.7254477739334106 | Testing Loss: 0.5435619950294495\n",
            "Epoch: 20530 | Training Loss: 0.7248873114585876 | Testing Loss: 0.5431213974952698\n",
            "Epoch: 20540 | Training Loss: 0.7243269085884094 | Testing Loss: 0.5426807999610901\n",
            "Epoch: 20550 | Training Loss: 0.7237665057182312 | Testing Loss: 0.5422402024269104\n",
            "Epoch: 20560 | Training Loss: 0.7232061624526978 | Testing Loss: 0.5417996048927307\n",
            "Epoch: 20570 | Training Loss: 0.7226456999778748 | Testing Loss: 0.541359007358551\n",
            "Epoch: 20580 | Training Loss: 0.7220852971076965 | Testing Loss: 0.5409184098243713\n",
            "Epoch: 20590 | Training Loss: 0.7215248942375183 | Testing Loss: 0.5404778122901917\n",
            "Epoch: 20600 | Training Loss: 0.7209645509719849 | Testing Loss: 0.540037214756012\n",
            "Epoch: 20610 | Training Loss: 0.7204040884971619 | Testing Loss: 0.5395966172218323\n",
            "Epoch: 20620 | Training Loss: 0.7198436856269836 | Testing Loss: 0.5391560196876526\n",
            "Epoch: 20630 | Training Loss: 0.7192832827568054 | Testing Loss: 0.5387154221534729\n",
            "Epoch: 20640 | Training Loss: 0.718722939491272 | Testing Loss: 0.5382748246192932\n",
            "Epoch: 20650 | Training Loss: 0.718162477016449 | Testing Loss: 0.5378342270851135\n",
            "Epoch: 20660 | Training Loss: 0.7176020741462708 | Testing Loss: 0.5373936295509338\n",
            "Epoch: 20670 | Training Loss: 0.7170416712760925 | Testing Loss: 0.5369530320167542\n",
            "Epoch: 20680 | Training Loss: 0.7164813280105591 | Testing Loss: 0.5365124344825745\n",
            "Epoch: 20690 | Training Loss: 0.7159208655357361 | Testing Loss: 0.5360718369483948\n",
            "Epoch: 20700 | Training Loss: 0.7153604626655579 | Testing Loss: 0.5356312394142151\n",
            "Epoch: 20710 | Training Loss: 0.7148000597953796 | Testing Loss: 0.5351906418800354\n",
            "Epoch: 20720 | Training Loss: 0.7142397165298462 | Testing Loss: 0.5347500443458557\n",
            "Epoch: 20730 | Training Loss: 0.7136792540550232 | Testing Loss: 0.534309446811676\n",
            "Epoch: 20740 | Training Loss: 0.713118851184845 | Testing Loss: 0.5338688492774963\n",
            "Epoch: 20750 | Training Loss: 0.7125584483146667 | Testing Loss: 0.5334282517433167\n",
            "Epoch: 20760 | Training Loss: 0.7119981050491333 | Testing Loss: 0.532987654209137\n",
            "Epoch: 20770 | Training Loss: 0.7114376425743103 | Testing Loss: 0.5325470566749573\n",
            "Epoch: 20780 | Training Loss: 0.7108772397041321 | Testing Loss: 0.5321064591407776\n",
            "Epoch: 20790 | Training Loss: 0.7103168368339539 | Testing Loss: 0.5316658616065979\n",
            "Epoch: 20800 | Training Loss: 0.7097564935684204 | Testing Loss: 0.5312252640724182\n",
            "Epoch: 20810 | Training Loss: 0.7091960310935974 | Testing Loss: 0.5307846665382385\n",
            "Epoch: 20820 | Training Loss: 0.7086356282234192 | Testing Loss: 0.5303440690040588\n",
            "Epoch: 20830 | Training Loss: 0.708075225353241 | Testing Loss: 0.5299034714698792\n",
            "Epoch: 20840 | Training Loss: 0.7075148820877075 | Testing Loss: 0.5294628739356995\n",
            "Epoch: 20850 | Training Loss: 0.7069544196128845 | Testing Loss: 0.5290222764015198\n",
            "Epoch: 20860 | Training Loss: 0.7063940167427063 | Testing Loss: 0.5285816788673401\n",
            "Epoch: 20870 | Training Loss: 0.7058336138725281 | Testing Loss: 0.5281410813331604\n",
            "Epoch: 20880 | Training Loss: 0.7052732706069946 | Testing Loss: 0.5277004837989807\n",
            "Epoch: 20890 | Training Loss: 0.7047128081321716 | Testing Loss: 0.527259886264801\n",
            "Epoch: 20900 | Training Loss: 0.7041524052619934 | Testing Loss: 0.5268192887306213\n",
            "Epoch: 20910 | Training Loss: 0.7035920023918152 | Testing Loss: 0.5263786911964417\n",
            "Epoch: 20920 | Training Loss: 0.7030316591262817 | Testing Loss: 0.525938093662262\n",
            "Epoch: 20930 | Training Loss: 0.7024711966514587 | Testing Loss: 0.5254974961280823\n",
            "Epoch: 20940 | Training Loss: 0.7019107937812805 | Testing Loss: 0.5250568985939026\n",
            "Epoch: 20950 | Training Loss: 0.7013503909111023 | Testing Loss: 0.5246163010597229\n",
            "Epoch: 20960 | Training Loss: 0.7007900476455688 | Testing Loss: 0.5241757035255432\n",
            "Epoch: 20970 | Training Loss: 0.7002295851707458 | Testing Loss: 0.5237351059913635\n",
            "Epoch: 20980 | Training Loss: 0.6996691823005676 | Testing Loss: 0.5232945084571838\n",
            "Epoch: 20990 | Training Loss: 0.6991087794303894 | Testing Loss: 0.5228539109230042\n",
            "Epoch: 21000 | Training Loss: 0.698548436164856 | Testing Loss: 0.5224133133888245\n",
            "Epoch: 21010 | Training Loss: 0.697987973690033 | Testing Loss: 0.5219727158546448\n",
            "Epoch: 21020 | Training Loss: 0.6974275708198547 | Testing Loss: 0.5215321183204651\n",
            "Epoch: 21030 | Training Loss: 0.6968671679496765 | Testing Loss: 0.5210915207862854\n",
            "Epoch: 21040 | Training Loss: 0.6963068246841431 | Testing Loss: 0.5206509232521057\n",
            "Epoch: 21050 | Training Loss: 0.6957463622093201 | Testing Loss: 0.520210325717926\n",
            "Epoch: 21060 | Training Loss: 0.6951859593391418 | Testing Loss: 0.5197697281837463\n",
            "Epoch: 21070 | Training Loss: 0.6946255564689636 | Testing Loss: 0.5193291306495667\n",
            "Epoch: 21080 | Training Loss: 0.6940652132034302 | Testing Loss: 0.518888533115387\n",
            "Epoch: 21090 | Training Loss: 0.6935047507286072 | Testing Loss: 0.5184479355812073\n",
            "Epoch: 21100 | Training Loss: 0.692944347858429 | Testing Loss: 0.5180073380470276\n",
            "Epoch: 21110 | Training Loss: 0.6923839449882507 | Testing Loss: 0.5175667405128479\n",
            "Epoch: 21120 | Training Loss: 0.6918236017227173 | Testing Loss: 0.5171261429786682\n",
            "Epoch: 21130 | Training Loss: 0.6912631392478943 | Testing Loss: 0.5166855454444885\n",
            "Epoch: 21140 | Training Loss: 0.6907027363777161 | Testing Loss: 0.5162449479103088\n",
            "Epoch: 21150 | Training Loss: 0.6901423335075378 | Testing Loss: 0.5158043503761292\n",
            "Epoch: 21160 | Training Loss: 0.6895819902420044 | Testing Loss: 0.5153637528419495\n",
            "Epoch: 21170 | Training Loss: 0.6890215277671814 | Testing Loss: 0.5149231553077698\n",
            "Epoch: 21180 | Training Loss: 0.6884611248970032 | Testing Loss: 0.5144825577735901\n",
            "Epoch: 21190 | Training Loss: 0.687900722026825 | Testing Loss: 0.5140419602394104\n",
            "Epoch: 21200 | Training Loss: 0.6873403787612915 | Testing Loss: 0.5136013627052307\n",
            "Epoch: 21210 | Training Loss: 0.6867799162864685 | Testing Loss: 0.513160765171051\n",
            "Epoch: 21220 | Training Loss: 0.6862195134162903 | Testing Loss: 0.5127201676368713\n",
            "Epoch: 21230 | Training Loss: 0.6856591105461121 | Testing Loss: 0.5122795701026917\n",
            "Epoch: 21240 | Training Loss: 0.6850987672805786 | Testing Loss: 0.511838972568512\n",
            "Epoch: 21250 | Training Loss: 0.6845383048057556 | Testing Loss: 0.5113983750343323\n",
            "Epoch: 21260 | Training Loss: 0.6839779019355774 | Testing Loss: 0.5109577775001526\n",
            "Epoch: 21270 | Training Loss: 0.6834174990653992 | Testing Loss: 0.5105171799659729\n",
            "Epoch: 21280 | Training Loss: 0.6828571557998657 | Testing Loss: 0.5100765824317932\n",
            "Epoch: 21290 | Training Loss: 0.6822966933250427 | Testing Loss: 0.5096359848976135\n",
            "Epoch: 21300 | Training Loss: 0.6817362904548645 | Testing Loss: 0.5091953873634338\n",
            "Epoch: 21310 | Training Loss: 0.6811758875846863 | Testing Loss: 0.5087547898292542\n",
            "Epoch: 21320 | Training Loss: 0.6806155443191528 | Testing Loss: 0.5083141922950745\n",
            "Epoch: 21330 | Training Loss: 0.6800550818443298 | Testing Loss: 0.5078735947608948\n",
            "Epoch: 21340 | Training Loss: 0.6794946789741516 | Testing Loss: 0.5074329972267151\n",
            "Epoch: 21350 | Training Loss: 0.6789342761039734 | Testing Loss: 0.5069923996925354\n",
            "Epoch: 21360 | Training Loss: 0.6783739328384399 | Testing Loss: 0.5065518021583557\n",
            "Epoch: 21370 | Training Loss: 0.6778134703636169 | Testing Loss: 0.506111204624176\n",
            "Epoch: 21380 | Training Loss: 0.6772530674934387 | Testing Loss: 0.5056706070899963\n",
            "Epoch: 21390 | Training Loss: 0.6766926646232605 | Testing Loss: 0.5052300095558167\n",
            "Epoch: 21400 | Training Loss: 0.676132321357727 | Testing Loss: 0.504789412021637\n",
            "Epoch: 21410 | Training Loss: 0.675571858882904 | Testing Loss: 0.5043488144874573\n",
            "Epoch: 21420 | Training Loss: 0.6750114560127258 | Testing Loss: 0.5039082169532776\n",
            "Epoch: 21430 | Training Loss: 0.6744510531425476 | Testing Loss: 0.5034676194190979\n",
            "Epoch: 21440 | Training Loss: 0.6738907098770142 | Testing Loss: 0.5030270218849182\n",
            "Epoch: 21450 | Training Loss: 0.6733302474021912 | Testing Loss: 0.5025864243507385\n",
            "Epoch: 21460 | Training Loss: 0.6727698445320129 | Testing Loss: 0.5021458268165588\n",
            "Epoch: 21470 | Training Loss: 0.6722094416618347 | Testing Loss: 0.5017052292823792\n",
            "Epoch: 21480 | Training Loss: 0.6716490983963013 | Testing Loss: 0.5012646317481995\n",
            "Epoch: 21490 | Training Loss: 0.6710886359214783 | Testing Loss: 0.5008240342140198\n",
            "Epoch: 21500 | Training Loss: 0.6705282330513 | Testing Loss: 0.5003834366798401\n",
            "Epoch: 21510 | Training Loss: 0.6699678301811218 | Testing Loss: 0.4999428391456604\n",
            "Epoch: 21520 | Training Loss: 0.6694074869155884 | Testing Loss: 0.4995022416114807\n",
            "Epoch: 21530 | Training Loss: 0.6688470244407654 | Testing Loss: 0.499061644077301\n",
            "Epoch: 21540 | Training Loss: 0.6682866215705872 | Testing Loss: 0.49862104654312134\n",
            "Epoch: 21550 | Training Loss: 0.6677262187004089 | Testing Loss: 0.49818044900894165\n",
            "Epoch: 21560 | Training Loss: 0.6671658754348755 | Testing Loss: 0.49773985147476196\n",
            "Epoch: 21570 | Training Loss: 0.6666054129600525 | Testing Loss: 0.4972992539405823\n",
            "Epoch: 21580 | Training Loss: 0.6660450100898743 | Testing Loss: 0.4968586564064026\n",
            "Epoch: 21590 | Training Loss: 0.665484607219696 | Testing Loss: 0.4964180588722229\n",
            "Epoch: 21600 | Training Loss: 0.6649242639541626 | Testing Loss: 0.4959774613380432\n",
            "Epoch: 21610 | Training Loss: 0.6643638014793396 | Testing Loss: 0.4955368638038635\n",
            "Epoch: 21620 | Training Loss: 0.6638033986091614 | Testing Loss: 0.49509626626968384\n",
            "Epoch: 21630 | Training Loss: 0.6632429957389832 | Testing Loss: 0.49465566873550415\n",
            "Epoch: 21640 | Training Loss: 0.6626826524734497 | Testing Loss: 0.49421507120132446\n",
            "Epoch: 21650 | Training Loss: 0.6621221899986267 | Testing Loss: 0.4937744736671448\n",
            "Epoch: 21660 | Training Loss: 0.6615617871284485 | Testing Loss: 0.4933338761329651\n",
            "Epoch: 21670 | Training Loss: 0.6610013842582703 | Testing Loss: 0.4928932785987854\n",
            "Epoch: 21680 | Training Loss: 0.6604410409927368 | Testing Loss: 0.4924526810646057\n",
            "Epoch: 21690 | Training Loss: 0.6598805785179138 | Testing Loss: 0.492012083530426\n",
            "Epoch: 21700 | Training Loss: 0.6593201756477356 | Testing Loss: 0.49157148599624634\n",
            "Epoch: 21710 | Training Loss: 0.6587597727775574 | Testing Loss: 0.49113088846206665\n",
            "Epoch: 21720 | Training Loss: 0.6581994295120239 | Testing Loss: 0.49069029092788696\n",
            "Epoch: 21730 | Training Loss: 0.6576389670372009 | Testing Loss: 0.4902496933937073\n",
            "Epoch: 21740 | Training Loss: 0.6570785641670227 | Testing Loss: 0.4898090958595276\n",
            "Epoch: 21750 | Training Loss: 0.6565181612968445 | Testing Loss: 0.4893684983253479\n",
            "Epoch: 21760 | Training Loss: 0.655957818031311 | Testing Loss: 0.4889279007911682\n",
            "Epoch: 21770 | Training Loss: 0.655397355556488 | Testing Loss: 0.4884873032569885\n",
            "Epoch: 21780 | Training Loss: 0.6548369526863098 | Testing Loss: 0.48804670572280884\n",
            "Epoch: 21790 | Training Loss: 0.6542765498161316 | Testing Loss: 0.48760610818862915\n",
            "Epoch: 21800 | Training Loss: 0.6537162065505981 | Testing Loss: 0.48716551065444946\n",
            "Epoch: 21810 | Training Loss: 0.6531557440757751 | Testing Loss: 0.4867249131202698\n",
            "Epoch: 21820 | Training Loss: 0.6525953412055969 | Testing Loss: 0.4862843155860901\n",
            "Epoch: 21830 | Training Loss: 0.6520349383354187 | Testing Loss: 0.4858437180519104\n",
            "Epoch: 21840 | Training Loss: 0.6514745950698853 | Testing Loss: 0.4854031205177307\n",
            "Epoch: 21850 | Training Loss: 0.6509141325950623 | Testing Loss: 0.484962522983551\n",
            "Epoch: 21860 | Training Loss: 0.650353729724884 | Testing Loss: 0.48452192544937134\n",
            "Epoch: 21870 | Training Loss: 0.6497933268547058 | Testing Loss: 0.48408132791519165\n",
            "Epoch: 21880 | Training Loss: 0.6492329835891724 | Testing Loss: 0.48364073038101196\n",
            "Epoch: 21890 | Training Loss: 0.6486725211143494 | Testing Loss: 0.4832001328468323\n",
            "Epoch: 21900 | Training Loss: 0.6481121182441711 | Testing Loss: 0.4827595353126526\n",
            "Epoch: 21910 | Training Loss: 0.6475517153739929 | Testing Loss: 0.4823189377784729\n",
            "Epoch: 21920 | Training Loss: 0.6469913721084595 | Testing Loss: 0.4818783402442932\n",
            "Epoch: 21930 | Training Loss: 0.6464309096336365 | Testing Loss: 0.4814377427101135\n",
            "Epoch: 21940 | Training Loss: 0.6458705067634583 | Testing Loss: 0.48099714517593384\n",
            "Epoch: 21950 | Training Loss: 0.64531010389328 | Testing Loss: 0.48055654764175415\n",
            "Epoch: 21960 | Training Loss: 0.6447497606277466 | Testing Loss: 0.48011595010757446\n",
            "Epoch: 21970 | Training Loss: 0.6441892981529236 | Testing Loss: 0.4796753525733948\n",
            "Epoch: 21980 | Training Loss: 0.6436288952827454 | Testing Loss: 0.4792347550392151\n",
            "Epoch: 21990 | Training Loss: 0.6430684924125671 | Testing Loss: 0.4787941575050354\n",
            "Epoch: 22000 | Training Loss: 0.6425081491470337 | Testing Loss: 0.4783535599708557\n",
            "Epoch: 22010 | Training Loss: 0.6419476866722107 | Testing Loss: 0.477912962436676\n",
            "Epoch: 22020 | Training Loss: 0.6413872838020325 | Testing Loss: 0.47747236490249634\n",
            "Epoch: 22030 | Training Loss: 0.6408268809318542 | Testing Loss: 0.47703176736831665\n",
            "Epoch: 22040 | Training Loss: 0.6402665376663208 | Testing Loss: 0.47659116983413696\n",
            "Epoch: 22050 | Training Loss: 0.6397060751914978 | Testing Loss: 0.4761505722999573\n",
            "Epoch: 22060 | Training Loss: 0.6391456723213196 | Testing Loss: 0.4757099747657776\n",
            "Epoch: 22070 | Training Loss: 0.6385852694511414 | Testing Loss: 0.4752693772315979\n",
            "Epoch: 22080 | Training Loss: 0.6380249261856079 | Testing Loss: 0.4748287796974182\n",
            "Epoch: 22090 | Training Loss: 0.6374644637107849 | Testing Loss: 0.4743881821632385\n",
            "Epoch: 22100 | Training Loss: 0.6369040608406067 | Testing Loss: 0.47394758462905884\n",
            "Epoch: 22110 | Training Loss: 0.6363436579704285 | Testing Loss: 0.47350698709487915\n",
            "Epoch: 22120 | Training Loss: 0.635783314704895 | Testing Loss: 0.47306638956069946\n",
            "Epoch: 22130 | Training Loss: 0.635222852230072 | Testing Loss: 0.4726257920265198\n",
            "Epoch: 22140 | Training Loss: 0.6346624493598938 | Testing Loss: 0.4721851944923401\n",
            "Epoch: 22150 | Training Loss: 0.6341020464897156 | Testing Loss: 0.4717445969581604\n",
            "Epoch: 22160 | Training Loss: 0.6335417032241821 | Testing Loss: 0.4713039994239807\n",
            "Epoch: 22170 | Training Loss: 0.6329812407493591 | Testing Loss: 0.470863401889801\n",
            "Epoch: 22180 | Training Loss: 0.6324208378791809 | Testing Loss: 0.47042280435562134\n",
            "Epoch: 22190 | Training Loss: 0.6318604350090027 | Testing Loss: 0.46998220682144165\n",
            "Epoch: 22200 | Training Loss: 0.6313000917434692 | Testing Loss: 0.46954160928726196\n",
            "Epoch: 22210 | Training Loss: 0.6307396292686462 | Testing Loss: 0.4691010117530823\n",
            "Epoch: 22220 | Training Loss: 0.630179226398468 | Testing Loss: 0.4686604142189026\n",
            "Epoch: 22230 | Training Loss: 0.6296188235282898 | Testing Loss: 0.4682198166847229\n",
            "Epoch: 22240 | Training Loss: 0.6290584802627563 | Testing Loss: 0.4677792191505432\n",
            "Epoch: 22250 | Training Loss: 0.6284980177879333 | Testing Loss: 0.4673386216163635\n",
            "Epoch: 22260 | Training Loss: 0.6279376149177551 | Testing Loss: 0.46689802408218384\n",
            "Epoch: 22270 | Training Loss: 0.6273772120475769 | Testing Loss: 0.46645742654800415\n",
            "Epoch: 22280 | Training Loss: 0.6268168687820435 | Testing Loss: 0.46601682901382446\n",
            "Epoch: 22290 | Training Loss: 0.6262564063072205 | Testing Loss: 0.4655762314796448\n",
            "Epoch: 22300 | Training Loss: 0.6256960034370422 | Testing Loss: 0.4651356339454651\n",
            "Epoch: 22310 | Training Loss: 0.625135600566864 | Testing Loss: 0.4646950364112854\n",
            "Epoch: 22320 | Training Loss: 0.6245752573013306 | Testing Loss: 0.4642544388771057\n",
            "Epoch: 22330 | Training Loss: 0.6240147948265076 | Testing Loss: 0.463813841342926\n",
            "Epoch: 22340 | Training Loss: 0.6234543919563293 | Testing Loss: 0.46337324380874634\n",
            "Epoch: 22350 | Training Loss: 0.6228939890861511 | Testing Loss: 0.46293264627456665\n",
            "Epoch: 22360 | Training Loss: 0.6223336458206177 | Testing Loss: 0.46249204874038696\n",
            "Epoch: 22370 | Training Loss: 0.6217731833457947 | Testing Loss: 0.4620514512062073\n",
            "Epoch: 22380 | Training Loss: 0.6212127804756165 | Testing Loss: 0.4616108536720276\n",
            "Epoch: 22390 | Training Loss: 0.6206523776054382 | Testing Loss: 0.4611702561378479\n",
            "Epoch: 22400 | Training Loss: 0.6200920343399048 | Testing Loss: 0.4607296586036682\n",
            "Epoch: 22410 | Training Loss: 0.6195315718650818 | Testing Loss: 0.4602890610694885\n",
            "Epoch: 22420 | Training Loss: 0.6189711689949036 | Testing Loss: 0.45984846353530884\n",
            "Epoch: 22430 | Training Loss: 0.6184107661247253 | Testing Loss: 0.45940786600112915\n",
            "Epoch: 22440 | Training Loss: 0.6178504228591919 | Testing Loss: 0.45896726846694946\n",
            "Epoch: 22450 | Training Loss: 0.6172899603843689 | Testing Loss: 0.4585266709327698\n",
            "Epoch: 22460 | Training Loss: 0.6167295575141907 | Testing Loss: 0.4580860733985901\n",
            "Epoch: 22470 | Training Loss: 0.6161691546440125 | Testing Loss: 0.4576454758644104\n",
            "Epoch: 22480 | Training Loss: 0.615608811378479 | Testing Loss: 0.4572048783302307\n",
            "Epoch: 22490 | Training Loss: 0.615048348903656 | Testing Loss: 0.456764280796051\n",
            "Epoch: 22500 | Training Loss: 0.6144879460334778 | Testing Loss: 0.45632368326187134\n",
            "Epoch: 22510 | Training Loss: 0.6139275431632996 | Testing Loss: 0.45588308572769165\n",
            "Epoch: 22520 | Training Loss: 0.6133671998977661 | Testing Loss: 0.45544248819351196\n",
            "Epoch: 22530 | Training Loss: 0.6128067374229431 | Testing Loss: 0.4550018906593323\n",
            "Epoch: 22540 | Training Loss: 0.6122463345527649 | Testing Loss: 0.4545612931251526\n",
            "Epoch: 22550 | Training Loss: 0.6116859316825867 | Testing Loss: 0.4541206955909729\n",
            "Epoch: 22560 | Training Loss: 0.6111255884170532 | Testing Loss: 0.4536800980567932\n",
            "Epoch: 22570 | Training Loss: 0.6105651259422302 | Testing Loss: 0.4532395005226135\n",
            "Epoch: 22580 | Training Loss: 0.610004723072052 | Testing Loss: 0.45279890298843384\n",
            "Epoch: 22590 | Training Loss: 0.6094443202018738 | Testing Loss: 0.45235830545425415\n",
            "Epoch: 22600 | Training Loss: 0.6088839769363403 | Testing Loss: 0.45191770792007446\n",
            "Epoch: 22610 | Training Loss: 0.6083235144615173 | Testing Loss: 0.4514771103858948\n",
            "Epoch: 22620 | Training Loss: 0.6077631115913391 | Testing Loss: 0.4510365128517151\n",
            "Epoch: 22630 | Training Loss: 0.6072027087211609 | Testing Loss: 0.4505959153175354\n",
            "Epoch: 22640 | Training Loss: 0.6066423654556274 | Testing Loss: 0.4501553177833557\n",
            "Epoch: 22650 | Training Loss: 0.6060819029808044 | Testing Loss: 0.449714720249176\n",
            "Epoch: 22660 | Training Loss: 0.6055215001106262 | Testing Loss: 0.44927412271499634\n",
            "Epoch: 22670 | Training Loss: 0.604961097240448 | Testing Loss: 0.44883352518081665\n",
            "Epoch: 22680 | Training Loss: 0.6044007539749146 | Testing Loss: 0.44839292764663696\n",
            "Epoch: 22690 | Training Loss: 0.6038402915000916 | Testing Loss: 0.4479523301124573\n",
            "Epoch: 22700 | Training Loss: 0.6032798886299133 | Testing Loss: 0.4475117325782776\n",
            "Epoch: 22710 | Training Loss: 0.6027194857597351 | Testing Loss: 0.4470711350440979\n",
            "Epoch: 22720 | Training Loss: 0.6021591424942017 | Testing Loss: 0.4466305375099182\n",
            "Epoch: 22730 | Training Loss: 0.6015986800193787 | Testing Loss: 0.4461899399757385\n",
            "Epoch: 22740 | Training Loss: 0.6010382771492004 | Testing Loss: 0.44574934244155884\n",
            "Epoch: 22750 | Training Loss: 0.6004778742790222 | Testing Loss: 0.44530874490737915\n",
            "Epoch: 22760 | Training Loss: 0.5999175310134888 | Testing Loss: 0.44486814737319946\n",
            "Epoch: 22770 | Training Loss: 0.5993570685386658 | Testing Loss: 0.4444275498390198\n",
            "Epoch: 22780 | Training Loss: 0.5987966656684875 | Testing Loss: 0.4439869523048401\n",
            "Epoch: 22790 | Training Loss: 0.5982362627983093 | Testing Loss: 0.4435463547706604\n",
            "Epoch: 22800 | Training Loss: 0.5976759195327759 | Testing Loss: 0.4431057572364807\n",
            "Epoch: 22810 | Training Loss: 0.5971154570579529 | Testing Loss: 0.442665159702301\n",
            "Epoch: 22820 | Training Loss: 0.5965550541877747 | Testing Loss: 0.44222456216812134\n",
            "Epoch: 22830 | Training Loss: 0.5959946513175964 | Testing Loss: 0.44178396463394165\n",
            "Epoch: 22840 | Training Loss: 0.595434308052063 | Testing Loss: 0.44134336709976196\n",
            "Epoch: 22850 | Training Loss: 0.59487384557724 | Testing Loss: 0.4409027695655823\n",
            "Epoch: 22860 | Training Loss: 0.5943134427070618 | Testing Loss: 0.4404621720314026\n",
            "Epoch: 22870 | Training Loss: 0.5937530398368835 | Testing Loss: 0.4400215744972229\n",
            "Epoch: 22880 | Training Loss: 0.5931926965713501 | Testing Loss: 0.4395809769630432\n",
            "Epoch: 22890 | Training Loss: 0.5926322340965271 | Testing Loss: 0.4391403794288635\n",
            "Epoch: 22900 | Training Loss: 0.5920718312263489 | Testing Loss: 0.43869978189468384\n",
            "Epoch: 22910 | Training Loss: 0.5915114283561707 | Testing Loss: 0.43825918436050415\n",
            "Epoch: 22920 | Training Loss: 0.5909510850906372 | Testing Loss: 0.43781858682632446\n",
            "Epoch: 22930 | Training Loss: 0.5903906226158142 | Testing Loss: 0.4373779892921448\n",
            "Epoch: 22940 | Training Loss: 0.589830219745636 | Testing Loss: 0.4369373917579651\n",
            "Epoch: 22950 | Training Loss: 0.5892698168754578 | Testing Loss: 0.4364967942237854\n",
            "Epoch: 22960 | Training Loss: 0.5887094736099243 | Testing Loss: 0.4360561966896057\n",
            "Epoch: 22970 | Training Loss: 0.5881490111351013 | Testing Loss: 0.435615599155426\n",
            "Epoch: 22980 | Training Loss: 0.5875886082649231 | Testing Loss: 0.43517500162124634\n",
            "Epoch: 22990 | Training Loss: 0.5870282053947449 | Testing Loss: 0.43473440408706665\n",
            "Epoch: 23000 | Training Loss: 0.5864678621292114 | Testing Loss: 0.43429380655288696\n",
            "Epoch: 23010 | Training Loss: 0.5859073996543884 | Testing Loss: 0.4338532090187073\n",
            "Epoch: 23020 | Training Loss: 0.5853469967842102 | Testing Loss: 0.4334126114845276\n",
            "Epoch: 23030 | Training Loss: 0.584786593914032 | Testing Loss: 0.4329720139503479\n",
            "Epoch: 23040 | Training Loss: 0.5842262506484985 | Testing Loss: 0.4325314164161682\n",
            "Epoch: 23050 | Training Loss: 0.5836657881736755 | Testing Loss: 0.4320908188819885\n",
            "Epoch: 23060 | Training Loss: 0.5831053853034973 | Testing Loss: 0.43165022134780884\n",
            "Epoch: 23070 | Training Loss: 0.5825449824333191 | Testing Loss: 0.43120962381362915\n",
            "Epoch: 23080 | Training Loss: 0.5819846391677856 | Testing Loss: 0.43076902627944946\n",
            "Epoch: 23090 | Training Loss: 0.5814241766929626 | Testing Loss: 0.4303284287452698\n",
            "Epoch: 23100 | Training Loss: 0.5808637738227844 | Testing Loss: 0.4298878312110901\n",
            "Epoch: 23110 | Training Loss: 0.5803033709526062 | Testing Loss: 0.4294472336769104\n",
            "Epoch: 23120 | Training Loss: 0.5797430276870728 | Testing Loss: 0.4290066361427307\n",
            "Epoch: 23130 | Training Loss: 0.5791825652122498 | Testing Loss: 0.428566038608551\n",
            "Epoch: 23140 | Training Loss: 0.5786221623420715 | Testing Loss: 0.42812544107437134\n",
            "Epoch: 23150 | Training Loss: 0.5780617594718933 | Testing Loss: 0.42768484354019165\n",
            "Epoch: 23160 | Training Loss: 0.5775014162063599 | Testing Loss: 0.42724424600601196\n",
            "Epoch: 23170 | Training Loss: 0.5769409537315369 | Testing Loss: 0.4268036484718323\n",
            "Epoch: 23180 | Training Loss: 0.5763805508613586 | Testing Loss: 0.4263630509376526\n",
            "Epoch: 23190 | Training Loss: 0.5758201479911804 | Testing Loss: 0.4259224534034729\n",
            "Epoch: 23200 | Training Loss: 0.575259804725647 | Testing Loss: 0.4254818558692932\n",
            "Epoch: 23210 | Training Loss: 0.574699342250824 | Testing Loss: 0.4250412583351135\n",
            "Epoch: 23220 | Training Loss: 0.5741389393806458 | Testing Loss: 0.42460066080093384\n",
            "Epoch: 23230 | Training Loss: 0.5735785365104675 | Testing Loss: 0.42416006326675415\n",
            "Epoch: 23240 | Training Loss: 0.5730181932449341 | Testing Loss: 0.42371946573257446\n",
            "Epoch: 23250 | Training Loss: 0.5724577307701111 | Testing Loss: 0.4232788681983948\n",
            "Epoch: 23260 | Training Loss: 0.5718973278999329 | Testing Loss: 0.4228382706642151\n",
            "Epoch: 23270 | Training Loss: 0.5713369250297546 | Testing Loss: 0.4223976731300354\n",
            "Epoch: 23280 | Training Loss: 0.5707765817642212 | Testing Loss: 0.4219570755958557\n",
            "Epoch: 23290 | Training Loss: 0.5702161192893982 | Testing Loss: 0.421516478061676\n",
            "Epoch: 23300 | Training Loss: 0.56965571641922 | Testing Loss: 0.42107588052749634\n",
            "Epoch: 23310 | Training Loss: 0.5690953135490417 | Testing Loss: 0.42063528299331665\n",
            "Epoch: 23320 | Training Loss: 0.5685349702835083 | Testing Loss: 0.42019468545913696\n",
            "Epoch: 23330 | Training Loss: 0.5679745078086853 | Testing Loss: 0.4197540879249573\n",
            "Epoch: 23340 | Training Loss: 0.5674141049385071 | Testing Loss: 0.4193134903907776\n",
            "Epoch: 23350 | Training Loss: 0.5668537020683289 | Testing Loss: 0.4188728928565979\n",
            "Epoch: 23360 | Training Loss: 0.5662933588027954 | Testing Loss: 0.4184322953224182\n",
            "Epoch: 23370 | Training Loss: 0.5657328963279724 | Testing Loss: 0.4179916977882385\n",
            "Epoch: 23380 | Training Loss: 0.5651724934577942 | Testing Loss: 0.41755110025405884\n",
            "Epoch: 23390 | Training Loss: 0.564612090587616 | Testing Loss: 0.41711050271987915\n",
            "Epoch: 23400 | Training Loss: 0.5640517473220825 | Testing Loss: 0.41666990518569946\n",
            "Epoch: 23410 | Training Loss: 0.5634912848472595 | Testing Loss: 0.4162293076515198\n",
            "Epoch: 23420 | Training Loss: 0.5629308819770813 | Testing Loss: 0.4157887101173401\n",
            "Epoch: 23430 | Training Loss: 0.5623704791069031 | Testing Loss: 0.4153481125831604\n",
            "Epoch: 23440 | Training Loss: 0.5618101358413696 | Testing Loss: 0.4149075150489807\n",
            "Epoch: 23450 | Training Loss: 0.5612496733665466 | Testing Loss: 0.414466917514801\n",
            "Epoch: 23460 | Training Loss: 0.5606892704963684 | Testing Loss: 0.41402631998062134\n",
            "Epoch: 23470 | Training Loss: 0.5601288676261902 | Testing Loss: 0.41358572244644165\n",
            "Epoch: 23480 | Training Loss: 0.5595685243606567 | Testing Loss: 0.41314512491226196\n",
            "Epoch: 23490 | Training Loss: 0.5590080618858337 | Testing Loss: 0.4127045273780823\n",
            "Epoch: 23500 | Training Loss: 0.5584476590156555 | Testing Loss: 0.4122639298439026\n",
            "Epoch: 23510 | Training Loss: 0.5578872561454773 | Testing Loss: 0.4118233323097229\n",
            "Epoch: 23520 | Training Loss: 0.5573269128799438 | Testing Loss: 0.4113827347755432\n",
            "Epoch: 23530 | Training Loss: 0.5567664504051208 | Testing Loss: 0.4109421372413635\n",
            "Epoch: 23540 | Training Loss: 0.5562060475349426 | Testing Loss: 0.41050153970718384\n",
            "Epoch: 23550 | Training Loss: 0.5556456446647644 | Testing Loss: 0.41006094217300415\n",
            "Epoch: 23560 | Training Loss: 0.555085301399231 | Testing Loss: 0.40962034463882446\n",
            "Epoch: 23570 | Training Loss: 0.554524838924408 | Testing Loss: 0.4091797471046448\n",
            "Epoch: 23580 | Training Loss: 0.5539644360542297 | Testing Loss: 0.4087391495704651\n",
            "Epoch: 23590 | Training Loss: 0.5534040331840515 | Testing Loss: 0.4082985520362854\n",
            "Epoch: 23600 | Training Loss: 0.5528436899185181 | Testing Loss: 0.4078579545021057\n",
            "Epoch: 23610 | Training Loss: 0.5522832274436951 | Testing Loss: 0.407417356967926\n",
            "Epoch: 23620 | Training Loss: 0.5517228245735168 | Testing Loss: 0.40697675943374634\n",
            "Epoch: 23630 | Training Loss: 0.5511624217033386 | Testing Loss: 0.40653616189956665\n",
            "Epoch: 23640 | Training Loss: 0.5506020784378052 | Testing Loss: 0.40609556436538696\n",
            "Epoch: 23650 | Training Loss: 0.5500416159629822 | Testing Loss: 0.4056549668312073\n",
            "Epoch: 23660 | Training Loss: 0.549481213092804 | Testing Loss: 0.4052143692970276\n",
            "Epoch: 23670 | Training Loss: 0.5489208102226257 | Testing Loss: 0.4047737717628479\n",
            "Epoch: 23680 | Training Loss: 0.5483604669570923 | Testing Loss: 0.4043331742286682\n",
            "Epoch: 23690 | Training Loss: 0.5478000044822693 | Testing Loss: 0.4038925766944885\n",
            "Epoch: 23700 | Training Loss: 0.5472396016120911 | Testing Loss: 0.40345197916030884\n",
            "Epoch: 23710 | Training Loss: 0.5466791987419128 | Testing Loss: 0.40301138162612915\n",
            "Epoch: 23720 | Training Loss: 0.5461188554763794 | Testing Loss: 0.40257078409194946\n",
            "Epoch: 23730 | Training Loss: 0.5455583930015564 | Testing Loss: 0.4021301865577698\n",
            "Epoch: 23740 | Training Loss: 0.5449979901313782 | Testing Loss: 0.4016895890235901\n",
            "Epoch: 23750 | Training Loss: 0.5444375872612 | Testing Loss: 0.4012489914894104\n",
            "Epoch: 23760 | Training Loss: 0.5438772439956665 | Testing Loss: 0.4008083939552307\n",
            "Epoch: 23770 | Training Loss: 0.5433167815208435 | Testing Loss: 0.400367796421051\n",
            "Epoch: 23780 | Training Loss: 0.5427563786506653 | Testing Loss: 0.39992719888687134\n",
            "Epoch: 23790 | Training Loss: 0.5421959757804871 | Testing Loss: 0.39948660135269165\n",
            "Epoch: 23800 | Training Loss: 0.5416356325149536 | Testing Loss: 0.39904600381851196\n",
            "Epoch: 23810 | Training Loss: 0.5410751700401306 | Testing Loss: 0.3986054062843323\n",
            "Epoch: 23820 | Training Loss: 0.5405147671699524 | Testing Loss: 0.3981648087501526\n",
            "Epoch: 23830 | Training Loss: 0.5399543642997742 | Testing Loss: 0.3977242112159729\n",
            "Epoch: 23840 | Training Loss: 0.5393940210342407 | Testing Loss: 0.3972836136817932\n",
            "Epoch: 23850 | Training Loss: 0.5388335585594177 | Testing Loss: 0.3968430161476135\n",
            "Epoch: 23860 | Training Loss: 0.5382731556892395 | Testing Loss: 0.39640241861343384\n",
            "Epoch: 23870 | Training Loss: 0.5377127528190613 | Testing Loss: 0.39596182107925415\n",
            "Epoch: 23880 | Training Loss: 0.5371524095535278 | Testing Loss: 0.39552122354507446\n",
            "Epoch: 23890 | Training Loss: 0.5365919470787048 | Testing Loss: 0.3950806260108948\n",
            "Epoch: 23900 | Training Loss: 0.5360315442085266 | Testing Loss: 0.3946400284767151\n",
            "Epoch: 23910 | Training Loss: 0.5354711413383484 | Testing Loss: 0.3941994309425354\n",
            "Epoch: 23920 | Training Loss: 0.5349107980728149 | Testing Loss: 0.3937588334083557\n",
            "Epoch: 23930 | Training Loss: 0.5343503355979919 | Testing Loss: 0.393318235874176\n",
            "Epoch: 23940 | Training Loss: 0.5337899327278137 | Testing Loss: 0.39287763833999634\n",
            "Epoch: 23950 | Training Loss: 0.5332295298576355 | Testing Loss: 0.39243704080581665\n",
            "Epoch: 23960 | Training Loss: 0.532669186592102 | Testing Loss: 0.39199644327163696\n",
            "Epoch: 23970 | Training Loss: 0.532108724117279 | Testing Loss: 0.3915558457374573\n",
            "Epoch: 23980 | Training Loss: 0.5315483212471008 | Testing Loss: 0.3911152482032776\n",
            "Epoch: 23990 | Training Loss: 0.5309879183769226 | Testing Loss: 0.3906746506690979\n",
            "Epoch: 24000 | Training Loss: 0.5304275751113892 | Testing Loss: 0.3902340531349182\n",
            "Epoch: 24010 | Training Loss: 0.5298671126365662 | Testing Loss: 0.3897934556007385\n",
            "Epoch: 24020 | Training Loss: 0.5293067097663879 | Testing Loss: 0.38935285806655884\n",
            "Epoch: 24030 | Training Loss: 0.5287463068962097 | Testing Loss: 0.38891226053237915\n",
            "Epoch: 24040 | Training Loss: 0.5281859636306763 | Testing Loss: 0.38847166299819946\n",
            "Epoch: 24050 | Training Loss: 0.5276255011558533 | Testing Loss: 0.3880310654640198\n",
            "Epoch: 24060 | Training Loss: 0.527065098285675 | Testing Loss: 0.3875904679298401\n",
            "Epoch: 24070 | Training Loss: 0.5265046954154968 | Testing Loss: 0.3871498703956604\n",
            "Epoch: 24080 | Training Loss: 0.5259443521499634 | Testing Loss: 0.3867092728614807\n",
            "Epoch: 24090 | Training Loss: 0.5253838896751404 | Testing Loss: 0.386268675327301\n",
            "Epoch: 24100 | Training Loss: 0.5248234868049622 | Testing Loss: 0.38582807779312134\n",
            "Epoch: 24110 | Training Loss: 0.5242630839347839 | Testing Loss: 0.38538748025894165\n",
            "Epoch: 24120 | Training Loss: 0.5237027406692505 | Testing Loss: 0.38494688272476196\n",
            "Epoch: 24130 | Training Loss: 0.5231422781944275 | Testing Loss: 0.3845062851905823\n",
            "Epoch: 24140 | Training Loss: 0.5225818753242493 | Testing Loss: 0.3840656876564026\n",
            "Epoch: 24150 | Training Loss: 0.522021472454071 | Testing Loss: 0.3836250901222229\n",
            "Epoch: 24160 | Training Loss: 0.5214611291885376 | Testing Loss: 0.3831844925880432\n",
            "Epoch: 24170 | Training Loss: 0.5209006667137146 | Testing Loss: 0.3827438950538635\n",
            "Epoch: 24180 | Training Loss: 0.5203402638435364 | Testing Loss: 0.38230329751968384\n",
            "Epoch: 24190 | Training Loss: 0.5197798609733582 | Testing Loss: 0.38186269998550415\n",
            "Epoch: 24200 | Training Loss: 0.5192195177078247 | Testing Loss: 0.38142210245132446\n",
            "Epoch: 24210 | Training Loss: 0.5186590552330017 | Testing Loss: 0.3809815049171448\n",
            "Epoch: 24220 | Training Loss: 0.5180986523628235 | Testing Loss: 0.3805409073829651\n",
            "Epoch: 24230 | Training Loss: 0.5175382494926453 | Testing Loss: 0.3801003098487854\n",
            "Epoch: 24240 | Training Loss: 0.5169779062271118 | Testing Loss: 0.3796597123146057\n",
            "Epoch: 24250 | Training Loss: 0.5164174437522888 | Testing Loss: 0.379219114780426\n",
            "Epoch: 24260 | Training Loss: 0.5158570408821106 | Testing Loss: 0.37877851724624634\n",
            "Epoch: 24270 | Training Loss: 0.5152966380119324 | Testing Loss: 0.37833791971206665\n",
            "Epoch: 24280 | Training Loss: 0.5147362947463989 | Testing Loss: 0.37789732217788696\n",
            "Epoch: 24290 | Training Loss: 0.5141758322715759 | Testing Loss: 0.3774567246437073\n",
            "Epoch: 24300 | Training Loss: 0.5136154294013977 | Testing Loss: 0.3770161271095276\n",
            "Epoch: 24310 | Training Loss: 0.5130550265312195 | Testing Loss: 0.3765755295753479\n",
            "Epoch: 24320 | Training Loss: 0.512494683265686 | Testing Loss: 0.3761349320411682\n",
            "Epoch: 24330 | Training Loss: 0.511934220790863 | Testing Loss: 0.3756943345069885\n",
            "Epoch: 24340 | Training Loss: 0.5113738179206848 | Testing Loss: 0.37525373697280884\n",
            "Epoch: 24350 | Training Loss: 0.5108134150505066 | Testing Loss: 0.37481313943862915\n",
            "Epoch: 24360 | Training Loss: 0.5102530717849731 | Testing Loss: 0.37437254190444946\n",
            "Epoch: 24370 | Training Loss: 0.5096926093101501 | Testing Loss: 0.3739319443702698\n",
            "Epoch: 24380 | Training Loss: 0.5091322064399719 | Testing Loss: 0.3734913468360901\n",
            "Epoch: 24390 | Training Loss: 0.5085718035697937 | Testing Loss: 0.3730507493019104\n",
            "Epoch: 24400 | Training Loss: 0.5080114603042603 | Testing Loss: 0.3726101517677307\n",
            "Epoch: 24410 | Training Loss: 0.5074509978294373 | Testing Loss: 0.372169554233551\n",
            "Epoch: 24420 | Training Loss: 0.506890594959259 | Testing Loss: 0.37172895669937134\n",
            "Epoch: 24430 | Training Loss: 0.5063301920890808 | Testing Loss: 0.37128835916519165\n",
            "Epoch: 24440 | Training Loss: 0.5057698488235474 | Testing Loss: 0.37084776163101196\n",
            "Epoch: 24450 | Training Loss: 0.5052093863487244 | Testing Loss: 0.3704071640968323\n",
            "Epoch: 24460 | Training Loss: 0.5046489834785461 | Testing Loss: 0.3699665665626526\n",
            "Epoch: 24470 | Training Loss: 0.5040885806083679 | Testing Loss: 0.3695259690284729\n",
            "Epoch: 24480 | Training Loss: 0.5035282373428345 | Testing Loss: 0.3690853714942932\n",
            "Epoch: 24490 | Training Loss: 0.5029677748680115 | Testing Loss: 0.3686447739601135\n",
            "Epoch: 24500 | Training Loss: 0.5024073719978333 | Testing Loss: 0.36820417642593384\n",
            "Epoch: 24510 | Training Loss: 0.501846969127655 | Testing Loss: 0.36776357889175415\n",
            "Epoch: 24520 | Training Loss: 0.5012866258621216 | Testing Loss: 0.36732298135757446\n",
            "Epoch: 24530 | Training Loss: 0.5007261633872986 | Testing Loss: 0.3668823838233948\n",
            "Epoch: 24540 | Training Loss: 0.5001657605171204 | Testing Loss: 0.3664417862892151\n",
            "Epoch: 24550 | Training Loss: 0.4996053874492645 | Testing Loss: 0.3660011887550354\n",
            "Epoch: 24560 | Training Loss: 0.4990449845790863 | Testing Loss: 0.3655605912208557\n",
            "Epoch: 24570 | Training Loss: 0.4984845221042633 | Testing Loss: 0.365119993686676\n",
            "Epoch: 24580 | Training Loss: 0.49792414903640747 | Testing Loss: 0.36467939615249634\n",
            "Epoch: 24590 | Training Loss: 0.49736377596855164 | Testing Loss: 0.36423879861831665\n",
            "Epoch: 24600 | Training Loss: 0.4968033730983734 | Testing Loss: 0.36379820108413696\n",
            "Epoch: 24610 | Training Loss: 0.4962429106235504 | Testing Loss: 0.3633576035499573\n",
            "Epoch: 24620 | Training Loss: 0.4956825375556946 | Testing Loss: 0.3629170060157776\n",
            "Epoch: 24630 | Training Loss: 0.49512216448783875 | Testing Loss: 0.3624764084815979\n",
            "Epoch: 24640 | Training Loss: 0.4945617616176605 | Testing Loss: 0.3620358109474182\n",
            "Epoch: 24650 | Training Loss: 0.4940012991428375 | Testing Loss: 0.3615952134132385\n",
            "Epoch: 24660 | Training Loss: 0.4934409260749817 | Testing Loss: 0.36115461587905884\n",
            "Epoch: 24670 | Training Loss: 0.49288055300712585 | Testing Loss: 0.36071401834487915\n",
            "Epoch: 24680 | Training Loss: 0.49232015013694763 | Testing Loss: 0.36027342081069946\n",
            "Epoch: 24690 | Training Loss: 0.49175968766212463 | Testing Loss: 0.3598328232765198\n",
            "Epoch: 24700 | Training Loss: 0.4911993145942688 | Testing Loss: 0.3593922257423401\n",
            "Epoch: 24710 | Training Loss: 0.49063894152641296 | Testing Loss: 0.3589516282081604\n",
            "Epoch: 24720 | Training Loss: 0.49007853865623474 | Testing Loss: 0.3585110306739807\n",
            "Epoch: 24730 | Training Loss: 0.48951807618141174 | Testing Loss: 0.358070433139801\n",
            "Epoch: 24740 | Training Loss: 0.4889577031135559 | Testing Loss: 0.35762983560562134\n",
            "Epoch: 24750 | Training Loss: 0.4883973300457001 | Testing Loss: 0.35718923807144165\n",
            "Epoch: 24760 | Training Loss: 0.48783692717552185 | Testing Loss: 0.35674864053726196\n",
            "Epoch: 24770 | Training Loss: 0.48727646470069885 | Testing Loss: 0.3563080430030823\n",
            "Epoch: 24780 | Training Loss: 0.486716091632843 | Testing Loss: 0.3558674454689026\n",
            "Epoch: 24790 | Training Loss: 0.4861557185649872 | Testing Loss: 0.3554268479347229\n",
            "Epoch: 24800 | Training Loss: 0.48559531569480896 | Testing Loss: 0.3549862504005432\n",
            "Epoch: 24810 | Training Loss: 0.48503485321998596 | Testing Loss: 0.3545456528663635\n",
            "Epoch: 24820 | Training Loss: 0.4844744801521301 | Testing Loss: 0.35410505533218384\n",
            "Epoch: 24830 | Training Loss: 0.4839141070842743 | Testing Loss: 0.35366445779800415\n",
            "Epoch: 24840 | Training Loss: 0.48335370421409607 | Testing Loss: 0.35322386026382446\n",
            "Epoch: 24850 | Training Loss: 0.48279324173927307 | Testing Loss: 0.3527832627296448\n",
            "Epoch: 24860 | Training Loss: 0.48223286867141724 | Testing Loss: 0.3523426651954651\n",
            "Epoch: 24870 | Training Loss: 0.4816724956035614 | Testing Loss: 0.3519020676612854\n",
            "Epoch: 24880 | Training Loss: 0.4811120927333832 | Testing Loss: 0.3514614701271057\n",
            "Epoch: 24890 | Training Loss: 0.4805516302585602 | Testing Loss: 0.351020872592926\n",
            "Epoch: 24900 | Training Loss: 0.47999125719070435 | Testing Loss: 0.35058027505874634\n",
            "Epoch: 24910 | Training Loss: 0.4794308841228485 | Testing Loss: 0.35013967752456665\n",
            "Epoch: 24920 | Training Loss: 0.4788704812526703 | Testing Loss: 0.34969907999038696\n",
            "Epoch: 24930 | Training Loss: 0.4783100187778473 | Testing Loss: 0.3492584824562073\n",
            "Epoch: 24940 | Training Loss: 0.47774964570999146 | Testing Loss: 0.3488178849220276\n",
            "Epoch: 24950 | Training Loss: 0.4771892726421356 | Testing Loss: 0.3483772873878479\n",
            "Epoch: 24960 | Training Loss: 0.4766288697719574 | Testing Loss: 0.3479366898536682\n",
            "Epoch: 24970 | Training Loss: 0.4760684072971344 | Testing Loss: 0.3474960923194885\n",
            "Epoch: 24980 | Training Loss: 0.47550803422927856 | Testing Loss: 0.34705549478530884\n",
            "Epoch: 24990 | Training Loss: 0.47494766116142273 | Testing Loss: 0.34661489725112915\n",
            "Epoch: 25000 | Training Loss: 0.4743872582912445 | Testing Loss: 0.34617429971694946\n",
            "Epoch: 25010 | Training Loss: 0.4738267958164215 | Testing Loss: 0.3457337021827698\n",
            "Epoch: 25020 | Training Loss: 0.4732664227485657 | Testing Loss: 0.3452931046485901\n",
            "Epoch: 25030 | Training Loss: 0.47270604968070984 | Testing Loss: 0.3448525071144104\n",
            "Epoch: 25040 | Training Loss: 0.4721456468105316 | Testing Loss: 0.3444119095802307\n",
            "Epoch: 25050 | Training Loss: 0.4715851843357086 | Testing Loss: 0.343971312046051\n",
            "Epoch: 25060 | Training Loss: 0.4710248112678528 | Testing Loss: 0.34353071451187134\n",
            "Epoch: 25070 | Training Loss: 0.47046443819999695 | Testing Loss: 0.34309011697769165\n",
            "Epoch: 25080 | Training Loss: 0.4699040353298187 | Testing Loss: 0.34264951944351196\n",
            "Epoch: 25090 | Training Loss: 0.4693435728549957 | Testing Loss: 0.3422089219093323\n",
            "Epoch: 25100 | Training Loss: 0.4687831997871399 | Testing Loss: 0.3417683243751526\n",
            "Epoch: 25110 | Training Loss: 0.46822282671928406 | Testing Loss: 0.3413277268409729\n",
            "Epoch: 25120 | Training Loss: 0.46766242384910583 | Testing Loss: 0.3408871293067932\n",
            "Epoch: 25130 | Training Loss: 0.46710196137428284 | Testing Loss: 0.3404465317726135\n",
            "Epoch: 25140 | Training Loss: 0.466541588306427 | Testing Loss: 0.34000593423843384\n",
            "Epoch: 25150 | Training Loss: 0.46598121523857117 | Testing Loss: 0.33956533670425415\n",
            "Epoch: 25160 | Training Loss: 0.46542081236839294 | Testing Loss: 0.33912473917007446\n",
            "Epoch: 25170 | Training Loss: 0.46486034989356995 | Testing Loss: 0.3386841416358948\n",
            "Epoch: 25180 | Training Loss: 0.4642999768257141 | Testing Loss: 0.3382435441017151\n",
            "Epoch: 25190 | Training Loss: 0.4637396037578583 | Testing Loss: 0.3378029465675354\n",
            "Epoch: 25200 | Training Loss: 0.46317920088768005 | Testing Loss: 0.3373623490333557\n",
            "Epoch: 25210 | Training Loss: 0.46261873841285706 | Testing Loss: 0.336921751499176\n",
            "Epoch: 25220 | Training Loss: 0.4620583653450012 | Testing Loss: 0.33648115396499634\n",
            "Epoch: 25230 | Training Loss: 0.4614979922771454 | Testing Loss: 0.33604055643081665\n",
            "Epoch: 25240 | Training Loss: 0.46093758940696716 | Testing Loss: 0.33559995889663696\n",
            "Epoch: 25250 | Training Loss: 0.46037712693214417 | Testing Loss: 0.3351593613624573\n",
            "Epoch: 25260 | Training Loss: 0.45981675386428833 | Testing Loss: 0.3347187638282776\n",
            "Epoch: 25270 | Training Loss: 0.4592563807964325 | Testing Loss: 0.3342781662940979\n",
            "Epoch: 25280 | Training Loss: 0.4586959779262543 | Testing Loss: 0.3338375687599182\n",
            "Epoch: 25290 | Training Loss: 0.4581355154514313 | Testing Loss: 0.3333969712257385\n",
            "Epoch: 25300 | Training Loss: 0.45757514238357544 | Testing Loss: 0.33295637369155884\n",
            "Epoch: 25310 | Training Loss: 0.4570147693157196 | Testing Loss: 0.33251577615737915\n",
            "Epoch: 25320 | Training Loss: 0.4564543664455414 | Testing Loss: 0.33207517862319946\n",
            "Epoch: 25330 | Training Loss: 0.4558939039707184 | Testing Loss: 0.3316345810890198\n",
            "Epoch: 25340 | Training Loss: 0.45533353090286255 | Testing Loss: 0.3311939835548401\n",
            "Epoch: 25350 | Training Loss: 0.4547731578350067 | Testing Loss: 0.3307533860206604\n",
            "Epoch: 25360 | Training Loss: 0.4542127549648285 | Testing Loss: 0.3303127884864807\n",
            "Epoch: 25370 | Training Loss: 0.4536522924900055 | Testing Loss: 0.329872190952301\n",
            "Epoch: 25380 | Training Loss: 0.45309191942214966 | Testing Loss: 0.32943159341812134\n",
            "Epoch: 25390 | Training Loss: 0.4525315463542938 | Testing Loss: 0.32899099588394165\n",
            "Epoch: 25400 | Training Loss: 0.4519711434841156 | Testing Loss: 0.32855039834976196\n",
            "Epoch: 25410 | Training Loss: 0.4514106810092926 | Testing Loss: 0.3281098008155823\n",
            "Epoch: 25420 | Training Loss: 0.45085030794143677 | Testing Loss: 0.3276692032814026\n",
            "Epoch: 25430 | Training Loss: 0.45028993487358093 | Testing Loss: 0.3272286057472229\n",
            "Epoch: 25440 | Training Loss: 0.4497295320034027 | Testing Loss: 0.3267880082130432\n",
            "Epoch: 25450 | Training Loss: 0.4491690695285797 | Testing Loss: 0.3263474106788635\n",
            "Epoch: 25460 | Training Loss: 0.4486086964607239 | Testing Loss: 0.32590681314468384\n",
            "Epoch: 25470 | Training Loss: 0.44804832339286804 | Testing Loss: 0.32546621561050415\n",
            "Epoch: 25480 | Training Loss: 0.4474879205226898 | Testing Loss: 0.32502561807632446\n",
            "Epoch: 25490 | Training Loss: 0.4469274580478668 | Testing Loss: 0.3245850205421448\n",
            "Epoch: 25500 | Training Loss: 0.446367084980011 | Testing Loss: 0.3241444230079651\n",
            "Epoch: 25510 | Training Loss: 0.44580671191215515 | Testing Loss: 0.3237038254737854\n",
            "Epoch: 25520 | Training Loss: 0.44524630904197693 | Testing Loss: 0.3232632279396057\n",
            "Epoch: 25530 | Training Loss: 0.44468584656715393 | Testing Loss: 0.322822630405426\n",
            "Epoch: 25540 | Training Loss: 0.4441254734992981 | Testing Loss: 0.32238203287124634\n",
            "Epoch: 25550 | Training Loss: 0.44356510043144226 | Testing Loss: 0.3219418227672577\n",
            "Epoch: 25560 | Training Loss: 0.44300469756126404 | Testing Loss: 0.3215155303478241\n",
            "Epoch: 25570 | Training Loss: 0.44244423508644104 | Testing Loss: 0.3210892379283905\n",
            "Epoch: 25580 | Training Loss: 0.4418838620185852 | Testing Loss: 0.3206629455089569\n",
            "Epoch: 25590 | Training Loss: 0.44132348895072937 | Testing Loss: 0.3202366530895233\n",
            "Epoch: 25600 | Training Loss: 0.44076308608055115 | Testing Loss: 0.3198103606700897\n",
            "Epoch: 25610 | Training Loss: 0.44020262360572815 | Testing Loss: 0.31938406825065613\n",
            "Epoch: 25620 | Training Loss: 0.4396422505378723 | Testing Loss: 0.31895777583122253\n",
            "Epoch: 25630 | Training Loss: 0.4390818774700165 | Testing Loss: 0.31853148341178894\n",
            "Epoch: 25640 | Training Loss: 0.43852147459983826 | Testing Loss: 0.31810519099235535\n",
            "Epoch: 25650 | Training Loss: 0.43796101212501526 | Testing Loss: 0.31767889857292175\n",
            "Epoch: 25660 | Training Loss: 0.4374006390571594 | Testing Loss: 0.31725260615348816\n",
            "Epoch: 25670 | Training Loss: 0.4368402659893036 | Testing Loss: 0.31682631373405457\n",
            "Epoch: 25680 | Training Loss: 0.43627986311912537 | Testing Loss: 0.31640002131462097\n",
            "Epoch: 25690 | Training Loss: 0.43571940064430237 | Testing Loss: 0.3159737288951874\n",
            "Epoch: 25700 | Training Loss: 0.43515902757644653 | Testing Loss: 0.3155474364757538\n",
            "Epoch: 25710 | Training Loss: 0.4345986545085907 | Testing Loss: 0.3151211440563202\n",
            "Epoch: 25720 | Training Loss: 0.4340382516384125 | Testing Loss: 0.3146948516368866\n",
            "Epoch: 25730 | Training Loss: 0.4334777891635895 | Testing Loss: 0.314268559217453\n",
            "Epoch: 25740 | Training Loss: 0.43291741609573364 | Testing Loss: 0.3138422667980194\n",
            "Epoch: 25750 | Training Loss: 0.4323570430278778 | Testing Loss: 0.3134159743785858\n",
            "Epoch: 25760 | Training Loss: 0.4317966401576996 | Testing Loss: 0.3129896819591522\n",
            "Epoch: 25770 | Training Loss: 0.4312361776828766 | Testing Loss: 0.31256338953971863\n",
            "Epoch: 25780 | Training Loss: 0.43067580461502075 | Testing Loss: 0.31213709712028503\n",
            "Epoch: 25790 | Training Loss: 0.4301154315471649 | Testing Loss: 0.31171080470085144\n",
            "Epoch: 25800 | Training Loss: 0.4295550286769867 | Testing Loss: 0.31128451228141785\n",
            "Epoch: 25810 | Training Loss: 0.4289945662021637 | Testing Loss: 0.31085821986198425\n",
            "Epoch: 25820 | Training Loss: 0.42843419313430786 | Testing Loss: 0.31043192744255066\n",
            "Epoch: 25830 | Training Loss: 0.427873820066452 | Testing Loss: 0.31000563502311707\n",
            "Epoch: 25840 | Training Loss: 0.4273134171962738 | Testing Loss: 0.30957934260368347\n",
            "Epoch: 25850 | Training Loss: 0.4267529547214508 | Testing Loss: 0.3091530501842499\n",
            "Epoch: 25860 | Training Loss: 0.42619258165359497 | Testing Loss: 0.3087267577648163\n",
            "Epoch: 25870 | Training Loss: 0.42563220858573914 | Testing Loss: 0.3083004653453827\n",
            "Epoch: 25880 | Training Loss: 0.4250718057155609 | Testing Loss: 0.3078741729259491\n",
            "Epoch: 25890 | Training Loss: 0.4245113432407379 | Testing Loss: 0.3074478805065155\n",
            "Epoch: 25900 | Training Loss: 0.4239509701728821 | Testing Loss: 0.3070215880870819\n",
            "Epoch: 25910 | Training Loss: 0.42339059710502625 | Testing Loss: 0.3065952956676483\n",
            "Epoch: 25920 | Training Loss: 0.422830194234848 | Testing Loss: 0.3061690032482147\n",
            "Epoch: 25930 | Training Loss: 0.422269731760025 | Testing Loss: 0.30574271082878113\n",
            "Epoch: 25940 | Training Loss: 0.4217093586921692 | Testing Loss: 0.30531641840934753\n",
            "Epoch: 25950 | Training Loss: 0.42114898562431335 | Testing Loss: 0.30489012598991394\n",
            "Epoch: 25960 | Training Loss: 0.42058858275413513 | Testing Loss: 0.30446383357048035\n",
            "Epoch: 25970 | Training Loss: 0.42002812027931213 | Testing Loss: 0.30403754115104675\n",
            "Epoch: 25980 | Training Loss: 0.4194677472114563 | Testing Loss: 0.30361124873161316\n",
            "Epoch: 25990 | Training Loss: 0.41890737414360046 | Testing Loss: 0.30318495631217957\n",
            "Epoch: 26000 | Training Loss: 0.41834697127342224 | Testing Loss: 0.30275866389274597\n",
            "Epoch: 26010 | Training Loss: 0.41778650879859924 | Testing Loss: 0.3023323714733124\n",
            "Epoch: 26020 | Training Loss: 0.4172261357307434 | Testing Loss: 0.3019060790538788\n",
            "Epoch: 26030 | Training Loss: 0.4166657626628876 | Testing Loss: 0.3014797866344452\n",
            "Epoch: 26040 | Training Loss: 0.41610535979270935 | Testing Loss: 0.3010534942150116\n",
            "Epoch: 26050 | Training Loss: 0.41554489731788635 | Testing Loss: 0.300627201795578\n",
            "Epoch: 26060 | Training Loss: 0.4149845242500305 | Testing Loss: 0.3002009093761444\n",
            "Epoch: 26070 | Training Loss: 0.4144241511821747 | Testing Loss: 0.2997746169567108\n",
            "Epoch: 26080 | Training Loss: 0.41386374831199646 | Testing Loss: 0.2993483245372772\n",
            "Epoch: 26090 | Training Loss: 0.41330328583717346 | Testing Loss: 0.29892203211784363\n",
            "Epoch: 26100 | Training Loss: 0.4127429127693176 | Testing Loss: 0.29849573969841003\n",
            "Epoch: 26110 | Training Loss: 0.4121825397014618 | Testing Loss: 0.29806944727897644\n",
            "Epoch: 26120 | Training Loss: 0.41162213683128357 | Testing Loss: 0.29764315485954285\n",
            "Epoch: 26130 | Training Loss: 0.41106167435646057 | Testing Loss: 0.29721686244010925\n",
            "Epoch: 26140 | Training Loss: 0.41050130128860474 | Testing Loss: 0.29679057002067566\n",
            "Epoch: 26150 | Training Loss: 0.4099409282207489 | Testing Loss: 0.29636427760124207\n",
            "Epoch: 26160 | Training Loss: 0.4093805253505707 | Testing Loss: 0.29593798518180847\n",
            "Epoch: 26170 | Training Loss: 0.4088200628757477 | Testing Loss: 0.2955116927623749\n",
            "Epoch: 26180 | Training Loss: 0.40825968980789185 | Testing Loss: 0.2950854003429413\n",
            "Epoch: 26190 | Training Loss: 0.407699316740036 | Testing Loss: 0.2946591079235077\n",
            "Epoch: 26200 | Training Loss: 0.4071389138698578 | Testing Loss: 0.2942328155040741\n",
            "Epoch: 26210 | Training Loss: 0.4065784513950348 | Testing Loss: 0.2938065230846405\n",
            "Epoch: 26220 | Training Loss: 0.40601807832717896 | Testing Loss: 0.2933802306652069\n",
            "Epoch: 26230 | Training Loss: 0.4054577052593231 | Testing Loss: 0.2929539382457733\n",
            "Epoch: 26240 | Training Loss: 0.4048973023891449 | Testing Loss: 0.2925276458263397\n",
            "Epoch: 26250 | Training Loss: 0.4043368399143219 | Testing Loss: 0.29210135340690613\n",
            "Epoch: 26260 | Training Loss: 0.40377646684646606 | Testing Loss: 0.29167506098747253\n",
            "Epoch: 26270 | Training Loss: 0.40321609377861023 | Testing Loss: 0.29124876856803894\n",
            "Epoch: 26280 | Training Loss: 0.402655690908432 | Testing Loss: 0.29082247614860535\n",
            "Epoch: 26290 | Training Loss: 0.402095228433609 | Testing Loss: 0.29039618372917175\n",
            "Epoch: 26300 | Training Loss: 0.4015348553657532 | Testing Loss: 0.28996989130973816\n",
            "Epoch: 26310 | Training Loss: 0.40097448229789734 | Testing Loss: 0.28954359889030457\n",
            "Epoch: 26320 | Training Loss: 0.4004140794277191 | Testing Loss: 0.28911730647087097\n",
            "Epoch: 26330 | Training Loss: 0.3998536467552185 | Testing Loss: 0.2886910140514374\n",
            "Epoch: 26340 | Training Loss: 0.39929327368736267 | Testing Loss: 0.2882647216320038\n",
            "Epoch: 26350 | Training Loss: 0.39873284101486206 | Testing Loss: 0.2878384292125702\n",
            "Epoch: 26360 | Training Loss: 0.3981724679470062 | Testing Loss: 0.2874121367931366\n",
            "Epoch: 26370 | Training Loss: 0.3976120352745056 | Testing Loss: 0.286985844373703\n",
            "Epoch: 26380 | Training Loss: 0.3970516622066498 | Testing Loss: 0.2865595519542694\n",
            "Epoch: 26390 | Training Loss: 0.39649122953414917 | Testing Loss: 0.2861332595348358\n",
            "Epoch: 26400 | Training Loss: 0.39593085646629333 | Testing Loss: 0.2857069671154022\n",
            "Epoch: 26410 | Training Loss: 0.3953704237937927 | Testing Loss: 0.28528067469596863\n",
            "Epoch: 26420 | Training Loss: 0.3948100507259369 | Testing Loss: 0.28485438227653503\n",
            "Epoch: 26430 | Training Loss: 0.3942496180534363 | Testing Loss: 0.28442808985710144\n",
            "Epoch: 26440 | Training Loss: 0.39368924498558044 | Testing Loss: 0.28400179743766785\n",
            "Epoch: 26450 | Training Loss: 0.39312881231307983 | Testing Loss: 0.28357550501823425\n",
            "Epoch: 26460 | Training Loss: 0.392568439245224 | Testing Loss: 0.28314921259880066\n",
            "Epoch: 26470 | Training Loss: 0.3920080065727234 | Testing Loss: 0.28272292017936707\n",
            "Epoch: 26480 | Training Loss: 0.39144763350486755 | Testing Loss: 0.28229662775993347\n",
            "Epoch: 26490 | Training Loss: 0.39088720083236694 | Testing Loss: 0.2818703353404999\n",
            "Epoch: 26500 | Training Loss: 0.3903268277645111 | Testing Loss: 0.2814440429210663\n",
            "Epoch: 26510 | Training Loss: 0.3897663950920105 | Testing Loss: 0.2810177505016327\n",
            "Epoch: 26520 | Training Loss: 0.38920602202415466 | Testing Loss: 0.2805914580821991\n",
            "Epoch: 26530 | Training Loss: 0.38864558935165405 | Testing Loss: 0.2801651656627655\n",
            "Epoch: 26540 | Training Loss: 0.3880852162837982 | Testing Loss: 0.2797388732433319\n",
            "Epoch: 26550 | Training Loss: 0.3875247836112976 | Testing Loss: 0.2793125808238983\n",
            "Epoch: 26560 | Training Loss: 0.3869644105434418 | Testing Loss: 0.2788862884044647\n",
            "Epoch: 26570 | Training Loss: 0.38640397787094116 | Testing Loss: 0.27845999598503113\n",
            "Epoch: 26580 | Training Loss: 0.3858436048030853 | Testing Loss: 0.27803370356559753\n",
            "Epoch: 26590 | Training Loss: 0.3852831721305847 | Testing Loss: 0.27760741114616394\n",
            "Epoch: 26600 | Training Loss: 0.3847227990627289 | Testing Loss: 0.27718111872673035\n",
            "Epoch: 26610 | Training Loss: 0.38416236639022827 | Testing Loss: 0.27675482630729675\n",
            "Epoch: 26620 | Training Loss: 0.38360199332237244 | Testing Loss: 0.27632853388786316\n",
            "Epoch: 26630 | Training Loss: 0.3830415606498718 | Testing Loss: 0.27590224146842957\n",
            "Epoch: 26640 | Training Loss: 0.382481187582016 | Testing Loss: 0.27547594904899597\n",
            "Epoch: 26650 | Training Loss: 0.3819207549095154 | Testing Loss: 0.2750496566295624\n",
            "Epoch: 26660 | Training Loss: 0.38136038184165955 | Testing Loss: 0.2746233642101288\n",
            "Epoch: 26670 | Training Loss: 0.38079994916915894 | Testing Loss: 0.2741970717906952\n",
            "Epoch: 26680 | Training Loss: 0.3802395761013031 | Testing Loss: 0.2737707793712616\n",
            "Epoch: 26690 | Training Loss: 0.3796791434288025 | Testing Loss: 0.273344486951828\n",
            "Epoch: 26700 | Training Loss: 0.37911877036094666 | Testing Loss: 0.2729181945323944\n",
            "Epoch: 26710 | Training Loss: 0.37855833768844604 | Testing Loss: 0.2724919021129608\n",
            "Epoch: 26720 | Training Loss: 0.3779979646205902 | Testing Loss: 0.2720656096935272\n",
            "Epoch: 26730 | Training Loss: 0.3774375319480896 | Testing Loss: 0.27163931727409363\n",
            "Epoch: 26740 | Training Loss: 0.37687715888023376 | Testing Loss: 0.27121302485466003\n",
            "Epoch: 26750 | Training Loss: 0.37631672620773315 | Testing Loss: 0.27078673243522644\n",
            "Epoch: 26760 | Training Loss: 0.3757563531398773 | Testing Loss: 0.27036044001579285\n",
            "Epoch: 26770 | Training Loss: 0.3751959204673767 | Testing Loss: 0.26993414759635925\n",
            "Epoch: 26780 | Training Loss: 0.3746355473995209 | Testing Loss: 0.26950785517692566\n",
            "Epoch: 26790 | Training Loss: 0.37407511472702026 | Testing Loss: 0.26908156275749207\n",
            "Epoch: 26800 | Training Loss: 0.37351474165916443 | Testing Loss: 0.26865527033805847\n",
            "Epoch: 26810 | Training Loss: 0.3729543089866638 | Testing Loss: 0.2682289779186249\n",
            "Epoch: 26820 | Training Loss: 0.372393935918808 | Testing Loss: 0.2678026854991913\n",
            "Epoch: 26830 | Training Loss: 0.3718335032463074 | Testing Loss: 0.2673763930797577\n",
            "Epoch: 26840 | Training Loss: 0.37127313017845154 | Testing Loss: 0.2669501006603241\n",
            "Epoch: 26850 | Training Loss: 0.3707126975059509 | Testing Loss: 0.2665238082408905\n",
            "Epoch: 26860 | Training Loss: 0.3701523244380951 | Testing Loss: 0.2660975158214569\n",
            "Epoch: 26870 | Training Loss: 0.3695918917655945 | Testing Loss: 0.2656712234020233\n",
            "Epoch: 26880 | Training Loss: 0.36903151869773865 | Testing Loss: 0.2652449309825897\n",
            "Epoch: 26890 | Training Loss: 0.36847108602523804 | Testing Loss: 0.26481863856315613\n",
            "Epoch: 26900 | Training Loss: 0.3679107129573822 | Testing Loss: 0.26439234614372253\n",
            "Epoch: 26910 | Training Loss: 0.3673502802848816 | Testing Loss: 0.26396605372428894\n",
            "Epoch: 26920 | Training Loss: 0.36678990721702576 | Testing Loss: 0.26353976130485535\n",
            "Epoch: 26930 | Training Loss: 0.36622947454452515 | Testing Loss: 0.26311346888542175\n",
            "Epoch: 26940 | Training Loss: 0.3656691014766693 | Testing Loss: 0.26268717646598816\n",
            "Epoch: 26950 | Training Loss: 0.3651086688041687 | Testing Loss: 0.26226088404655457\n",
            "Epoch: 26960 | Training Loss: 0.36454829573631287 | Testing Loss: 0.26183459162712097\n",
            "Epoch: 26970 | Training Loss: 0.36398786306381226 | Testing Loss: 0.2614082992076874\n",
            "Epoch: 26980 | Training Loss: 0.3634274899959564 | Testing Loss: 0.2609820067882538\n",
            "Epoch: 26990 | Training Loss: 0.3628670573234558 | Testing Loss: 0.2605557143688202\n",
            "Epoch: 27000 | Training Loss: 0.3623066842556 | Testing Loss: 0.2601294219493866\n",
            "Epoch: 27010 | Training Loss: 0.36174625158309937 | Testing Loss: 0.259703129529953\n",
            "Epoch: 27020 | Training Loss: 0.36118587851524353 | Testing Loss: 0.2592768371105194\n",
            "Epoch: 27030 | Training Loss: 0.3606254458427429 | Testing Loss: 0.2588505446910858\n",
            "Epoch: 27040 | Training Loss: 0.3600650727748871 | Testing Loss: 0.2584242522716522\n",
            "Epoch: 27050 | Training Loss: 0.3595046401023865 | Testing Loss: 0.25799795985221863\n",
            "Epoch: 27060 | Training Loss: 0.35894426703453064 | Testing Loss: 0.25757166743278503\n",
            "Epoch: 27070 | Training Loss: 0.35838383436203003 | Testing Loss: 0.25714537501335144\n",
            "Epoch: 27080 | Training Loss: 0.3578234612941742 | Testing Loss: 0.25671908259391785\n",
            "Epoch: 27090 | Training Loss: 0.3572630286216736 | Testing Loss: 0.25629279017448425\n",
            "Epoch: 27100 | Training Loss: 0.35670265555381775 | Testing Loss: 0.25586649775505066\n",
            "Epoch: 27110 | Training Loss: 0.35614222288131714 | Testing Loss: 0.25544020533561707\n",
            "Epoch: 27120 | Training Loss: 0.3555818498134613 | Testing Loss: 0.25501391291618347\n",
            "Epoch: 27130 | Training Loss: 0.3550214171409607 | Testing Loss: 0.2545876204967499\n",
            "Epoch: 27140 | Training Loss: 0.35446104407310486 | Testing Loss: 0.2541613280773163\n",
            "Epoch: 27150 | Training Loss: 0.35390061140060425 | Testing Loss: 0.2537350356578827\n",
            "Epoch: 27160 | Training Loss: 0.3533402383327484 | Testing Loss: 0.2533087432384491\n",
            "Epoch: 27170 | Training Loss: 0.3527798056602478 | Testing Loss: 0.2528824508190155\n",
            "Epoch: 27180 | Training Loss: 0.35221943259239197 | Testing Loss: 0.2524561583995819\n",
            "Epoch: 27190 | Training Loss: 0.35165899991989136 | Testing Loss: 0.2520298659801483\n",
            "Epoch: 27200 | Training Loss: 0.3510986268520355 | Testing Loss: 0.2516035735607147\n",
            "Epoch: 27210 | Training Loss: 0.3505381941795349 | Testing Loss: 0.25117728114128113\n",
            "Epoch: 27220 | Training Loss: 0.3499778211116791 | Testing Loss: 0.25075098872184753\n",
            "Epoch: 27230 | Training Loss: 0.34941738843917847 | Testing Loss: 0.25032469630241394\n",
            "Epoch: 27240 | Training Loss: 0.34885701537132263 | Testing Loss: 0.24989838898181915\n",
            "Epoch: 27250 | Training Loss: 0.348296582698822 | Testing Loss: 0.24947209656238556\n",
            "Epoch: 27260 | Training Loss: 0.3477362096309662 | Testing Loss: 0.24904580414295197\n",
            "Epoch: 27270 | Training Loss: 0.3471757769584656 | Testing Loss: 0.24861951172351837\n",
            "Epoch: 27280 | Training Loss: 0.34661540389060974 | Testing Loss: 0.24819321930408478\n",
            "Epoch: 27290 | Training Loss: 0.34605497121810913 | Testing Loss: 0.24776692688465118\n",
            "Epoch: 27300 | Training Loss: 0.3454945981502533 | Testing Loss: 0.2473406344652176\n",
            "Epoch: 27310 | Training Loss: 0.3449341654777527 | Testing Loss: 0.246914342045784\n",
            "Epoch: 27320 | Training Loss: 0.34437379240989685 | Testing Loss: 0.2464880496263504\n",
            "Epoch: 27330 | Training Loss: 0.34381335973739624 | Testing Loss: 0.2460617572069168\n",
            "Epoch: 27340 | Training Loss: 0.3432529866695404 | Testing Loss: 0.24563546478748322\n",
            "Epoch: 27350 | Training Loss: 0.3426925539970398 | Testing Loss: 0.24520917236804962\n",
            "Epoch: 27360 | Training Loss: 0.34213218092918396 | Testing Loss: 0.24478287994861603\n",
            "Epoch: 27370 | Training Loss: 0.34157174825668335 | Testing Loss: 0.24435658752918243\n",
            "Epoch: 27380 | Training Loss: 0.3410113751888275 | Testing Loss: 0.24393029510974884\n",
            "Epoch: 27390 | Training Loss: 0.3404509425163269 | Testing Loss: 0.24350400269031525\n",
            "Epoch: 27400 | Training Loss: 0.33989056944847107 | Testing Loss: 0.24307771027088165\n",
            "Epoch: 27410 | Training Loss: 0.33933013677597046 | Testing Loss: 0.24265141785144806\n",
            "Epoch: 27420 | Training Loss: 0.3387697637081146 | Testing Loss: 0.24222512543201447\n",
            "Epoch: 27430 | Training Loss: 0.338209331035614 | Testing Loss: 0.24179883301258087\n",
            "Epoch: 27440 | Training Loss: 0.3376489579677582 | Testing Loss: 0.24137254059314728\n",
            "Epoch: 27450 | Training Loss: 0.33708852529525757 | Testing Loss: 0.24094624817371368\n",
            "Epoch: 27460 | Training Loss: 0.33652815222740173 | Testing Loss: 0.2405199557542801\n",
            "Epoch: 27470 | Training Loss: 0.3359677195549011 | Testing Loss: 0.2400936633348465\n",
            "Epoch: 27480 | Training Loss: 0.3354073464870453 | Testing Loss: 0.2396673709154129\n",
            "Epoch: 27490 | Training Loss: 0.3348469138145447 | Testing Loss: 0.2392410784959793\n",
            "Epoch: 27500 | Training Loss: 0.33428654074668884 | Testing Loss: 0.23881478607654572\n",
            "Epoch: 27510 | Training Loss: 0.33372610807418823 | Testing Loss: 0.23838849365711212\n",
            "Epoch: 27520 | Training Loss: 0.3331657350063324 | Testing Loss: 0.23796220123767853\n",
            "Epoch: 27530 | Training Loss: 0.3326053023338318 | Testing Loss: 0.23753590881824493\n",
            "Epoch: 27540 | Training Loss: 0.33204492926597595 | Testing Loss: 0.23710961639881134\n",
            "Epoch: 27550 | Training Loss: 0.33148449659347534 | Testing Loss: 0.23668332397937775\n",
            "Epoch: 27560 | Training Loss: 0.3309241235256195 | Testing Loss: 0.23625703155994415\n",
            "Epoch: 27570 | Training Loss: 0.3303636908531189 | Testing Loss: 0.23583073914051056\n",
            "Epoch: 27580 | Training Loss: 0.32980331778526306 | Testing Loss: 0.23540444672107697\n",
            "Epoch: 27590 | Training Loss: 0.32924288511276245 | Testing Loss: 0.23497815430164337\n",
            "Epoch: 27600 | Training Loss: 0.3286825120449066 | Testing Loss: 0.23455186188220978\n",
            "Epoch: 27610 | Training Loss: 0.328122079372406 | Testing Loss: 0.23412556946277618\n",
            "Epoch: 27620 | Training Loss: 0.32756170630455017 | Testing Loss: 0.2336992770433426\n",
            "Epoch: 27630 | Training Loss: 0.32700127363204956 | Testing Loss: 0.233272984623909\n",
            "Epoch: 27640 | Training Loss: 0.3264409005641937 | Testing Loss: 0.2328466922044754\n",
            "Epoch: 27650 | Training Loss: 0.3258804678916931 | Testing Loss: 0.2324203997850418\n",
            "Epoch: 27660 | Training Loss: 0.3253200948238373 | Testing Loss: 0.23199410736560822\n",
            "Epoch: 27670 | Training Loss: 0.32475966215133667 | Testing Loss: 0.23156781494617462\n",
            "Epoch: 27680 | Training Loss: 0.32419928908348083 | Testing Loss: 0.23114152252674103\n",
            "Epoch: 27690 | Training Loss: 0.3236388564109802 | Testing Loss: 0.23071523010730743\n",
            "Epoch: 27700 | Training Loss: 0.3230784833431244 | Testing Loss: 0.23028893768787384\n",
            "Epoch: 27710 | Training Loss: 0.3225180506706238 | Testing Loss: 0.22986264526844025\n",
            "Epoch: 27720 | Training Loss: 0.32195767760276794 | Testing Loss: 0.22943635284900665\n",
            "Epoch: 27730 | Training Loss: 0.32139724493026733 | Testing Loss: 0.22901006042957306\n",
            "Epoch: 27740 | Training Loss: 0.3208368718624115 | Testing Loss: 0.22858376801013947\n",
            "Epoch: 27750 | Training Loss: 0.3202764391899109 | Testing Loss: 0.22815747559070587\n",
            "Epoch: 27760 | Training Loss: 0.31971606612205505 | Testing Loss: 0.22773118317127228\n",
            "Epoch: 27770 | Training Loss: 0.31915563344955444 | Testing Loss: 0.22730489075183868\n",
            "Epoch: 27780 | Training Loss: 0.3185952603816986 | Testing Loss: 0.2268785983324051\n",
            "Epoch: 27790 | Training Loss: 0.318034827709198 | Testing Loss: 0.2264523059129715\n",
            "Epoch: 27800 | Training Loss: 0.31747445464134216 | Testing Loss: 0.2260260134935379\n",
            "Epoch: 27810 | Training Loss: 0.31691402196884155 | Testing Loss: 0.2255997210741043\n",
            "Epoch: 27820 | Training Loss: 0.3163536489009857 | Testing Loss: 0.22517342865467072\n",
            "Epoch: 27830 | Training Loss: 0.3157932162284851 | Testing Loss: 0.22474713623523712\n",
            "Epoch: 27840 | Training Loss: 0.3152328431606293 | Testing Loss: 0.22432084381580353\n",
            "Epoch: 27850 | Training Loss: 0.31467241048812866 | Testing Loss: 0.22389455139636993\n",
            "Epoch: 27860 | Training Loss: 0.3141120374202728 | Testing Loss: 0.22346825897693634\n",
            "Epoch: 27870 | Training Loss: 0.3135516047477722 | Testing Loss: 0.22304196655750275\n",
            "Epoch: 27880 | Training Loss: 0.312991201877594 | Testing Loss: 0.22261567413806915\n",
            "Epoch: 27890 | Training Loss: 0.31243082880973816 | Testing Loss: 0.22218938171863556\n",
            "Epoch: 27900 | Training Loss: 0.31187039613723755 | Testing Loss: 0.22176308929920197\n",
            "Epoch: 27910 | Training Loss: 0.3113100230693817 | Testing Loss: 0.22133679687976837\n",
            "Epoch: 27920 | Training Loss: 0.3107495903968811 | Testing Loss: 0.22091050446033478\n",
            "Epoch: 27930 | Training Loss: 0.31018921732902527 | Testing Loss: 0.22048421204090118\n",
            "Epoch: 27940 | Training Loss: 0.30962878465652466 | Testing Loss: 0.2200579196214676\n",
            "Epoch: 27950 | Training Loss: 0.3090684115886688 | Testing Loss: 0.219631627202034\n",
            "Epoch: 27960 | Training Loss: 0.3085079789161682 | Testing Loss: 0.2192053347826004\n",
            "Epoch: 27970 | Training Loss: 0.3079476058483124 | Testing Loss: 0.2187790423631668\n",
            "Epoch: 27980 | Training Loss: 0.30738717317581177 | Testing Loss: 0.21835274994373322\n",
            "Epoch: 27990 | Training Loss: 0.30682680010795593 | Testing Loss: 0.21792645752429962\n",
            "Epoch: 28000 | Training Loss: 0.3062663674354553 | Testing Loss: 0.21750016510486603\n",
            "Epoch: 28010 | Training Loss: 0.3057059943675995 | Testing Loss: 0.21707387268543243\n",
            "Epoch: 28020 | Training Loss: 0.3051455616950989 | Testing Loss: 0.21664758026599884\n",
            "Epoch: 28030 | Training Loss: 0.30458518862724304 | Testing Loss: 0.21622128784656525\n",
            "Epoch: 28040 | Training Loss: 0.30402475595474243 | Testing Loss: 0.21579499542713165\n",
            "Epoch: 28050 | Training Loss: 0.3034643828868866 | Testing Loss: 0.21536870300769806\n",
            "Epoch: 28060 | Training Loss: 0.302903950214386 | Testing Loss: 0.21494241058826447\n",
            "Epoch: 28070 | Training Loss: 0.30234357714653015 | Testing Loss: 0.21451611816883087\n",
            "Epoch: 28080 | Training Loss: 0.30178314447402954 | Testing Loss: 0.21408982574939728\n",
            "Epoch: 28090 | Training Loss: 0.3012227714061737 | Testing Loss: 0.21366353332996368\n",
            "Epoch: 28100 | Training Loss: 0.3006623387336731 | Testing Loss: 0.2132372409105301\n",
            "Epoch: 28110 | Training Loss: 0.30010196566581726 | Testing Loss: 0.2128109484910965\n",
            "Epoch: 28120 | Training Loss: 0.29954153299331665 | Testing Loss: 0.2123846560716629\n",
            "Epoch: 28130 | Training Loss: 0.2989811599254608 | Testing Loss: 0.2119583636522293\n",
            "Epoch: 28140 | Training Loss: 0.2984207272529602 | Testing Loss: 0.21153207123279572\n",
            "Epoch: 28150 | Training Loss: 0.29786035418510437 | Testing Loss: 0.21110577881336212\n",
            "Epoch: 28160 | Training Loss: 0.29729992151260376 | Testing Loss: 0.21067948639392853\n",
            "Epoch: 28170 | Training Loss: 0.2967395484447479 | Testing Loss: 0.21025319397449493\n",
            "Epoch: 28180 | Training Loss: 0.2961791157722473 | Testing Loss: 0.20982690155506134\n",
            "Epoch: 28190 | Training Loss: 0.2956187427043915 | Testing Loss: 0.20940060913562775\n",
            "Epoch: 28200 | Training Loss: 0.29505831003189087 | Testing Loss: 0.20897431671619415\n",
            "Epoch: 28210 | Training Loss: 0.29449793696403503 | Testing Loss: 0.20854802429676056\n",
            "Epoch: 28220 | Training Loss: 0.2939375042915344 | Testing Loss: 0.20812173187732697\n",
            "Epoch: 28230 | Training Loss: 0.2933771312236786 | Testing Loss: 0.20769543945789337\n",
            "Epoch: 28240 | Training Loss: 0.292816698551178 | Testing Loss: 0.20726914703845978\n",
            "Epoch: 28250 | Training Loss: 0.29225632548332214 | Testing Loss: 0.20684285461902618\n",
            "Epoch: 28260 | Training Loss: 0.29169589281082153 | Testing Loss: 0.2064165621995926\n",
            "Epoch: 28270 | Training Loss: 0.2911355197429657 | Testing Loss: 0.205990269780159\n",
            "Epoch: 28280 | Training Loss: 0.2905750870704651 | Testing Loss: 0.2055639773607254\n",
            "Epoch: 28290 | Training Loss: 0.29001471400260925 | Testing Loss: 0.2051376849412918\n",
            "Epoch: 28300 | Training Loss: 0.28945428133010864 | Testing Loss: 0.20471139252185822\n",
            "Epoch: 28310 | Training Loss: 0.2888939082622528 | Testing Loss: 0.20428510010242462\n",
            "Epoch: 28320 | Training Loss: 0.2883334755897522 | Testing Loss: 0.20385880768299103\n",
            "Epoch: 28330 | Training Loss: 0.28777310252189636 | Testing Loss: 0.20343251526355743\n",
            "Epoch: 28340 | Training Loss: 0.28721266984939575 | Testing Loss: 0.20300622284412384\n",
            "Epoch: 28350 | Training Loss: 0.2866522967815399 | Testing Loss: 0.20257993042469025\n",
            "Epoch: 28360 | Training Loss: 0.2860918641090393 | Testing Loss: 0.20215363800525665\n",
            "Epoch: 28370 | Training Loss: 0.28553149104118347 | Testing Loss: 0.20172734558582306\n",
            "Epoch: 28380 | Training Loss: 0.28497105836868286 | Testing Loss: 0.20130105316638947\n",
            "Epoch: 28390 | Training Loss: 0.284410685300827 | Testing Loss: 0.20087476074695587\n",
            "Epoch: 28400 | Training Loss: 0.2838502526283264 | Testing Loss: 0.20044846832752228\n",
            "Epoch: 28410 | Training Loss: 0.2832898795604706 | Testing Loss: 0.20002217590808868\n",
            "Epoch: 28420 | Training Loss: 0.28272944688796997 | Testing Loss: 0.1995958834886551\n",
            "Epoch: 28430 | Training Loss: 0.28216907382011414 | Testing Loss: 0.1991695910692215\n",
            "Epoch: 28440 | Training Loss: 0.2816086411476135 | Testing Loss: 0.1987432986497879\n",
            "Epoch: 28450 | Training Loss: 0.2810482680797577 | Testing Loss: 0.1983170062303543\n",
            "Epoch: 28460 | Training Loss: 0.2804878354072571 | Testing Loss: 0.19789071381092072\n",
            "Epoch: 28470 | Training Loss: 0.27992746233940125 | Testing Loss: 0.19746442139148712\n",
            "Epoch: 28480 | Training Loss: 0.27936702966690063 | Testing Loss: 0.19703812897205353\n",
            "Epoch: 28490 | Training Loss: 0.2788066565990448 | Testing Loss: 0.19663415849208832\n",
            "Epoch: 28500 | Training Loss: 0.2782462239265442 | Testing Loss: 0.19623075425624847\n",
            "Epoch: 28510 | Training Loss: 0.27768585085868835 | Testing Loss: 0.19582735002040863\n",
            "Epoch: 28520 | Training Loss: 0.27712541818618774 | Testing Loss: 0.1954239457845688\n",
            "Epoch: 28530 | Training Loss: 0.2765650451183319 | Testing Loss: 0.19502054154872894\n",
            "Epoch: 28540 | Training Loss: 0.2760046124458313 | Testing Loss: 0.1946171373128891\n",
            "Epoch: 28550 | Training Loss: 0.27544423937797546 | Testing Loss: 0.19421373307704926\n",
            "Epoch: 28560 | Training Loss: 0.27488380670547485 | Testing Loss: 0.1938103288412094\n",
            "Epoch: 28570 | Training Loss: 0.274323433637619 | Testing Loss: 0.19340692460536957\n",
            "Epoch: 28580 | Training Loss: 0.2737630009651184 | Testing Loss: 0.19300352036952972\n",
            "Epoch: 28590 | Training Loss: 0.2732026278972626 | Testing Loss: 0.19260011613368988\n",
            "Epoch: 28600 | Training Loss: 0.27264219522476196 | Testing Loss: 0.19219671189785004\n",
            "Epoch: 28610 | Training Loss: 0.27208182215690613 | Testing Loss: 0.1917933076620102\n",
            "Epoch: 28620 | Training Loss: 0.2715213894844055 | Testing Loss: 0.19138990342617035\n",
            "Epoch: 28630 | Training Loss: 0.2709610164165497 | Testing Loss: 0.1909864991903305\n",
            "Epoch: 28640 | Training Loss: 0.2704005837440491 | Testing Loss: 0.19058309495449066\n",
            "Epoch: 28650 | Training Loss: 0.26984021067619324 | Testing Loss: 0.19017969071865082\n",
            "Epoch: 28660 | Training Loss: 0.2692797780036926 | Testing Loss: 0.18977628648281097\n",
            "Epoch: 28670 | Training Loss: 0.2687194049358368 | Testing Loss: 0.18937288224697113\n",
            "Epoch: 28680 | Training Loss: 0.2681589722633362 | Testing Loss: 0.1889694780111313\n",
            "Epoch: 28690 | Training Loss: 0.26759859919548035 | Testing Loss: 0.18856607377529144\n",
            "Epoch: 28700 | Training Loss: 0.26703816652297974 | Testing Loss: 0.1881626695394516\n",
            "Epoch: 28710 | Training Loss: 0.2664777934551239 | Testing Loss: 0.18775944411754608\n",
            "Epoch: 28720 | Training Loss: 0.2659173905849457 | Testing Loss: 0.18735595047473907\n",
            "Epoch: 28730 | Training Loss: 0.26535698771476746 | Testing Loss: 0.1869526356458664\n",
            "Epoch: 28740 | Training Loss: 0.26479658484458923 | Testing Loss: 0.1865491420030594\n",
            "Epoch: 28750 | Training Loss: 0.264236181974411 | Testing Loss: 0.1861458271741867\n",
            "Epoch: 28760 | Training Loss: 0.2636757791042328 | Testing Loss: 0.1857423335313797\n",
            "Epoch: 28770 | Training Loss: 0.26311537623405457 | Testing Loss: 0.18533901870250702\n",
            "Epoch: 28780 | Training Loss: 0.26255497336387634 | Testing Loss: 0.1849355250597\n",
            "Epoch: 28790 | Training Loss: 0.2619945704936981 | Testing Loss: 0.18453221023082733\n",
            "Epoch: 28800 | Training Loss: 0.2614341676235199 | Testing Loss: 0.18412871658802032\n",
            "Epoch: 28810 | Training Loss: 0.2608737647533417 | Testing Loss: 0.18372540175914764\n",
            "Epoch: 28820 | Training Loss: 0.26031336188316345 | Testing Loss: 0.18332190811634064\n",
            "Epoch: 28830 | Training Loss: 0.25975295901298523 | Testing Loss: 0.18291859328746796\n",
            "Epoch: 28840 | Training Loss: 0.259192556142807 | Testing Loss: 0.18251509964466095\n",
            "Epoch: 28850 | Training Loss: 0.2586321532726288 | Testing Loss: 0.18211178481578827\n",
            "Epoch: 28860 | Training Loss: 0.25807175040245056 | Testing Loss: 0.18170829117298126\n",
            "Epoch: 28870 | Training Loss: 0.25751134753227234 | Testing Loss: 0.18130497634410858\n",
            "Epoch: 28880 | Training Loss: 0.2569509446620941 | Testing Loss: 0.18090148270130157\n",
            "Epoch: 28890 | Training Loss: 0.2563905417919159 | Testing Loss: 0.1804981678724289\n",
            "Epoch: 28900 | Training Loss: 0.25583013892173767 | Testing Loss: 0.1800946742296219\n",
            "Epoch: 28910 | Training Loss: 0.25526973605155945 | Testing Loss: 0.1796913594007492\n",
            "Epoch: 28920 | Training Loss: 0.2547093331813812 | Testing Loss: 0.1792878657579422\n",
            "Epoch: 28930 | Training Loss: 0.254148930311203 | Testing Loss: 0.17888455092906952\n",
            "Epoch: 28940 | Training Loss: 0.2535885274410248 | Testing Loss: 0.1784810572862625\n",
            "Epoch: 28950 | Training Loss: 0.25302812457084656 | Testing Loss: 0.17807774245738983\n",
            "Epoch: 28960 | Training Loss: 0.25246772170066833 | Testing Loss: 0.17767424881458282\n",
            "Epoch: 28970 | Training Loss: 0.2519073188304901 | Testing Loss: 0.17727093398571014\n",
            "Epoch: 28980 | Training Loss: 0.2513469159603119 | Testing Loss: 0.17686744034290314\n",
            "Epoch: 28990 | Training Loss: 0.25078651309013367 | Testing Loss: 0.17646412551403046\n",
            "Epoch: 29000 | Training Loss: 0.25022611021995544 | Testing Loss: 0.17606063187122345\n",
            "Epoch: 29010 | Training Loss: 0.24966569244861603 | Testing Loss: 0.17565731704235077\n",
            "Epoch: 29020 | Training Loss: 0.2491053193807602 | Testing Loss: 0.17525382339954376\n",
            "Epoch: 29030 | Training Loss: 0.24854488670825958 | Testing Loss: 0.17485050857067108\n",
            "Epoch: 29040 | Training Loss: 0.24798451364040375 | Testing Loss: 0.17444701492786407\n",
            "Epoch: 29050 | Training Loss: 0.24742408096790314 | Testing Loss: 0.1740437000989914\n",
            "Epoch: 29060 | Training Loss: 0.2468637079000473 | Testing Loss: 0.1736402064561844\n",
            "Epoch: 29070 | Training Loss: 0.2463032752275467 | Testing Loss: 0.1732368916273117\n",
            "Epoch: 29080 | Training Loss: 0.24574290215969086 | Testing Loss: 0.1728333979845047\n",
            "Epoch: 29090 | Training Loss: 0.24518246948719025 | Testing Loss: 0.17243008315563202\n",
            "Epoch: 29100 | Training Loss: 0.2446220964193344 | Testing Loss: 0.172026589512825\n",
            "Epoch: 29110 | Training Loss: 0.2440616637468338 | Testing Loss: 0.17162327468395233\n",
            "Epoch: 29120 | Training Loss: 0.24350129067897797 | Testing Loss: 0.17121978104114532\n",
            "Epoch: 29130 | Training Loss: 0.24294085800647736 | Testing Loss: 0.17081646621227264\n",
            "Epoch: 29140 | Training Loss: 0.24238048493862152 | Testing Loss: 0.17041297256946564\n",
            "Epoch: 29150 | Training Loss: 0.2418200522661209 | Testing Loss: 0.17000965774059296\n",
            "Epoch: 29160 | Training Loss: 0.24125967919826508 | Testing Loss: 0.16960616409778595\n",
            "Epoch: 29170 | Training Loss: 0.24069924652576447 | Testing Loss: 0.16920284926891327\n",
            "Epoch: 29180 | Training Loss: 0.24013887345790863 | Testing Loss: 0.16879935562610626\n",
            "Epoch: 29190 | Training Loss: 0.23957844078540802 | Testing Loss: 0.16839604079723358\n",
            "Epoch: 29200 | Training Loss: 0.23901806771755219 | Testing Loss: 0.16799254715442657\n",
            "Epoch: 29210 | Training Loss: 0.23845763504505157 | Testing Loss: 0.1675892323255539\n",
            "Epoch: 29220 | Training Loss: 0.23789726197719574 | Testing Loss: 0.1671857386827469\n",
            "Epoch: 29230 | Training Loss: 0.23733682930469513 | Testing Loss: 0.1667824238538742\n",
            "Epoch: 29240 | Training Loss: 0.2367764562368393 | Testing Loss: 0.1663789302110672\n",
            "Epoch: 29250 | Training Loss: 0.23621602356433868 | Testing Loss: 0.16597561538219452\n",
            "Epoch: 29260 | Training Loss: 0.23565565049648285 | Testing Loss: 0.1655721217393875\n",
            "Epoch: 29270 | Training Loss: 0.23509521782398224 | Testing Loss: 0.16516880691051483\n",
            "Epoch: 29280 | Training Loss: 0.2345348447561264 | Testing Loss: 0.16476531326770782\n",
            "Epoch: 29290 | Training Loss: 0.2339744120836258 | Testing Loss: 0.16436199843883514\n",
            "Epoch: 29300 | Training Loss: 0.23341403901576996 | Testing Loss: 0.16395850479602814\n",
            "Epoch: 29310 | Training Loss: 0.23285360634326935 | Testing Loss: 0.16355518996715546\n",
            "Epoch: 29320 | Training Loss: 0.2322932332754135 | Testing Loss: 0.16315169632434845\n",
            "Epoch: 29330 | Training Loss: 0.2317328006029129 | Testing Loss: 0.16274838149547577\n",
            "Epoch: 29340 | Training Loss: 0.23117242753505707 | Testing Loss: 0.16234488785266876\n",
            "Epoch: 29350 | Training Loss: 0.23061199486255646 | Testing Loss: 0.16194157302379608\n",
            "Epoch: 29360 | Training Loss: 0.23005162179470062 | Testing Loss: 0.16153807938098907\n",
            "Epoch: 29370 | Training Loss: 0.2294911891222 | Testing Loss: 0.1611347645521164\n",
            "Epoch: 29380 | Training Loss: 0.22893081605434418 | Testing Loss: 0.1607312709093094\n",
            "Epoch: 29390 | Training Loss: 0.22837038338184357 | Testing Loss: 0.1603279560804367\n",
            "Epoch: 29400 | Training Loss: 0.22781001031398773 | Testing Loss: 0.1599244624376297\n",
            "Epoch: 29410 | Training Loss: 0.22724957764148712 | Testing Loss: 0.15952114760875702\n",
            "Epoch: 29420 | Training Loss: 0.2266892045736313 | Testing Loss: 0.15911765396595\n",
            "Epoch: 29430 | Training Loss: 0.22612877190113068 | Testing Loss: 0.15871433913707733\n",
            "Epoch: 29440 | Training Loss: 0.22556839883327484 | Testing Loss: 0.15831084549427032\n",
            "Epoch: 29450 | Training Loss: 0.22500796616077423 | Testing Loss: 0.15790753066539764\n",
            "Epoch: 29460 | Training Loss: 0.2244475930929184 | Testing Loss: 0.15750403702259064\n",
            "Epoch: 29470 | Training Loss: 0.22388716042041779 | Testing Loss: 0.15710072219371796\n",
            "Epoch: 29480 | Training Loss: 0.22332678735256195 | Testing Loss: 0.15669722855091095\n",
            "Epoch: 29490 | Training Loss: 0.22276635468006134 | Testing Loss: 0.15629391372203827\n",
            "Epoch: 29500 | Training Loss: 0.2222059816122055 | Testing Loss: 0.15589042007923126\n",
            "Epoch: 29510 | Training Loss: 0.2216455489397049 | Testing Loss: 0.15548710525035858\n",
            "Epoch: 29520 | Training Loss: 0.22108517587184906 | Testing Loss: 0.15508361160755157\n",
            "Epoch: 29530 | Training Loss: 0.22052474319934845 | Testing Loss: 0.1546802967786789\n",
            "Epoch: 29540 | Training Loss: 0.21996437013149261 | Testing Loss: 0.1542768031358719\n",
            "Epoch: 29550 | Training Loss: 0.219403937458992 | Testing Loss: 0.1538734883069992\n",
            "Epoch: 29560 | Training Loss: 0.21884356439113617 | Testing Loss: 0.1534699946641922\n",
            "Epoch: 29570 | Training Loss: 0.21828313171863556 | Testing Loss: 0.15306667983531952\n",
            "Epoch: 29580 | Training Loss: 0.21772275865077972 | Testing Loss: 0.1526631861925125\n",
            "Epoch: 29590 | Training Loss: 0.2171623259782791 | Testing Loss: 0.15225987136363983\n",
            "Epoch: 29600 | Training Loss: 0.21660195291042328 | Testing Loss: 0.15185637772083282\n",
            "Epoch: 29610 | Training Loss: 0.21604152023792267 | Testing Loss: 0.15145306289196014\n",
            "Epoch: 29620 | Training Loss: 0.21548114717006683 | Testing Loss: 0.15104956924915314\n",
            "Epoch: 29630 | Training Loss: 0.21492071449756622 | Testing Loss: 0.15064625442028046\n",
            "Epoch: 29640 | Training Loss: 0.2143603414297104 | Testing Loss: 0.15024276077747345\n",
            "Epoch: 29650 | Training Loss: 0.21379990875720978 | Testing Loss: 0.14983944594860077\n",
            "Epoch: 29660 | Training Loss: 0.21323953568935394 | Testing Loss: 0.14943595230579376\n",
            "Epoch: 29670 | Training Loss: 0.21267910301685333 | Testing Loss: 0.14903263747692108\n",
            "Epoch: 29680 | Training Loss: 0.2121187299489975 | Testing Loss: 0.14862914383411407\n",
            "Epoch: 29690 | Training Loss: 0.2115582972764969 | Testing Loss: 0.1482258290052414\n",
            "Epoch: 29700 | Training Loss: 0.21099792420864105 | Testing Loss: 0.1478223353624344\n",
            "Epoch: 29710 | Training Loss: 0.21043749153614044 | Testing Loss: 0.1474190205335617\n",
            "Epoch: 29720 | Training Loss: 0.2098771184682846 | Testing Loss: 0.1470155268907547\n",
            "Epoch: 29730 | Training Loss: 0.209316685795784 | Testing Loss: 0.14661221206188202\n",
            "Epoch: 29740 | Training Loss: 0.20875631272792816 | Testing Loss: 0.146208718419075\n",
            "Epoch: 29750 | Training Loss: 0.20819588005542755 | Testing Loss: 0.14580540359020233\n",
            "Epoch: 29760 | Training Loss: 0.20763550698757172 | Testing Loss: 0.14540190994739532\n",
            "Epoch: 29770 | Training Loss: 0.2070750743150711 | Testing Loss: 0.14499859511852264\n",
            "Epoch: 29780 | Training Loss: 0.20651470124721527 | Testing Loss: 0.14459510147571564\n",
            "Epoch: 29790 | Training Loss: 0.20595426857471466 | Testing Loss: 0.14419178664684296\n",
            "Epoch: 29800 | Training Loss: 0.20539389550685883 | Testing Loss: 0.14378829300403595\n",
            "Epoch: 29810 | Training Loss: 0.20483346283435822 | Testing Loss: 0.14338497817516327\n",
            "Epoch: 29820 | Training Loss: 0.20427460968494415 | Testing Loss: 0.1429913491010666\n",
            "Epoch: 29830 | Training Loss: 0.20371584594249725 | Testing Loss: 0.14260315895080566\n",
            "Epoch: 29840 | Training Loss: 0.20315733551979065 | Testing Loss: 0.1422097235918045\n",
            "Epoch: 29850 | Training Loss: 0.2025984823703766 | Testing Loss: 0.14181652665138245\n",
            "Epoch: 29860 | Training Loss: 0.20204022526741028 | Testing Loss: 0.14142803847789764\n",
            "Epoch: 29870 | Training Loss: 0.2014811486005783 | Testing Loss: 0.141034796833992\n",
            "Epoch: 29880 | Training Loss: 0.2009229212999344 | Testing Loss: 0.14064641296863556\n",
            "Epoch: 29890 | Training Loss: 0.2003643959760666 | Testing Loss: 0.14025560021400452\n",
            "Epoch: 29900 | Training Loss: 0.19980548322200775 | Testing Loss: 0.13986225426197052\n",
            "Epoch: 29910 | Training Loss: 0.19924695789813995 | Testing Loss: 0.1394737809896469\n",
            "Epoch: 29920 | Training Loss: 0.19868819415569305 | Testing Loss: 0.13908052444458008\n",
            "Epoch: 29930 | Training Loss: 0.1981293261051178 | Testing Loss: 0.13868717849254608\n",
            "Epoch: 29940 | Training Loss: 0.1975710541009903 | Testing Loss: 0.13829870522022247\n",
            "Epoch: 29950 | Training Loss: 0.19701208174228668 | Testing Loss: 0.13790535926818848\n",
            "Epoch: 29960 | Training Loss: 0.19645379483699799 | Testing Loss: 0.13751697540283203\n",
            "Epoch: 29970 | Training Loss: 0.19589489698410034 | Testing Loss: 0.13712362945079803\n",
            "Epoch: 29980 | Training Loss: 0.19533634185791016 | Testing Loss: 0.13673292100429535\n",
            "Epoch: 29990 | Training Loss: 0.1947777420282364 | Testing Loss: 0.13634443283081055\n",
            "Epoch: 30000 | Training Loss: 0.19421909749507904 | Testing Loss: 0.13595108687877655\n",
            "Epoch: 30010 | Training Loss: 0.19366015493869781 | Testing Loss: 0.1355627030134201\n",
            "Epoch: 30020 | Training Loss: 0.1931019276380539 | Testing Loss: 0.1351693719625473\n",
            "Epoch: 30030 | Training Loss: 0.19254295527935028 | Testing Loss: 0.1347760260105133\n",
            "Epoch: 30040 | Training Loss: 0.19198474287986755 | Testing Loss: 0.13438773155212402\n",
            "Epoch: 30050 | Training Loss: 0.19142578542232513 | Testing Loss: 0.13399429619312286\n",
            "Epoch: 30060 | Training Loss: 0.1908671259880066 | Testing Loss: 0.13360348343849182\n",
            "Epoch: 30070 | Training Loss: 0.19030870497226715 | Testing Loss: 0.13321509957313538\n",
            "Epoch: 30080 | Training Loss: 0.18974998593330383 | Testing Loss: 0.13282175362110138\n",
            "Epoch: 30090 | Training Loss: 0.18919113278388977 | Testing Loss: 0.1324334591627121\n",
            "Epoch: 30100 | Training Loss: 0.1886328160762787 | Testing Loss: 0.13204002380371094\n",
            "Epoch: 30110 | Training Loss: 0.18807384371757507 | Testing Loss: 0.13164667785167694\n",
            "Epoch: 30120 | Training Loss: 0.187515527009964 | Testing Loss: 0.13125844299793243\n",
            "Epoch: 30130 | Training Loss: 0.18695667386054993 | Testing Loss: 0.13086485862731934\n",
            "Epoch: 30140 | Training Loss: 0.18639801442623138 | Testing Loss: 0.13047413527965546\n",
            "Epoch: 30150 | Training Loss: 0.18583963811397552 | Testing Loss: 0.130085751414299\n",
            "Epoch: 30160 | Training Loss: 0.18528085947036743 | Testing Loss: 0.1296924203634262\n",
            "Epoch: 30170 | Training Loss: 0.18472208082675934 | Testing Loss: 0.1293041706085205\n",
            "Epoch: 30180 | Training Loss: 0.18416368961334229 | Testing Loss: 0.1289105862379074\n",
            "Epoch: 30190 | Training Loss: 0.18360482156276703 | Testing Loss: 0.12851734459400177\n",
            "Epoch: 30200 | Training Loss: 0.18304644525051117 | Testing Loss: 0.1281290054321289\n",
            "Epoch: 30210 | Training Loss: 0.18248747289180756 | Testing Loss: 0.12773557007312775\n",
            "Epoch: 30220 | Training Loss: 0.18192926049232483 | Testing Loss: 0.12734727561473846\n",
            "Epoch: 30230 | Training Loss: 0.18137064576148987 | Testing Loss: 0.12695632874965668\n",
            "Epoch: 30240 | Training Loss: 0.1808118373155594 | Testing Loss: 0.12656307220458984\n",
            "Epoch: 30250 | Training Loss: 0.1802530288696289 | Testing Loss: 0.12617473304271698\n",
            "Epoch: 30260 | Training Loss: 0.17969448864459991 | Testing Loss: 0.12578129768371582\n",
            "Epoch: 30270 | Training Loss: 0.17913557589054108 | Testing Loss: 0.1253880113363266\n",
            "Epoch: 30280 | Training Loss: 0.17857734858989716 | Testing Loss: 0.1249997615814209\n",
            "Epoch: 30290 | Training Loss: 0.17801837623119354 | Testing Loss: 0.1246064230799675\n",
            "Epoch: 30300 | Training Loss: 0.17746011912822723 | Testing Loss: 0.12421803921461105\n",
            "Epoch: 30310 | Training Loss: 0.17690157890319824 | Testing Loss: 0.12382703274488449\n",
            "Epoch: 30320 | Training Loss: 0.17634259164333344 | Testing Loss: 0.12343373149633408\n",
            "Epoch: 30330 | Training Loss: 0.1757839173078537 | Testing Loss: 0.12304549664258957\n",
            "Epoch: 30340 | Training Loss: 0.1752253919839859 | Testing Loss: 0.12265215069055557\n",
            "Epoch: 30350 | Training Loss: 0.1746663898229599 | Testing Loss: 0.12225856631994247\n",
            "Epoch: 30360 | Training Loss: 0.17410819232463837 | Testing Loss: 0.12187042087316513\n",
            "Epoch: 30370 | Training Loss: 0.1735493242740631 | Testing Loss: 0.12147698551416397\n",
            "Epoch: 30380 | Training Loss: 0.17299088835716248 | Testing Loss: 0.12108869850635529\n",
            "Epoch: 30390 | Training Loss: 0.17243202030658722 | Testing Loss: 0.12069535255432129\n",
            "Epoch: 30400 | Training Loss: 0.17187340557575226 | Testing Loss: 0.12030430138111115\n",
            "Epoch: 30410 | Training Loss: 0.1713147908449173 | Testing Loss: 0.1199161559343338\n",
            "Epoch: 30420 | Training Loss: 0.17075634002685547 | Testing Loss: 0.11952271312475204\n",
            "Epoch: 30430 | Training Loss: 0.17019730806350708 | Testing Loss: 0.11913442611694336\n",
            "Epoch: 30440 | Training Loss: 0.16963903605937958 | Testing Loss: 0.11874108761548996\n",
            "Epoch: 30450 | Training Loss: 0.1690800040960312 | Testing Loss: 0.1183476448059082\n",
            "Epoch: 30460 | Training Loss: 0.16852180659770966 | Testing Loss: 0.11795926094055176\n",
            "Epoch: 30470 | Training Loss: 0.16796280443668365 | Testing Loss: 0.11756592243909836\n",
            "Epoch: 30480 | Training Loss: 0.16740427911281586 | Testing Loss: 0.11717519909143448\n",
            "Epoch: 30490 | Training Loss: 0.16684584319591522 | Testing Loss: 0.11678681522607803\n",
            "Epoch: 30500 | Training Loss: 0.16628703474998474 | Testing Loss: 0.11639337986707687\n",
            "Epoch: 30510 | Training Loss: 0.16572830080986023 | Testing Loss: 0.11600499600172043\n",
            "Epoch: 30520 | Training Loss: 0.1651695966720581 | Testing Loss: 0.11560916900634766\n",
            "Epoch: 30530 | Training Loss: 0.1646113097667694 | Testing Loss: 0.11522092670202255\n",
            "Epoch: 30540 | Training Loss: 0.1640523374080658 | Testing Loss: 0.11482744663953781\n",
            "Epoch: 30550 | Training Loss: 0.1634940505027771 | Testing Loss: 0.11443910747766495\n",
            "Epoch: 30560 | Training Loss: 0.16293518245220184 | Testing Loss: 0.11404585838317871\n",
            "Epoch: 30570 | Training Loss: 0.16237682104110718 | Testing Loss: 0.1136573776602745\n",
            "Epoch: 30580 | Training Loss: 0.16181789338588715 | Testing Loss: 0.1132640391588211\n",
            "Epoch: 30590 | Training Loss: 0.1612592190504074 | Testing Loss: 0.11287565529346466\n",
            "Epoch: 30600 | Training Loss: 0.1607007086277008 | Testing Loss: 0.11248230934143066\n",
            "Epoch: 30610 | Training Loss: 0.1601421982049942 | Testing Loss: 0.11209159344434738\n",
            "Epoch: 30620 | Training Loss: 0.1595832258462906 | Testing Loss: 0.11169824749231339\n",
            "Epoch: 30630 | Training Loss: 0.1590249240398407 | Testing Loss: 0.11130976676940918\n",
            "Epoch: 30640 | Training Loss: 0.15846605598926544 | Testing Loss: 0.1109161376953125\n",
            "Epoch: 30650 | Training Loss: 0.15790772438049316 | Testing Loss: 0.11052804440259933\n",
            "Epoch: 30660 | Training Loss: 0.15734878182411194 | Testing Loss: 0.11013460159301758\n",
            "Epoch: 30670 | Training Loss: 0.156790092587471 | Testing Loss: 0.10974631458520889\n",
            "Epoch: 30680 | Training Loss: 0.1562316119670868 | Testing Loss: 0.10935306549072266\n",
            "Epoch: 30690 | Training Loss: 0.1556730717420578 | Testing Loss: 0.10896187275648117\n",
            "Epoch: 30700 | Training Loss: 0.155114084482193 | Testing Loss: 0.10856862366199493\n",
            "Epoch: 30710 | Training Loss: 0.1545557975769043 | Testing Loss: 0.10818033665418625\n",
            "Epoch: 30720 | Training Loss: 0.15399682521820068 | Testing Loss: 0.10778699070215225\n",
            "Epoch: 30730 | Training Loss: 0.15343855321407318 | Testing Loss: 0.10739880055189133\n",
            "Epoch: 30740 | Training Loss: 0.15287970006465912 | Testing Loss: 0.10700535774230957\n",
            "Epoch: 30750 | Training Loss: 0.15232107043266296 | Testing Loss: 0.10661697387695312\n",
            "Epoch: 30760 | Training Loss: 0.1517624706029892 | Testing Loss: 0.10622363537549973\n",
            "Epoch: 30770 | Training Loss: 0.15120351314544678 | Testing Loss: 0.10583271831274033\n",
            "Epoch: 30780 | Training Loss: 0.15064482390880585 | Testing Loss: 0.10543937981128693\n",
            "Epoch: 30790 | Training Loss: 0.15008671581745148 | Testing Loss: 0.10505109280347824\n",
            "Epoch: 30800 | Training Loss: 0.14952771365642548 | Testing Loss: 0.10465764999389648\n",
            "Epoch: 30810 | Training Loss: 0.14896948635578156 | Testing Loss: 0.1042693629860878\n",
            "Epoch: 30820 | Training Loss: 0.14841054379940033 | Testing Loss: 0.1038760170340538\n",
            "Epoch: 30830 | Training Loss: 0.14785201847553253 | Testing Loss: 0.10348763316869736\n",
            "Epoch: 30840 | Training Loss: 0.14729325473308563 | Testing Loss: 0.10309439152479172\n",
            "Epoch: 30850 | Training Loss: 0.14673446118831635 | Testing Loss: 0.10270605236291885\n",
            "Epoch: 30860 | Training Loss: 0.14617577195167542 | Testing Loss: 0.10231013596057892\n",
            "Epoch: 30870 | Training Loss: 0.1456175595521927 | Testing Loss: 0.10192175209522247\n",
            "Epoch: 30880 | Training Loss: 0.14505861699581146 | Testing Loss: 0.10152830928564072\n",
            "Epoch: 30890 | Training Loss: 0.14450030028820038 | Testing Loss: 0.10114011913537979\n",
            "Epoch: 30900 | Training Loss: 0.14394143223762512 | Testing Loss: 0.10074658691883087\n",
            "Epoch: 30910 | Training Loss: 0.1433829814195633 | Testing Loss: 0.10035820305347443\n",
            "Epoch: 30920 | Training Loss: 0.14282409846782684 | Testing Loss: 0.09996485710144043\n",
            "Epoch: 30930 | Training Loss: 0.1422654241323471 | Testing Loss: 0.09957661479711533\n",
            "Epoch: 30940 | Training Loss: 0.1417066603899002 | Testing Loss: 0.09918069839477539\n",
            "Epoch: 30950 | Training Loss: 0.14114844799041748 | Testing Loss: 0.09879231452941895\n",
            "Epoch: 30960 | Training Loss: 0.14058947563171387 | Testing Loss: 0.09839897602796555\n",
            "Epoch: 30970 | Training Loss: 0.1400311142206192 | Testing Loss: 0.0980105921626091\n",
            "Epoch: 30980 | Training Loss: 0.13947230577468872 | Testing Loss: 0.09761734306812286\n",
            "Epoch: 30990 | Training Loss: 0.13891392946243286 | Testing Loss: 0.09722910076379776\n",
            "Epoch: 31000 | Training Loss: 0.13835503160953522 | Testing Loss: 0.09683561325073242\n",
            "Epoch: 31010 | Training Loss: 0.13779622316360474 | Testing Loss: 0.09644728153944016\n",
            "Epoch: 31020 | Training Loss: 0.13723786175251007 | Testing Loss: 0.09605403244495392\n",
            "Epoch: 31030 | Training Loss: 0.13667933642864227 | Testing Loss: 0.09566307067871094\n",
            "Epoch: 31040 | Training Loss: 0.13612033426761627 | Testing Loss: 0.09526963531970978\n",
            "Epoch: 31050 | Training Loss: 0.13556204736232758 | Testing Loss: 0.09488134831190109\n",
            "Epoch: 31060 | Training Loss: 0.1350030153989792 | Testing Loss: 0.09448809921741486\n",
            "Epoch: 31070 | Training Loss: 0.13444451987743378 | Testing Loss: 0.0940970927476883\n",
            "Epoch: 31080 | Training Loss: 0.13388611376285553 | Testing Loss: 0.09370880573987961\n",
            "Epoch: 31090 | Training Loss: 0.13332735002040863 | Testing Loss: 0.09331536293029785\n",
            "Epoch: 31100 | Training Loss: 0.1327683925628662 | Testing Loss: 0.09292707592248917\n",
            "Epoch: 31110 | Training Loss: 0.13221003115177155 | Testing Loss: 0.09253382682800293\n",
            "Epoch: 31120 | Training Loss: 0.1316511482000351 | Testing Loss: 0.09214039146900177\n",
            "Epoch: 31130 | Training Loss: 0.13109295070171356 | Testing Loss: 0.09175214916467667\n",
            "Epoch: 31140 | Training Loss: 0.13053388893604279 | Testing Loss: 0.09135856479406357\n",
            "Epoch: 31150 | Training Loss: 0.1299753189086914 | Testing Loss: 0.09096784889698029\n",
            "Epoch: 31160 | Training Loss: 0.12941700220108032 | Testing Loss: 0.0905795618891716\n",
            "Epoch: 31170 | Training Loss: 0.12885817885398865 | Testing Loss: 0.09018611907958984\n",
            "Epoch: 31180 | Training Loss: 0.12829940021038055 | Testing Loss: 0.08979787677526474\n",
            "Epoch: 31190 | Training Loss: 0.12774090468883514 | Testing Loss: 0.08940429985523224\n",
            "Epoch: 31200 | Training Loss: 0.1271820217370987 | Testing Loss: 0.08901095390319824\n",
            "Epoch: 31210 | Training Loss: 0.12662379443645477 | Testing Loss: 0.08862252533435822\n",
            "Epoch: 31220 | Training Loss: 0.12606482207775116 | Testing Loss: 0.0882292315363884\n",
            "Epoch: 31230 | Training Loss: 0.12550656497478485 | Testing Loss: 0.08784089237451553\n",
            "Epoch: 31240 | Training Loss: 0.12494790554046631 | Testing Loss: 0.08745002746582031\n",
            "Epoch: 31250 | Training Loss: 0.12438904494047165 | Testing Loss: 0.08705668896436691\n",
            "Epoch: 31260 | Training Loss: 0.12383037060499191 | Testing Loss: 0.08666825294494629\n",
            "Epoch: 31270 | Training Loss: 0.12327184528112411 | Testing Loss: 0.08627495914697647\n",
            "Epoch: 31280 | Training Loss: 0.12271291017532349 | Testing Loss: 0.08588152378797531\n",
            "Epoch: 31290 | Training Loss: 0.12215454876422882 | Testing Loss: 0.08549328148365021\n",
            "Epoch: 31300 | Training Loss: 0.12159563601016998 | Testing Loss: 0.08510003238916397\n",
            "Epoch: 31310 | Training Loss: 0.12103742361068726 | Testing Loss: 0.08471155166625977\n",
            "Epoch: 31320 | Training Loss: 0.12047886848449707 | Testing Loss: 0.08432068675756454\n",
            "Epoch: 31330 | Training Loss: 0.11991993337869644 | Testing Loss: 0.08392725139856339\n",
            "Epoch: 31340 | Training Loss: 0.11936129629611969 | Testing Loss: 0.08353900909423828\n",
            "Epoch: 31350 | Training Loss: 0.11880265921354294 | Testing Loss: 0.08314575999975204\n",
            "Epoch: 31360 | Training Loss: 0.11824377626180649 | Testing Loss: 0.08275218307971954\n",
            "Epoch: 31370 | Training Loss: 0.11768551170825958 | Testing Loss: 0.0823640376329422\n",
            "Epoch: 31380 | Training Loss: 0.11712653934955597 | Testing Loss: 0.0819706916809082\n",
            "Epoch: 31390 | Training Loss: 0.11656832695007324 | Testing Loss: 0.081582210958004\n",
            "Epoch: 31400 | Training Loss: 0.11600935459136963 | Testing Loss: 0.0811888724565506\n",
            "Epoch: 31410 | Training Loss: 0.11545079946517944 | Testing Loss: 0.08079791069030762\n",
            "Epoch: 31420 | Training Loss: 0.11489235609769821 | Testing Loss: 0.08040976524353027\n",
            "Epoch: 31430 | Training Loss: 0.11433356255292892 | Testing Loss: 0.08001642674207687\n",
            "Epoch: 31440 | Training Loss: 0.11377464979887009 | Testing Loss: 0.07962794601917267\n",
            "Epoch: 31450 | Training Loss: 0.11321637779474258 | Testing Loss: 0.07923460006713867\n",
            "Epoch: 31460 | Training Loss: 0.11265744268894196 | Testing Loss: 0.07884135097265244\n",
            "Epoch: 31470 | Training Loss: 0.11209919303655624 | Testing Loss: 0.07845287770032883\n",
            "Epoch: 31480 | Training Loss: 0.11154019832611084 | Testing Loss: 0.07805953174829483\n",
            "Epoch: 31490 | Training Loss: 0.11098162084817886 | Testing Loss: 0.07766871899366379\n",
            "Epoch: 31500 | Training Loss: 0.11042322963476181 | Testing Loss: 0.07728033512830734\n",
            "Epoch: 31510 | Training Loss: 0.10986446589231491 | Testing Loss: 0.0768870860338211\n",
            "Epoch: 31520 | Training Loss: 0.10930563509464264 | Testing Loss: 0.0764986053109169\n",
            "Epoch: 31530 | Training Loss: 0.1087472215294838 | Testing Loss: 0.0761052593588829\n",
            "Epoch: 31540 | Training Loss: 0.10818828642368317 | Testing Loss: 0.0757119208574295\n",
            "Epoch: 31550 | Training Loss: 0.10763005167245865 | Testing Loss: 0.07532353699207306\n",
            "Epoch: 31560 | Training Loss: 0.10707099735736847 | Testing Loss: 0.07493028789758682\n",
            "Epoch: 31570 | Training Loss: 0.10651250928640366 | Testing Loss: 0.07453947514295578\n",
            "Epoch: 31580 | Training Loss: 0.10595407336950302 | Testing Loss: 0.07415099442005157\n",
            "Epoch: 31590 | Training Loss: 0.10539530962705612 | Testing Loss: 0.07375764846801758\n",
            "Epoch: 31600 | Training Loss: 0.10483663529157639 | Testing Loss: 0.07336926460266113\n",
            "Epoch: 31610 | Training Loss: 0.10427802056074142 | Testing Loss: 0.0729760155081749\n",
            "Epoch: 31620 | Training Loss: 0.10371913015842438 | Testing Loss: 0.0725826770067215\n",
            "Epoch: 31630 | Training Loss: 0.10316087305545807 | Testing Loss: 0.07219429314136505\n",
            "Epoch: 31640 | Training Loss: 0.10260190814733505 | Testing Loss: 0.07180094718933105\n",
            "Epoch: 31650 | Training Loss: 0.10204360634088516 | Testing Loss: 0.07141246646642685\n",
            "Epoch: 31660 | Training Loss: 0.10148505121469498 | Testing Loss: 0.07102175056934357\n",
            "Epoch: 31670 | Training Loss: 0.10092615336179733 | Testing Loss: 0.07062840461730957\n",
            "Epoch: 31680 | Training Loss: 0.10036740452051163 | Testing Loss: 0.07024002075195312\n",
            "Epoch: 31690 | Training Loss: 0.09980893135070801 | Testing Loss: 0.06984668225049973\n",
            "Epoch: 31700 | Training Loss: 0.09925048798322678 | Testing Loss: 0.06945567578077316\n",
            "Epoch: 31710 | Training Loss: 0.09869140386581421 | Testing Loss: 0.06906242668628693\n",
            "Epoch: 31720 | Training Loss: 0.09813316911458969 | Testing Loss: 0.06867413967847824\n",
            "Epoch: 31730 | Training Loss: 0.0975741371512413 | Testing Loss: 0.06828069686889648\n",
            "Epoch: 31740 | Training Loss: 0.09701602905988693 | Testing Loss: 0.0678924098610878\n",
            "Epoch: 31750 | Training Loss: 0.09645695984363556 | Testing Loss: 0.0674990639090538\n",
            "Epoch: 31760 | Training Loss: 0.09589838981628418 | Testing Loss: 0.0671105906367302\n",
            "Epoch: 31770 | Training Loss: 0.09533979743719101 | Testing Loss: 0.0667172446846962\n",
            "Epoch: 31780 | Training Loss: 0.0947808027267456 | Testing Loss: 0.06632643193006516\n",
            "Epoch: 31790 | Training Loss: 0.0942222848534584 | Testing Loss: 0.06593318283557892\n",
            "Epoch: 31800 | Training Loss: 0.09366398304700851 | Testing Loss: 0.06554479897022247\n",
            "Epoch: 31810 | Training Loss: 0.09310503304004669 | Testing Loss: 0.06515145301818848\n",
            "Epoch: 31820 | Training Loss: 0.09254682064056396 | Testing Loss: 0.06476297229528427\n",
            "Epoch: 31830 | Training Loss: 0.09198782593011856 | Testing Loss: 0.06436963379383087\n",
            "Epoch: 31840 | Training Loss: 0.09142930805683136 | Testing Loss: 0.06398115307092667\n",
            "Epoch: 31850 | Training Loss: 0.0908706784248352 | Testing Loss: 0.06358780711889267\n",
            "Epoch: 31860 | Training Loss: 0.09031175822019577 | Testing Loss: 0.06319966167211533\n",
            "Epoch: 31870 | Training Loss: 0.0897531658411026 | Testing Loss: 0.06280355900526047\n",
            "Epoch: 31880 | Training Loss: 0.08919484913349152 | Testing Loss: 0.062415361404418945\n",
            "Epoch: 31890 | Training Loss: 0.0886358991265297 | Testing Loss: 0.06202192232012749\n",
            "Epoch: 31900 | Training Loss: 0.08807770162820816 | Testing Loss: 0.06163353845477104\n",
            "Epoch: 31910 | Training Loss: 0.08751872926950455 | Testing Loss: 0.061240293085575104\n",
            "Epoch: 31920 | Training Loss: 0.0869603157043457 | Testing Loss: 0.0608518123626709\n",
            "Epoch: 31930 | Training Loss: 0.08640160411596298 | Testing Loss: 0.06045856699347496\n",
            "Epoch: 31940 | Training Loss: 0.08584269136190414 | Testing Loss: 0.06007032468914986\n",
            "Epoch: 31950 | Training Loss: 0.08528395742177963 | Testing Loss: 0.05967431142926216\n",
            "Epoch: 31960 | Training Loss: 0.0847257524728775 | Testing Loss: 0.059286024421453476\n",
            "Epoch: 31970 | Training Loss: 0.08416681736707687 | Testing Loss: 0.05889258533716202\n",
            "Epoch: 31980 | Training Loss: 0.08360862731933594 | Testing Loss: 0.058504294604063034\n",
            "Epoch: 31990 | Training Loss: 0.08304957300424576 | Testing Loss: 0.058110952377319336\n",
            "Epoch: 32000 | Training Loss: 0.08249125629663467 | Testing Loss: 0.05772261694073677\n",
            "Epoch: 32010 | Training Loss: 0.0819324254989624 | Testing Loss: 0.05732932314276695\n",
            "Epoch: 32020 | Training Loss: 0.08137360960245132 | Testing Loss: 0.05694098398089409\n",
            "Epoch: 32030 | Training Loss: 0.08081521093845367 | Testing Loss: 0.05654764175415039\n",
            "Epoch: 32040 | Training Loss: 0.08025659620761871 | Testing Loss: 0.05615668371319771\n",
            "Epoch: 32050 | Training Loss: 0.07969766110181808 | Testing Loss: 0.05576334148645401\n",
            "Epoch: 32060 | Training Loss: 0.07913944870233536 | Testing Loss: 0.055375050753355026\n",
            "Epoch: 32070 | Training Loss: 0.07858039438724518 | Testing Loss: 0.05498180538415909\n",
            "Epoch: 32080 | Training Loss: 0.07802219688892365 | Testing Loss: 0.05459337309002876\n",
            "Epoch: 32090 | Training Loss: 0.07746322453022003 | Testing Loss: 0.05419979244470596\n",
            "Epoch: 32100 | Training Loss: 0.07690447568893433 | Testing Loss: 0.05381155014038086\n",
            "Epoch: 32110 | Training Loss: 0.07634615898132324 | Testing Loss: 0.0534181110560894\n",
            "Epoch: 32120 | Training Loss: 0.07578741759061813 | Testing Loss: 0.05302753672003746\n",
            "Epoch: 32130 | Training Loss: 0.07522854954004288 | Testing Loss: 0.05263400077819824\n",
            "Epoch: 32140 | Training Loss: 0.07467024773359299 | Testing Loss: 0.052245523780584335\n",
            "Epoch: 32150 | Training Loss: 0.0741112157702446 | Testing Loss: 0.05185217782855034\n",
            "Epoch: 32160 | Training Loss: 0.073553167283535 | Testing Loss: 0.05146384239196777\n",
            "Epoch: 32170 | Training Loss: 0.07299410551786423 | Testing Loss: 0.051070597022771835\n",
            "Epoch: 32180 | Training Loss: 0.07243553549051285 | Testing Loss: 0.05068230628967285\n",
            "Epoch: 32190 | Training Loss: 0.07187683880329132 | Testing Loss: 0.05028886720538139\n",
            "Epoch: 32200 | Training Loss: 0.07131796330213547 | Testing Loss: 0.04989790916442871\n",
            "Epoch: 32210 | Training Loss: 0.07075943052768707 | Testing Loss: 0.04950466379523277\n",
            "Epoch: 32220 | Training Loss: 0.07020112127065659 | Testing Loss: 0.04911632463335991\n",
            "Epoch: 32230 | Training Loss: 0.06964217871427536 | Testing Loss: 0.04872284084558487\n",
            "Epoch: 32240 | Training Loss: 0.06908386200666428 | Testing Loss: 0.048334598541259766\n",
            "Epoch: 32250 | Training Loss: 0.06852497905492783 | Testing Loss: 0.04794135317206383\n",
            "Epoch: 32260 | Training Loss: 0.06796649843454361 | Testing Loss: 0.04755296930670738\n",
            "Epoch: 32270 | Training Loss: 0.06740772724151611 | Testing Loss: 0.047159623354673386\n",
            "Epoch: 32280 | Training Loss: 0.06684888899326324 | Testing Loss: 0.04677114635705948\n",
            "Epoch: 32290 | Training Loss: 0.06629014015197754 | Testing Loss: 0.046375323086977005\n",
            "Epoch: 32300 | Training Loss: 0.06573200225830078 | Testing Loss: 0.0459870807826519\n",
            "Epoch: 32310 | Training Loss: 0.06517304480075836 | Testing Loss: 0.0455937385559082\n",
            "Epoch: 32320 | Training Loss: 0.06461475044488907 | Testing Loss: 0.04520535469055176\n",
            "Epoch: 32330 | Training Loss: 0.06405613571405411 | Testing Loss: 0.044814299792051315\n",
            "Epoch: 32340 | Training Loss: 0.0634971633553505 | Testing Loss: 0.04442105442285538\n",
            "Epoch: 32350 | Training Loss: 0.06293860822916031 | Testing Loss: 0.04403281211853027\n",
            "Epoch: 32360 | Training Loss: 0.06238006427884102 | Testing Loss: 0.043639469891786575\n",
            "Epoch: 32370 | Training Loss: 0.06182100996375084 | Testing Loss: 0.043245889246463776\n",
            "Epoch: 32380 | Training Loss: 0.06126288324594498 | Testing Loss: 0.04285745695233345\n",
            "Epoch: 32390 | Training Loss: 0.060703884810209274 | Testing Loss: 0.042464207857847214\n",
            "Epoch: 32400 | Training Loss: 0.06014561653137207 | Testing Loss: 0.04207582399249077\n",
            "Epoch: 32410 | Training Loss: 0.05958665534853935 | Testing Loss: 0.04168248176574707\n",
            "Epoch: 32420 | Training Loss: 0.059028029441833496 | Testing Loss: 0.04129162058234215\n",
            "Epoch: 32430 | Training Loss: 0.05846957117319107 | Testing Loss: 0.04090318828821182\n",
            "Epoch: 32440 | Training Loss: 0.05791090801358223 | Testing Loss: 0.040509939193725586\n",
            "Epoch: 32450 | Training Loss: 0.05735199525952339 | Testing Loss: 0.04012155532836914\n",
            "Epoch: 32460 | Training Loss: 0.05679367855191231 | Testing Loss: 0.03972821310162544\n",
            "Epoch: 32470 | Training Loss: 0.05623464658856392 | Testing Loss: 0.039334964007139206\n",
            "Epoch: 32480 | Training Loss: 0.05567653104662895 | Testing Loss: 0.0389464870095253\n",
            "Epoch: 32490 | Training Loss: 0.05511752516031265 | Testing Loss: 0.03855304792523384\n",
            "Epoch: 32500 | Training Loss: 0.05455897003412247 | Testing Loss: 0.03816237673163414\n",
            "Epoch: 32510 | Training Loss: 0.05400055646896362 | Testing Loss: 0.037773944437503815\n",
            "Epoch: 32520 | Training Loss: 0.053441669791936874 | Testing Loss: 0.03738069534301758\n",
            "Epoch: 32530 | Training Loss: 0.05288287624716759 | Testing Loss: 0.03699221834540367\n",
            "Epoch: 32540 | Training Loss: 0.05232454463839531 | Testing Loss: 0.03659877926111221\n",
            "Epoch: 32550 | Training Loss: 0.05176560953259468 | Testing Loss: 0.03620553016662598\n",
            "Epoch: 32560 | Training Loss: 0.05120737478137016 | Testing Loss: 0.03581724315881729\n",
            "Epoch: 32570 | Training Loss: 0.05064842849969864 | Testing Loss: 0.03542380407452583\n",
            "Epoch: 32580 | Training Loss: 0.0500897541642189 | Testing Loss: 0.03503308445215225\n",
            "Epoch: 32590 | Training Loss: 0.049531448632478714 | Testing Loss: 0.034644510596990585\n",
            "Epoch: 32600 | Training Loss: 0.048972632735967636 | Testing Loss: 0.03425126150250435\n",
            "Epoch: 32610 | Training Loss: 0.048413850367069244 | Testing Loss: 0.03386297449469566\n",
            "Epoch: 32620 | Training Loss: 0.047855447977781296 | Testing Loss: 0.033469535410404205\n",
            "Epoch: 32630 | Training Loss: 0.04729647561907768 | Testing Loss: 0.03307619318366051\n",
            "Epoch: 32640 | Training Loss: 0.046738266944885254 | Testing Loss: 0.03268790245056152\n",
            "Epoch: 32650 | Training Loss: 0.04617929458618164 | Testing Loss: 0.032294560223817825\n",
            "Epoch: 32660 | Training Loss: 0.04562092944979668 | Testing Loss: 0.03190607950091362\n",
            "Epoch: 32670 | Training Loss: 0.045062389224767685 | Testing Loss: 0.03151526674628258\n",
            "Epoch: 32680 | Training Loss: 0.04450349882245064 | Testing Loss: 0.03112192265689373\n",
            "Epoch: 32690 | Training Loss: 0.04394484683871269 | Testing Loss: 0.030733633786439896\n",
            "Epoch: 32700 | Training Loss: 0.043386317789554596 | Testing Loss: 0.03034028969705105\n",
            "Epoch: 32710 | Training Loss: 0.04282731935381889 | Testing Loss: 0.02994694747030735\n",
            "Epoch: 32720 | Training Loss: 0.04226914793252945 | Testing Loss: 0.029558563604950905\n",
            "Epoch: 32730 | Training Loss: 0.04171014949679375 | Testing Loss: 0.029165124520659447\n",
            "Epoch: 32740 | Training Loss: 0.04115189239382744 | Testing Loss: 0.028776740655303\n",
            "Epoch: 32750 | Training Loss: 0.04059337452054024 | Testing Loss: 0.02838602103292942\n",
            "Epoch: 32760 | Training Loss: 0.04003434255719185 | Testing Loss: 0.027992678806185722\n",
            "Epoch: 32770 | Training Loss: 0.03947578743100166 | Testing Loss: 0.027604294940829277\n",
            "Epoch: 32780 | Training Loss: 0.0389171727001667 | Testing Loss: 0.02721085585653782\n",
            "Epoch: 32790 | Training Loss: 0.03835811838507652 | Testing Loss: 0.02681760862469673\n",
            "Epoch: 32800 | Training Loss: 0.037799861282110214 | Testing Loss: 0.026429224759340286\n",
            "Epoch: 32810 | Training Loss: 0.03724097087979317 | Testing Loss: 0.026035785675048828\n",
            "Epoch: 32820 | Training Loss: 0.03668276220560074 | Testing Loss: 0.025647640228271484\n",
            "Epoch: 32830 | Training Loss: 0.03612370416522026 | Testing Loss: 0.025254059582948685\n",
            "Epoch: 32840 | Training Loss: 0.03556513786315918 | Testing Loss: 0.024863338097929955\n",
            "Epoch: 32850 | Training Loss: 0.035006679594516754 | Testing Loss: 0.02447495423257351\n",
            "Epoch: 32860 | Training Loss: 0.03444799408316612 | Testing Loss: 0.0240815170109272\n",
            "Epoch: 32870 | Training Loss: 0.033889081329107285 | Testing Loss: 0.023693371564149857\n",
            "Epoch: 32880 | Training Loss: 0.033330585807561874 | Testing Loss: 0.02329735830426216\n",
            "Epoch: 32890 | Training Loss: 0.032772161066532135 | Testing Loss: 0.022909069433808327\n",
            "Epoch: 32900 | Training Loss: 0.03221327066421509 | Testing Loss: 0.02251572720706463\n",
            "Epoch: 32910 | Training Loss: 0.03165501356124878 | Testing Loss: 0.022127246484160423\n",
            "Epoch: 32920 | Training Loss: 0.03109605424106121 | Testing Loss: 0.021733904257416725\n",
            "Epoch: 32930 | Training Loss: 0.03053758107125759 | Testing Loss: 0.02134552039206028\n",
            "Epoch: 32940 | Training Loss: 0.029978860169649124 | Testing Loss: 0.02095217816531658\n",
            "Epoch: 32950 | Training Loss: 0.029420042410492897 | Testing Loss: 0.020563840866088867\n",
            "Epoch: 32960 | Training Loss: 0.028861260041594505 | Testing Loss: 0.020168019458651543\n",
            "Epoch: 32970 | Training Loss: 0.028303075581789017 | Testing Loss: 0.019779635593295097\n",
            "Epoch: 32980 | Training Loss: 0.027744079008698463 | Testing Loss: 0.01938629150390625\n",
            "Epoch: 32990 | Training Loss: 0.02718588151037693 | Testing Loss: 0.018997907638549805\n",
            "Epoch: 33000 | Training Loss: 0.026626920327544212 | Testing Loss: 0.018604565411806107\n",
            "Epoch: 33010 | Training Loss: 0.026068544015288353 | Testing Loss: 0.018216276541352272\n",
            "Epoch: 33020 | Training Loss: 0.025509750470519066 | Testing Loss: 0.017822837457060814\n",
            "Epoch: 33030 | Training Loss: 0.02495095692574978 | Testing Loss: 0.01743459701538086\n",
            "Epoch: 33040 | Training Loss: 0.024392450228333473 | Testing Loss: 0.01704125478863716\n",
            "Epoch: 33050 | Training Loss: 0.023833943530917168 | Testing Loss: 0.01665029488503933\n",
            "Epoch: 33060 | Training Loss: 0.023274946957826614 | Testing Loss: 0.01625695265829563\n",
            "Epoch: 33070 | Training Loss: 0.02271677367389202 | Testing Loss: 0.015868568792939186\n",
            "Epoch: 33080 | Training Loss: 0.022157788276672363 | Testing Loss: 0.015475320629775524\n",
            "Epoch: 33090 | Training Loss: 0.02159949578344822 | Testing Loss: 0.015086985193192959\n",
            "Epoch: 33100 | Training Loss: 0.021040594205260277 | Testing Loss: 0.014693403616547585\n",
            "Epoch: 33110 | Training Loss: 0.02048199251294136 | Testing Loss: 0.014305162243545055\n",
            "Epoch: 33120 | Training Loss: 0.0199233777821064 | Testing Loss: 0.013911820016801357\n",
            "Epoch: 33130 | Training Loss: 0.01936480961740017 | Testing Loss: 0.013521051965653896\n",
            "Epoch: 33140 | Training Loss: 0.0188058502972126 | Testing Loss: 0.013127708807587624\n",
            "Epoch: 33150 | Training Loss: 0.018247617408633232 | Testing Loss: 0.012739134021103382\n",
            "Epoch: 33160 | Training Loss: 0.017688727006316185 | Testing Loss: 0.012345886789262295\n",
            "Epoch: 33170 | Training Loss: 0.01713043451309204 | Testing Loss: 0.011957550421357155\n",
            "Epoch: 33180 | Training Loss: 0.016571437940001488 | Testing Loss: 0.011564112268388271\n",
            "Epoch: 33190 | Training Loss: 0.016012907028198242 | Testing Loss: 0.011175823397934437\n",
            "Epoch: 33200 | Training Loss: 0.015454280190169811 | Testing Loss: 0.01078257616609335\n",
            "Epoch: 33210 | Training Loss: 0.014895307831466198 | Testing Loss: 0.010391617193818092\n",
            "Epoch: 33220 | Training Loss: 0.014336693100631237 | Testing Loss: 0.00999827403575182\n",
            "Epoch: 33230 | Training Loss: 0.013778460212051868 | Testing Loss: 0.009609842672944069\n",
            "Epoch: 33240 | Training Loss: 0.01321954745799303 | Testing Loss: 0.009216547012329102\n",
            "Epoch: 33250 | Training Loss: 0.012661302462220192 | Testing Loss: 0.008828306570649147\n",
            "Epoch: 33260 | Training Loss: 0.012102330103516579 | Testing Loss: 0.008434963412582874\n",
            "Epoch: 33270 | Training Loss: 0.011543750762939453 | Testing Loss: 0.008046579547226429\n",
            "Epoch: 33280 | Training Loss: 0.010985123924911022 | Testing Loss: 0.007653236389160156\n",
            "Epoch: 33290 | Training Loss: 0.01042630709707737 | Testing Loss: 0.007264757063239813\n",
            "Epoch: 33300 | Training Loss: 0.009867513552308083 | Testing Loss: 0.006869030185043812\n",
            "Epoch: 33310 | Training Loss: 0.009309351444244385 | Testing Loss: 0.006480693817138672\n",
            "Epoch: 33320 | Training Loss: 0.008750343695282936 | Testing Loss: 0.006087112706154585\n",
            "Epoch: 33330 | Training Loss: 0.008192146196961403 | Testing Loss: 0.005698967259377241\n",
            "Epoch: 33340 | Training Loss: 0.00763328094035387 | Testing Loss: 0.00530552864074707\n",
            "Epoch: 33350 | Training Loss: 0.007074725814163685 | Testing Loss: 0.004917240235954523\n",
            "Epoch: 33360 | Training Loss: 0.00651596812531352 | Testing Loss: 0.00452389707788825\n",
            "Epoch: 33370 | Training Loss: 0.005957079119980335 | Testing Loss: 0.004135418217629194\n",
            "Epoch: 33380 | Training Loss: 0.0053983330726623535 | Testing Loss: 0.0037395001854747534\n",
            "Epoch: 33390 | Training Loss: 0.004840302746742964 | Testing Loss: 0.0033512592781335115\n",
            "Epoch: 33400 | Training Loss: 0.004281222820281982 | Testing Loss: 0.002958011580631137\n",
            "Epoch: 33410 | Training Loss: 0.00372298969887197 | Testing Loss: 0.0025696277152746916\n",
            "Epoch: 33420 | Training Loss: 0.0031639577355235815 | Testing Loss: 0.002176189562305808\n",
            "Epoch: 33430 | Training Loss: 0.002605700632557273 | Testing Loss: 0.001787805580534041\n",
            "Epoch: 33440 | Training Loss: 0.002046775771304965 | Testing Loss: 0.0013944626552984118\n",
            "Epoch: 33450 | Training Loss: 0.0014880656963214278 | Testing Loss: 0.0010060787899419665\n",
            "Epoch: 33460 | Training Loss: 0.0009296179050579667 | Testing Loss: 0.0006127357482910156\n",
            "Epoch: 33470 | Training Loss: 0.00037097930908203125 | Testing Loss: 0.00022192001051735133\n",
            "Epoch: 33480 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 33490 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 33500 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 33510 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 33520 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 33530 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 33540 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 33550 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 33560 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 33570 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 33580 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 33590 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 33600 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 33610 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 33620 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 33630 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 33640 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 33650 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 33660 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 33670 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 33680 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 33690 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 33700 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 33710 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 33720 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 33730 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 33740 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 33750 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 33760 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 33770 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 33780 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 33790 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 33800 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 33810 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 33820 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 33830 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 33840 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 33850 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 33860 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 33870 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 33880 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 33890 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 33900 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 33910 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 33920 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 33930 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 33940 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 33950 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 33960 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 33970 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 33980 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 33990 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34000 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34010 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34020 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34030 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34040 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34050 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34060 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34070 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34080 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34090 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34100 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34110 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34120 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34130 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34140 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34150 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34160 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34170 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34180 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34190 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34200 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34210 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34220 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34230 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34240 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34250 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34260 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34270 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34280 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34290 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34300 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34310 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34320 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34330 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34340 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34350 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34360 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34370 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34380 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34390 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34400 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34410 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34420 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34430 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34440 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34450 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34460 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34470 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34480 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34490 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34500 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34510 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34520 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34530 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34540 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34550 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34560 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34570 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34580 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34590 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34600 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34610 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34620 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34630 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34640 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34650 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34660 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34670 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34680 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34690 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34700 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34710 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34720 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34730 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34740 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34750 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34760 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34770 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34780 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34790 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34800 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34810 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34820 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34830 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34840 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34850 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34860 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34870 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34880 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34890 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34900 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34910 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34920 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34930 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34940 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34950 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34960 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34970 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34980 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 34990 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35000 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35010 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35020 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35030 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35040 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35050 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35060 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35070 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35080 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35090 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35100 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35110 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35120 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35130 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35140 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35150 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35160 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35170 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35180 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35190 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35200 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35210 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35220 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35230 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35240 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35250 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35260 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35270 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35280 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35290 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35300 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35310 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35320 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35330 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35340 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35350 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35360 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35370 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35380 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35390 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35400 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35410 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35420 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35430 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35440 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35450 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35460 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35470 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35480 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35490 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35500 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35510 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35520 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35530 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35540 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35550 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35560 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35570 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35580 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35590 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35600 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35610 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35620 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35630 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35640 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35650 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35660 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35670 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35680 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35690 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35700 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35710 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35720 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35730 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35740 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35750 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35760 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35770 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35780 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35790 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35800 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35810 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35820 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35830 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35840 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35850 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35860 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35870 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35880 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35890 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35900 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35910 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35920 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35930 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35940 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35950 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35960 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35970 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35980 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 35990 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36000 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36010 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36020 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36030 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36040 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36050 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36060 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36070 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36080 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36090 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36100 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36110 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36120 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36130 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36140 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36150 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36160 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36170 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36180 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36190 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36200 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36210 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36220 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36230 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36240 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36250 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36260 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36270 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36280 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36290 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36300 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36310 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36320 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36330 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36340 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36350 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36360 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36370 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36380 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36390 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36400 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36410 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36420 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36430 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36440 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36450 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36460 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36470 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36480 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36490 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36500 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36510 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36520 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36530 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36540 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36550 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36560 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36570 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36580 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36590 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36600 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36610 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36620 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36630 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36640 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36650 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36660 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36670 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36680 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36690 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36700 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36710 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36720 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36730 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36740 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36750 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36760 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36770 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36780 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36790 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36800 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36810 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36820 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36830 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36840 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36850 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36860 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36870 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36880 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36890 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36900 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36910 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36920 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36930 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36940 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36950 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36960 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36970 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36980 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 36990 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37000 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37010 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37020 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37030 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37040 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37050 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37060 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37070 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37080 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37090 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37100 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37110 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37120 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37130 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37140 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37150 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37160 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37170 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37180 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37190 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37200 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37210 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37220 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37230 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37240 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37250 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37260 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37270 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37280 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37290 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37300 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37310 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37320 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37330 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37340 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37350 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37360 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37370 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37380 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37390 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37400 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37410 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37420 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37430 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37440 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37450 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37460 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37470 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37480 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37490 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37500 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37510 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37520 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37530 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37540 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37550 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37560 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37570 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37580 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37590 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37600 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37610 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37620 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37630 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37640 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37650 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37660 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37670 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37680 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37690 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37700 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37710 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37720 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37730 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37740 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37750 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37760 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37770 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37780 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37790 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37800 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37810 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37820 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37830 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37840 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37850 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37860 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37870 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37880 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37890 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37900 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37910 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37920 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37930 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37940 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37950 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37960 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37970 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37980 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 37990 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38000 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38010 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38020 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38030 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38040 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38050 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38060 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38070 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38080 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38090 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38100 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38110 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38120 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38130 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38140 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38150 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38160 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38170 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38180 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38190 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38200 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38210 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38220 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38230 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38240 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38250 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38260 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38270 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38280 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38290 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38300 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38310 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38320 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38330 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38340 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38350 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38360 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38370 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38380 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38390 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38400 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38410 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38420 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38430 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38440 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38450 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38460 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38470 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38480 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38490 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38500 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38510 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38520 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38530 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38540 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38550 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38560 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38570 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38580 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38590 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38600 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38610 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38620 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38630 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38640 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38650 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38660 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38670 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38680 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38690 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38700 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38710 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38720 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38730 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38740 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38750 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38760 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38770 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38780 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38790 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38800 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38810 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38820 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38830 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38840 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38850 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38860 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38870 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38880 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38890 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38900 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38910 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38920 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38930 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38940 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38950 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38960 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38970 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38980 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 38990 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39000 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39010 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39020 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39030 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39040 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39050 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39060 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39070 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39080 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39090 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39100 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39110 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39120 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39130 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39140 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39150 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39160 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39170 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39180 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39190 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39200 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39210 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39220 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39230 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39240 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39250 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39260 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39270 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39280 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39290 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39300 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39310 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39320 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39330 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39340 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39350 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39360 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39370 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39380 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39390 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39400 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39410 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39420 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39430 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39440 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39450 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39460 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39470 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39480 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39490 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39500 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39510 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39520 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39530 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39540 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39550 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39560 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39570 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39580 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39590 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39600 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39610 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39620 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39630 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39640 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39650 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39660 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39670 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39680 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39690 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39700 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39710 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39720 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39730 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39740 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39750 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39760 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39770 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39780 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39790 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39800 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39810 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39820 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39830 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39840 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39850 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39860 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39870 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39880 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39890 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39900 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39910 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39920 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39930 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39940 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39950 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39960 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39970 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39980 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 39990 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40000 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40010 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40020 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40030 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40040 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40050 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40060 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40070 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40080 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40090 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40100 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40110 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40120 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40130 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40140 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40150 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40160 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40170 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40180 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40190 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40200 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40210 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40220 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40230 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40240 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40250 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40260 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40270 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40280 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40290 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40300 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40310 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40320 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40330 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40340 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40350 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40360 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40370 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40380 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40390 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40400 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40410 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40420 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40430 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40440 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40450 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40460 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40470 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40480 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40490 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40500 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40510 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40520 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40530 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40540 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40550 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40560 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40570 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40580 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40590 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40600 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40610 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40620 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40630 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40640 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40650 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40660 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40670 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40680 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40690 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40700 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40710 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40720 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40730 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40740 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40750 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40760 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40770 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40780 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40790 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40800 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40810 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40820 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40830 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40840 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40850 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40860 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40870 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40880 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40890 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40900 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40910 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40920 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40930 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40940 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40950 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40960 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40970 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40980 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 40990 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41000 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41010 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41020 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41030 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41040 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41050 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41060 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41070 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41080 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41090 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41100 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41110 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41120 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41130 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41140 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41150 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41160 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41170 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41180 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41190 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41200 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41210 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41220 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41230 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41240 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41250 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41260 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41270 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41280 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41290 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41300 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41310 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41320 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41330 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41340 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41350 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41360 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41370 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41380 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41390 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41400 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41410 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41420 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41430 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41440 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41450 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41460 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41470 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41480 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41490 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41500 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41510 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41520 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41530 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41540 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41550 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41560 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41570 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41580 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41590 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41600 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41610 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41620 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41630 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41640 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41650 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41660 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41670 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41680 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41690 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41700 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41710 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41720 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41730 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41740 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41750 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41760 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41770 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41780 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41790 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41800 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41810 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41820 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41830 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41840 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41850 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41860 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41870 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41880 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41890 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41900 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41910 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41920 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41930 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41940 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41950 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41960 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41970 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41980 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 41990 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42000 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42010 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42020 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42030 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42040 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42050 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42060 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42070 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42080 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42090 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42100 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42110 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42120 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42130 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42140 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42150 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42160 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42170 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42180 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42190 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42200 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42210 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42220 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42230 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42240 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42250 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42260 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42270 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42280 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42290 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42300 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42310 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42320 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42330 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42340 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42350 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42360 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42370 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42380 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42390 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42400 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42410 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42420 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42430 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42440 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42450 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42460 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42470 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42480 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42490 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42500 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42510 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42520 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42530 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42540 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42550 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42560 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42570 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42580 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42590 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42600 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42610 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42620 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42630 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42640 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42650 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42660 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42670 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42680 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42690 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42700 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42710 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42720 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42730 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42740 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42750 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42760 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42770 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42780 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42790 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42800 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42810 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42820 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42830 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42840 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42850 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42860 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42870 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42880 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42890 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42900 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42910 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42920 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42930 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42940 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42950 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42960 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42970 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42980 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 42990 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43000 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43010 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43020 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43030 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43040 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43050 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43060 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43070 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43080 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43090 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43100 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43110 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43120 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43130 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43140 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43150 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43160 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43170 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43180 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43190 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43200 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43210 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43220 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43230 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43240 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43250 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43260 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43270 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43280 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43290 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43300 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43310 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43320 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43330 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43340 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43350 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43360 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43370 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43380 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43390 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43400 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43410 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43420 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43430 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43440 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43450 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43460 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43470 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43480 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43490 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43500 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43510 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43520 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43530 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43540 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43550 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43560 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43570 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43580 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43590 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43600 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43610 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43620 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43630 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43640 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43650 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43660 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43670 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43680 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43690 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43700 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43710 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43720 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43730 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43740 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43750 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43760 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43770 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43780 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43790 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43800 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43810 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43820 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43830 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43840 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43850 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43860 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43870 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43880 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43890 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43900 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43910 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43920 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43930 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43940 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43950 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43960 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43970 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43980 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 43990 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44000 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44010 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44020 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44030 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44040 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44050 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44060 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44070 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44080 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44090 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44100 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44110 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44120 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44130 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44140 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44150 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44160 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44170 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44180 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44190 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44200 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44210 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44220 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44230 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44240 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44250 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44260 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44270 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44280 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44290 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44300 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44310 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44320 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44330 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44340 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44350 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44360 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44370 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44380 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44390 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44400 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44410 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44420 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44430 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44440 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44450 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44460 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44470 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44480 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44490 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44500 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44510 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44520 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44530 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44540 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44550 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44560 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44570 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44580 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44590 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44600 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44610 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44620 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44630 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44640 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44650 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44660 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44670 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44680 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44690 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44700 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44710 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44720 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44730 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44740 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44750 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44760 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44770 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44780 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44790 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44800 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44810 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44820 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44830 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44840 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44850 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44860 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44870 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44880 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44890 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44900 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44910 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44920 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44930 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44940 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44950 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44960 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44970 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44980 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 44990 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45000 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45010 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45020 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45030 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45040 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45050 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45060 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45070 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45080 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45090 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45100 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45110 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45120 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45130 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45140 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45150 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45160 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45170 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45180 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45190 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45200 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45210 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45220 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45230 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45240 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45250 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45260 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45270 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45280 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45290 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45300 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45310 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45320 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45330 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45340 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45350 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45360 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45370 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45380 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45390 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45400 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45410 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45420 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45430 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45440 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45450 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45460 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45470 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45480 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45490 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45500 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45510 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45520 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45530 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45540 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45550 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45560 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45570 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45580 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45590 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45600 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45610 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45620 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45630 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45640 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45650 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45660 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45670 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45680 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45690 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45700 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45710 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45720 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45730 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45740 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45750 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45760 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45770 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45780 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45790 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45800 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45810 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45820 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45830 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45840 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45850 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45860 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45870 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45880 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45890 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45900 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45910 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45920 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45930 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45940 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45950 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45960 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45970 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45980 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 45990 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46000 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46010 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46020 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46030 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46040 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46050 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46060 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46070 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46080 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46090 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46100 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46110 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46120 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46130 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46140 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46150 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46160 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46170 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46180 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46190 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46200 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46210 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46220 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46230 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46240 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46250 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46260 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46270 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46280 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46290 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46300 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46310 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46320 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46330 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46340 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46350 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46360 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46370 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46380 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46390 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46400 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46410 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46420 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46430 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46440 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46450 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46460 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46470 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46480 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46490 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46500 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46510 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46520 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46530 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46540 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46550 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46560 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46570 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46580 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46590 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46600 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46610 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46620 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46630 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46640 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46650 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46660 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46670 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46680 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46690 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46700 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46710 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46720 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46730 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46740 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46750 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46760 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46770 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46780 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46790 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46800 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46810 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46820 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46830 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46840 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46850 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46860 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46870 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46880 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46890 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46900 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46910 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46920 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46930 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46940 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46950 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46960 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46970 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46980 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 46990 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47000 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47010 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47020 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47030 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47040 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47050 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47060 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47070 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47080 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47090 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47100 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47110 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47120 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47130 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47140 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47150 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47160 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47170 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47180 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47190 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47200 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47210 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47220 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47230 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47240 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47250 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47260 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47270 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47280 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47290 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47300 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47310 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47320 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47330 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47340 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47350 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47360 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47370 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47380 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47390 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47400 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47410 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47420 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47430 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47440 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47450 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47460 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47470 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47480 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47490 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47500 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47510 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47520 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47530 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47540 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47550 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47560 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47570 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47580 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47590 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47600 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47610 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47620 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47630 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47640 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47650 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47660 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47670 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47680 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47690 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47700 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47710 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47720 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47730 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47740 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47750 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47760 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47770 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47780 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47790 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47800 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47810 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47820 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47830 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47840 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47850 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47860 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47870 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47880 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47890 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47900 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47910 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47920 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47930 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47940 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47950 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47960 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47970 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47980 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 47990 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48000 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48010 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48020 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48030 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48040 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48050 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48060 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48070 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48080 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48090 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48100 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48110 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48120 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48130 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48140 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48150 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48160 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48170 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48180 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48190 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48200 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48210 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48220 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48230 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48240 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48250 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48260 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48270 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48280 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48290 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48300 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48310 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48320 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48330 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48340 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48350 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48360 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48370 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48380 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48390 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48400 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48410 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48420 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48430 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48440 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48450 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48460 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48470 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48480 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48490 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48500 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48510 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48520 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48530 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48540 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48550 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48560 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48570 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48580 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48590 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48600 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48610 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48620 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48630 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48640 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48650 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48660 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48670 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48680 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48690 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48700 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48710 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48720 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48730 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48740 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48750 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48760 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48770 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48780 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48790 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48800 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48810 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48820 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48830 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48840 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48850 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48860 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48870 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48880 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48890 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48900 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48910 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48920 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48930 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48940 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48950 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48960 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48970 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48980 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 48990 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49000 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49010 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49020 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49030 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49040 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49050 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49060 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49070 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49080 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49090 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49100 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49110 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49120 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49130 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49140 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49150 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49160 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49170 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49180 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49190 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49200 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49210 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49220 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49230 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49240 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49250 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49260 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49270 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49280 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49290 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49300 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49310 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49320 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49330 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49340 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49350 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49360 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49370 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49380 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49390 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49400 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49410 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49420 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49430 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49440 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49450 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49460 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49470 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49480 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49490 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49500 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49510 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49520 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49530 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49540 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49550 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49560 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49570 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49580 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49590 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49600 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49610 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49620 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49630 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49640 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49650 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49660 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49670 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49680 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49690 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49700 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49710 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49720 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49730 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49740 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49750 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49760 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49770 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49780 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49790 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49800 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49810 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49820 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49830 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49840 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49850 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49860 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49870 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49880 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49890 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49900 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49910 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49920 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49930 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49940 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49950 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49960 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49970 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49980 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n",
            "Epoch: 49990 | Training Loss: 0.0002528071345295757 | Testing Loss: 0.0009891033405438066\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(list(my_model.parameters()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMtpnq-2sv0x",
        "outputId": "e943e3dd-7063-4c00-9a96-e284963a02d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Parameter containing:\n",
            "tensor([[-8.9997]], device='cuda:0', requires_grad=True), Parameter containing:\n",
            "tensor([-4.9999], device='cuda:0', requires_grad=True)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "my_model.eval()\n",
        "with torch.no_grad():\n",
        "  y_preds = my_model(X_test)\n",
        "plot_dataset(X_train, X_test, y_train, y_test, y_preds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "rZ4LOLOesxOY",
        "outputId": "a2461681-cb52-4faa-9186-6de06ce3352d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(obj=my_model.state_dict(), f=\"saved_model.pth\")"
      ],
      "metadata": {
        "id": "3fLaIig-tAGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_model.load_state_dict(torch.load(f=\"saved_model.pth\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9c_8icGhtD-Q",
        "outputId": "acd81c2c-2e34-45e7-8d84-8f504865154a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lWjLznJxruMY"
      }
    }
  ]
}